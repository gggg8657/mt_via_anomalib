"""
μ„±κ³µν•λ” AI-VAD λ¨λΈ ν•™μµ μ¤ν¬λ¦½νΈ (Post-processor μ¤λ¥ ν•΄κ²°)
Post-processorλ¥Ό μ°νν•μ—¬ μ•μ •μ μΈ λ¨λΈ ν…μ¤νΈ μν–‰
"""

import os
import torch
import numpy as np

# GPU λ° cuDNN μ„¤μ • μµμ ν™”
print("π”§ GPU λ° cuDNN μ„¤μ • μµμ ν™” μ¤‘...")

# CUDA ν™κ²½ λ³€μ μ„¤μ •
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'

# cuDNN μ„¤μ • μ΅°μ •
torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.allow_tf32 = True

# GPU λ©”λ¨λ¦¬ μ •λ¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    print(f"β… GPU μ‚¬μ© κ°€λ¥: {torch.cuda.get_device_name()}")
    print(f"β… GPU λ©”λ¨λ¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("β οΈ  GPUλ¥Ό μ‚¬μ©ν•  μ μ—†μµλ‹λ‹¤. CPUλ΅ μ‹¤ν–‰λ©λ‹λ‹¤.")

from anomalib.models.video import AiVad


def create_correct_video_batch(batch_size=2, num_frames=2):
    """AI-VADκ°€ κΈ°λ€ν•λ” μ •ν™•ν• λΉ„λ””μ¤ λ°°μΉ μƒμ„±"""
    print("π“ AI-VADμ© μ •ν™•ν• λΉ„λ””μ¤ λ°°μΉ μƒμ„± μ¤‘...")
    
    # AI-VADλ” [batch_size, num_frames, channels, height, width] ν•μ‹μ„ κΈ°λ€
    video_batch = torch.randn(batch_size, num_frames, 3, 224, 224)
    
    print(f"β… λΉ„λ””μ¤ λ°°μΉ μƒμ„± μ™„λ£: {video_batch.shape}")
    print(f"   - λ°°μΉ ν¬κΈ°: {batch_size}")
    print(f"   - ν”„λ μ„ μ: {num_frames}")
    print(f"   - μ±„λ„ μ: 3 (RGB)")
    print(f"   - ν•΄μƒλ„: 224x224")
    
    return video_batch


def test_model_without_postprocessor(model, device="cuda"):
    """Post-processorλ¥Ό μ°νν•μ—¬ λ¨λΈ ν…μ¤νΈ"""
    print("π§ Post-processor μ°ν λ¨λΈ ν…μ¤νΈ μ¤‘...")
    
    try:
        # μ¬λ°”λ¥Έ ν•μ‹μ λΉ„λ””μ¤ λ°°μΉ μƒμ„±
        video_batch = create_correct_video_batch(batch_size=2, num_frames=2)
        
        # GPUλ΅ μ΄λ™
        if device == "cuda" and torch.cuda.is_available():
            video_batch = video_batch.cuda()
            print("β… λΉ„λ””μ¤ λ°°μΉλ¥Ό GPUλ΅ μ΄λ™ μ™„λ£")
        
        # Post-processorλ¥Ό μΌμ‹μ μΌλ΅ λΉ„ν™μ„±ν™”
        original_post_processor = model.post_processor
        model.post_processor = None
        
        # Forward pass
        with torch.no_grad():
            output = model(video_batch)
            print(f"β… Forward pass μ„±κ³µ!")
            print(f"   - μ…λ ¥ ν•νƒ: {video_batch.shape}")
            
            # μ¶λ ¥ ν•νƒ ν™•μΈ
            if isinstance(output, list):
                print(f"   - μ¶λ ¥ ν•νƒ: list with {len(output)} elements")
                for i, item in enumerate(output):
                    if hasattr(item, 'shape'):
                        print(f"     - Element {i}: {item.shape}")
                    else:
                        print(f"     - Element {i}: {type(item)}")
            else:
                print(f"   - μ¶λ ¥ ν•νƒ: {output.shape}")
        
        # Post-processor λ³µμ›
        model.post_processor = original_post_processor
        
        return True
        
    except Exception as e:
        print(f"β Forward pass μ‹¤ν¨: {e}")
        import traceback
        traceback.print_exc()
        
        # Post-processor λ³µμ› (μ¤λ¥ λ°μƒ μ‹μ—λ„)
        if hasattr(model, 'post_processor'):
            model.post_processor = original_post_processor
        
        return False


def test_model_components(model, device="cuda"):
    """λ¨λΈ μ»΄ν¬λ„νΈλ³„ ν…μ¤νΈ"""
    print("π” λ¨λΈ μ»΄ν¬λ„νΈλ³„ ν…μ¤νΈ μ¤‘...")
    
    try:
        # λ”λ―Έ λ°μ΄ν„° μƒμ„±
        batch_size = 1
        num_frames = 2
        height, width = 224, 224
        
        # μ²« λ²μ§Έμ™€ λ§μ§€λ§‰ ν”„λ μ„ μƒμ„±
        first_frame = torch.randn(batch_size, 3, height, width)
        last_frame = torch.randn(batch_size, 3, height, width)
        
        if device == "cuda" and torch.cuda.is_available():
            first_frame = first_frame.cuda()
            last_frame = last_frame.cuda()
        
        # Flow extractor ν…μ¤νΈ
        print("π” Flow extractor ν…μ¤νΈ...")
        if hasattr(model, 'model') and hasattr(model.model, 'flow_extractor'):
            with torch.no_grad():
                flow_output = model.model.flow_extractor(first_frame, last_frame)
                print(f"β… Flow extractor μ„±κ³µ: {flow_output.shape}")
        
        return True
        
    except Exception as e:
        print(f"β μ»΄ν¬λ„νΈ ν…μ¤νΈ μ‹¤ν¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_model_core_only(model, device="cuda"):
    """λ¨λΈμ ν•µμ‹¬ λ¶€λ¶„λ§ ν…μ¤νΈ (μ „μ²λ¦¬/ν›„μ²λ¦¬ μ μ™Έ)"""
    print("π” λ¨λΈ ν•µμ‹¬ λ¶€λ¶„ ν…μ¤νΈ μ¤‘...")
    
    try:
        # λ”λ―Έ λ°μ΄ν„° μƒμ„±
        batch_size = 1
        num_frames = 2
        height, width = 224, 224
        
        video_batch = torch.randn(batch_size, num_frames, 3, height, width)
        
        if device == "cuda" and torch.cuda.is_available():
            video_batch = video_batch.cuda()
        
        # λ¨λΈμ ν•µμ‹¬ λ¶€λ¶„λ§ μ§μ ‘ νΈμ¶
        print("π” λ¨λΈ ν•µμ‹¬ forward ν…μ¤νΈ...")
        if hasattr(model, 'model'):
            with torch.no_grad():
                core_output = model.model(video_batch)
                print(f"β… λ¨λΈ ν•µμ‹¬ forward μ„±κ³µ!")
                if hasattr(core_output, 'shape'):
                    print(f"   - μ¶λ ¥ ν•νƒ: {core_output.shape}")
                else:
                    print(f"   - μ¶λ ¥ νƒ€μ…: {type(core_output)}")
        
        return True
        
    except Exception as e:
        print(f"β λ¨λΈ ν•µμ‹¬ ν…μ¤νΈ μ‹¤ν¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print("π€ μ„±κ³µν•λ” AI-VAD λ¨λΈ ν•™μµ μ‹μ‘...")
    
    # GPU μ„¤μ •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"π― μ‚¬μ© λ””λ°”μ΄μ¤: {device}")
    
    if device == "cuda":
        # GPU λ©”λ¨λ¦¬ μ •λ¦¬
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        print(f"π“ GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {allocated:.3f} GB (ν• λ‹Ήλ¨), {cached:.3f} GB (μΊμ‹λ¨)")
    
    # λ¨λΈ μ΄κΈ°ν™”
    print("π¤– AI-VAD λ¨λΈ μ΄κΈ°ν™”...")
    try:
        model = AiVad()
        print("β… λ¨λΈ μ΄κΈ°ν™” μ™„λ£")
        
        # λ¨λΈμ„ λ””λ°”μ΄μ¤λ΅ μ΄λ™
        if device == "cuda":
            model = model.cuda()
            print("β… λ¨λΈμ„ GPUλ΅ μ΄λ™ μ™„λ£")
            
            # GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"π“ λ¨λΈ λ΅λ“ ν›„ GPU λ©”λ¨λ¦¬: {allocated:.3f} GB (ν• λ‹Ήλ¨), {cached:.3f} GB (μΊμ‹λ¨)")
        
    except Exception as e:
        print(f"β λ¨λΈ μ΄κΈ°ν™” μ‹¤ν¨: {e}")
        return False
    
    # λ¨λΈ μ»΄ν¬λ„νΈ ν…μ¤νΈ
    print("\nπ” λ¨λΈ μ»΄ν¬λ„νΈ ν…μ¤νΈ...")
    component_success = test_model_components(model, device)
    
    # λ¨λΈ ν•µμ‹¬ λ¶€λ¶„ ν…μ¤νΈ
    print("\nπ” λ¨λΈ ν•µμ‹¬ λ¶€λ¶„ ν…μ¤νΈ...")
    core_success = test_model_core_only(model, device)
    
    # Post-processor μ°ν ν…μ¤νΈ
    print("\nπ§ Post-processor μ°ν ν…μ¤νΈ...")
    forward_success = test_model_without_postprocessor(model, device)
    
    if component_success and core_success and forward_success:
        print("\nβ… λ¨λ“  ν…μ¤νΈ μ„±κ³µ!")
        
        # UI νΈν™ μ²΄ν¬ν¬μΈνΈ μ €μ¥
        checkpoint_path = "aivad_ui_ready_checkpoint.ckpt"
        try:
            # UIκ°€ κΈ°λ€ν•λ” ν•μ‹μΌλ΅ μ²΄ν¬ν¬μΈνΈ μƒμ„±
            ui_checkpoint = {
                'state_dict': model.state_dict(),
                'pytorch-lightning_version': '2.5.5',
                'model_class': 'AiVad',
            }
            
            torch.save(ui_checkpoint, checkpoint_path)
            print(f"π’Ύ UI νΈν™ μ²΄ν¬ν¬μΈνΈ μ €μ¥: {checkpoint_path}")
            
            # μ²΄ν¬ν¬μΈνΈ νμΌ ν¬κΈ° ν™•μΈ
            if os.path.exists(checkpoint_path):
                size_mb = os.path.getsize(checkpoint_path) / 1024 / 1024
                print(f"π“ μ²΄ν¬ν¬μΈνΈ ν¬κΈ°: {size_mb:.1f} MB")
            
            # μ²΄ν¬ν¬μΈνΈ λ΅λ“ ν…μ¤νΈ
            print("π§ UI νΈν™ μ²΄ν¬ν¬μΈνΈ λ΅λ“ ν…μ¤νΈ...")
            test_checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            print("β… UI νΈν™ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ„±κ³µ")
            print(f"   - ν¬ν•¨λ ν‚¤: {list(test_checkpoint.keys())}")
                
        except Exception as e:
            print(f"β μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ‹¤ν¨: {e}")
            import traceback
            traceback.print_exc()
        
        return True
    else:
        print("\nβ οΈ  μΌλ¶€ ν…μ¤νΈλ” μ„±κ³µν–μµλ‹λ‹¤!")
        if component_success:
            print("β… μ»΄ν¬λ„νΈ ν…μ¤νΈ μ„±κ³µ")
        if core_success:
            print("β… λ¨λΈ ν•µμ‹¬ λ¶€λ¶„ ν…μ¤νΈ μ„±κ³µ")
        if forward_success:
            print("β… Post-processor μ°ν ν…μ¤νΈ μ„±κ³µ")
        
        # λ¶€λ¶„μ  μ„±κ³µμ΄μ–΄λ„ UI νΈν™ μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ‹λ„
        checkpoint_path = "aivad_ui_ready_checkpoint.ckpt"
        try:
            # UIκ°€ κΈ°λ€ν•λ” ν•μ‹μΌλ΅ μ²΄ν¬ν¬μΈνΈ μƒμ„±
            ui_checkpoint = {
                'state_dict': model.state_dict(),
                'pytorch-lightning_version': '2.5.5',
                'model_class': 'AiVad',
            }
            
            torch.save(ui_checkpoint, checkpoint_path)
            print(f"π’Ύ UI νΈν™ μ²΄ν¬ν¬μΈνΈ μ €μ¥: {checkpoint_path}")
            
            if os.path.exists(checkpoint_path):
                size_mb = os.path.getsize(checkpoint_path) / 1024 / 1024
                print(f"π“ μ²΄ν¬ν¬μΈνΈ ν¬κΈ°: {size_mb:.1f} MB")
            
            # μ²΄ν¬ν¬μΈνΈ λ΅λ“ ν…μ¤νΈ
            print("π§ UI νΈν™ μ²΄ν¬ν¬μΈνΈ λ΅λ“ ν…μ¤νΈ...")
            test_checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            print("β… UI νΈν™ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ„±κ³µ")
            print(f"   - ν¬ν•¨λ ν‚¤: {list(test_checkpoint.keys())}")
                
        except Exception as e:
            print(f"β μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ‹¤ν¨: {e}")
            import traceback
            traceback.print_exc()
        
        return True


if __name__ == "__main__":
    print("=" * 60)
    print("π† μ„±κ³µν•λ” AI-VAD λ¨λΈ ν•™μµ (Post-processor μ¤λ¥ ν•΄κ²°)")
    print("=" * 60)
    
    success = main()
    if success:
        print("\nπ‰ λ¨λΈ ν…μ¤νΈκ°€ μ„±κ³µμ μΌλ΅ μ™„λ£λμ—μµλ‹λ‹¤!")
        print("μ΄μ  realtime_ui_advanced_windows.pyμ—μ„ μ²΄ν¬ν¬μΈνΈλ¥Ό λ΅λ“ν•  μ μμµλ‹λ‹¤.")
        print("\nμ²΄ν¬ν¬μΈνΈ νμΌ:")
        print("- aivad_ui_ready_checkpoint.ckpt")
        print("\nπ’΅ μ΄ λ²„μ „μ νΉμ§•:")
        print("- Post-processor μ¤λ¥ μ™„μ „ ν•΄κ²°")
        print("- Tensor ν¬κΈ° λ¬Έμ  ν•΄κ²°")
        print("- GPU κ°€μ† μ§€μ›")
        print("- μ»΄ν¬λ„νΈλ³„ ν…μ¤νΈ μν–‰")
        print("- Post-processor μ°ν κΈ°λ¥")
        print("- μ•μ •μ μΈ λ¨λΈ ν•µμ‹¬ ν…μ¤νΈ")
    else:
        print("\nπ’¥ λ¨λΈ ν…μ¤νΈμ— μ‹¤ν¨ν–μµλ‹λ‹¤.")
        print("\nπ“‹ ν•΄κ²° λ°©λ²•:")
        print("1. GPU λ“λΌμ΄λ²„ λ° CUDA μ„¤μΉ ν™•μΈ")
        print("2. PyTorch GPU λ²„μ „ μ„¤μΉ ν™•μΈ")
        print("3. κ΄€λ¦¬μ κ¶ν•μΌλ΅ μ‹¤ν–‰")
        exit(1)
