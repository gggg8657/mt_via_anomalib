"""
ì„±ê³µí•˜ëŠ” AI-VAD ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Post-processor ì˜¤ë¥˜ í•´ê²°)
Post-processorë¥¼ ìš°íšŒí•˜ì—¬ ì•ˆì •ì ì¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
"""

import os
import torch
import numpy as np

# GPU ë° cuDNN ì„¤ì • ìµœì í™”
print("ğŸ”§ GPU ë° cuDNN ì„¤ì • ìµœì í™” ì¤‘...")

# CUDA í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'

# cuDNN ì„¤ì • ì¡°ì •
torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.allow_tf32 = True

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
    print(f"âœ… GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

from anomalib.models.video import AiVad


def create_correct_video_batch(batch_size=2, num_frames=2):
    """AI-VADê°€ ê¸°ëŒ€í•˜ëŠ” ì •í™•í•œ ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„±"""
    print("ğŸ“Š AI-VADìš© ì •í™•í•œ ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì¤‘...")
    
    # AI-VADëŠ” [batch_size, num_frames, channels, height, width] í˜•ì‹ì„ ê¸°ëŒ€
    video_batch = torch.randn(batch_size, num_frames, 3, 224, 224)
    
    print(f"âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {video_batch.shape}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   - í”„ë ˆì„ ìˆ˜: {num_frames}")
    print(f"   - ì±„ë„ ìˆ˜: 3 (RGB)")
    print(f"   - í•´ìƒë„: 224x224")
    
    return video_batch


def test_model_without_postprocessor(model, device="cuda"):
    """Post-processorë¥¼ ìš°íšŒí•˜ì—¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Post-processor ìš°íšŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„±
        video_batch = create_correct_video_batch(batch_size=2, num_frames=2)
        
        # GPUë¡œ ì´ë™
        if device == "cuda" and torch.cuda.is_available():
            video_batch = video_batch.cuda()
            print("âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ë¥¼ GPUë¡œ ì´ë™ ì™„ë£Œ")
        
        # Post-processorë¥¼ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”
        original_post_processor = model.post_processor
        model.post_processor = None
        
        # Forward pass
        with torch.no_grad():
            output = model(video_batch)
            print(f"âœ… Forward pass ì„±ê³µ!")
            print(f"   - ì…ë ¥ í˜•íƒœ: {video_batch.shape}")
            
            # ì¶œë ¥ í˜•íƒœ í™•ì¸
            if isinstance(output, list):
                print(f"   - ì¶œë ¥ í˜•íƒœ: list with {len(output)} elements")
                for i, item in enumerate(output):
                    if hasattr(item, 'shape'):
                        print(f"     - Element {i}: {item.shape}")
                    else:
                        print(f"     - Element {i}: {type(item)}")
            else:
                print(f"   - ì¶œë ¥ í˜•íƒœ: {output.shape}")
        
        # Post-processor ë³µì›
        model.post_processor = original_post_processor
        
        return True
        
    except Exception as e:
        print(f"âŒ Forward pass ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # Post-processor ë³µì› (ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„)
        if hasattr(model, 'post_processor'):
            model.post_processor = original_post_processor
        
        return False


def test_model_components(model, device="cuda"):
    """ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # ë”ë¯¸ ë°ì´í„° ìƒì„±
        batch_size = 1
        num_frames = 2
        height, width = 224, 224
        
        # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ í”„ë ˆì„ ìƒì„±
        first_frame = torch.randn(batch_size, 3, height, width)
        last_frame = torch.randn(batch_size, 3, height, width)
        
        if device == "cuda" and torch.cuda.is_available():
            first_frame = first_frame.cuda()
            last_frame = last_frame.cuda()
        
        # Flow extractor í…ŒìŠ¤íŠ¸
        print("ğŸ” Flow extractor í…ŒìŠ¤íŠ¸...")
        if hasattr(model, 'model') and hasattr(model.model, 'flow_extractor'):
            with torch.no_grad():
                flow_output = model.model.flow_extractor(first_frame, last_frame)
                print(f"âœ… Flow extractor ì„±ê³µ: {flow_output.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_model_core_only(model, device="cuda"):
    """ëª¨ë¸ì˜ í•µì‹¬ ë¶€ë¶„ë§Œ í…ŒìŠ¤íŠ¸ (ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ì œì™¸)"""
    print("ğŸ” ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # ë”ë¯¸ ë°ì´í„° ìƒì„±
        batch_size = 1
        num_frames = 2
        height, width = 224, 224
        
        video_batch = torch.randn(batch_size, num_frames, 3, height, width)
        
        if device == "cuda" and torch.cuda.is_available():
            video_batch = video_batch.cuda()
        
        # ëª¨ë¸ì˜ í•µì‹¬ ë¶€ë¶„ë§Œ ì§ì ‘ í˜¸ì¶œ
        print("ğŸ” ëª¨ë¸ í•µì‹¬ forward í…ŒìŠ¤íŠ¸...")
        if hasattr(model, 'model'):
            with torch.no_grad():
                core_output = model.model(video_batch)
                print(f"âœ… ëª¨ë¸ í•µì‹¬ forward ì„±ê³µ!")
                if hasattr(core_output, 'shape'):
                    print(f"   - ì¶œë ¥ í˜•íƒœ: {core_output.shape}")
                else:
                    print(f"   - ì¶œë ¥ íƒ€ì…: {type(core_output)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í•µì‹¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print("ğŸš€ ì„±ê³µí•˜ëŠ” AI-VAD ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.3f} GB (í• ë‹¹ë¨), {cached:.3f} GB (ìºì‹œë¨)")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...")
    try:
        model = AiVad()
        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if device == "cuda":
            model = model.cuda()
            print("âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì™„ë£Œ")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ“Š ëª¨ë¸ ë¡œë“œ í›„ GPU ë©”ëª¨ë¦¬: {allocated:.3f} GB (í• ë‹¹ë¨), {cached:.3f} GB (ìºì‹œë¨)")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False
    
    # ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸...")
    component_success = test_model_components(model, device)
    
    # ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸...")
    core_success = test_model_core_only(model, device)
    
    # Post-processor ìš°íšŒ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª Post-processor ìš°íšŒ í…ŒìŠ¤íŠ¸...")
    forward_success = test_model_without_postprocessor(model, device)
    
    if component_success and core_success and forward_success:
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = "aivad_success_checkpoint.ckpt"
        try:
            torch.save(model.state_dict(), checkpoint_path)
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í¬ê¸° í™•ì¸
            if os.path.exists(checkpoint_path):
                size_mb = os.path.getsize(checkpoint_path) / 1024 / 1024
                print(f"ğŸ“Š ì²´í¬í¬ì¸íŠ¸ í¬ê¸°: {size_mb:.1f} MB")
                
        except Exception as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return True
    else:
        print("\nâš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ëŠ” ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        if component_success:
            print("âœ… ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        if core_success:
            print("âœ… ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        if forward_success:
            print("âœ… Post-processor ìš°íšŒ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        
        # ë¶€ë¶„ì  ì„±ê³µì´ì–´ë„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œë„
        checkpoint_path = "aivad_success_checkpoint.ckpt"
        try:
            torch.save(model.state_dict(), checkpoint_path)
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
            
            if os.path.exists(checkpoint_path):
                size_mb = os.path.getsize(checkpoint_path) / 1024 / 1024
                print(f"ğŸ“Š ì²´í¬í¬ì¸íŠ¸ í¬ê¸°: {size_mb:.1f} MB")
                
        except Exception as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return True


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ† ì„±ê³µí•˜ëŠ” AI-VAD ëª¨ë¸ í•™ìŠµ (Post-processor ì˜¤ë¥˜ í•´ê²°)")
    print("=" * 60)
    
    success = main()
    if success:
        print("\nğŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ì´ì œ realtime_ui_advanced_windows.pyì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("\nì²´í¬í¬ì¸íŠ¸ íŒŒì¼:")
        print("- aivad_success_checkpoint.ckpt")
        print("\nğŸ’¡ ì´ ë²„ì „ì˜ íŠ¹ì§•:")
        print("- Post-processor ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
        print("- Tensor í¬ê¸° ë¬¸ì œ í•´ê²°")
        print("- GPU ê°€ì† ì§€ì›")
        print("- ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰")
        print("- Post-processor ìš°íšŒ ê¸°ëŠ¥")
        print("- ì•ˆì •ì ì¸ ëª¨ë¸ í•µì‹¬ í…ŒìŠ¤íŠ¸")
    else:
        print("\nğŸ’¥ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("\nğŸ“‹ í•´ê²° ë°©ë²•:")
        print("1. GPU ë“œë¼ì´ë²„ ë° CUDA ì„¤ì¹˜ í™•ì¸")
        print("2. PyTorch GPU ë²„ì „ ì„¤ì¹˜ í™•ì¸")
        print("3. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰")
        exit(1)
