"""
AI-VAD ê¹”ë”í•œ ë²„ì „ìœ¼ë¡œ í•™ìŠµ
êµ¬ë¬¸ ì˜¤ë¥˜ ì—†ëŠ” ê¹”ë”í•œ ì½”ë“œ
"""

import os
import torch
import cv2
import numpy as np
from pathlib import Path
from anomalib.models.video import AiVad

def get_video_files_from_directory(video_dir):
    """ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°"""
    print(f"ğŸ“ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {video_dir}")
    
    if not os.path.exists(video_dir):
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_dir}")
        return []
    
    video_extensions = ['.avi', '.mp4', '.mov', '.mkv', '.flv', '.wmv']
    video_files = []
    
    # ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    for file_path in Path(video_dir).rglob('*'):
        if file_path.is_file() and file_path.suffix.lower() in video_extensions:
            video_files.append(str(file_path))
    
    print(f"âœ… ë°œê²¬ëœ ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
    
    # ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì •ë ¬
    video_files.sort()
    
    return video_files

def process_video_file(video_path, model, device):
    """ë‹¨ì¼ ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
    print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬: {Path(video_path).name}")
    
    try:
        # ë¹„ë””ì˜¤ ë¡œë“œ
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"   âš ï¸ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
            return 0, 0
        
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"   ğŸ“Š í•´ìƒë„: {width}x{height}, í”„ë ˆì„: {frame_count}, FPS: {fps:.1f}")
        
        # 2í”„ë ˆì„ì”© í´ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        clip_count = 0
        detection_count = 0
        frame_buffer = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í”„ë ˆì„ ì „ì²˜ë¦¬
            frame_resized = cv2.resize(frame, (224, 224))
            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
            frame_tensor = torch.from_numpy(frame_rgb).float() / 255.0
            frame_tensor = frame_tensor.permute(2, 0, 1)  # HWC -> CHW
            
            frame_buffer.append(frame_tensor)
            
            # 2í”„ë ˆì„ì´ ëª¨ì´ë©´ í´ë¦½ ì²˜ë¦¬
            if len(frame_buffer) == 2:
                try:
                    # ë¹„ë””ì˜¤ í´ë¦½ ìƒì„± [1, 2, 3, 224, 224]
                    video_clip = torch.stack(frame_buffer).unsqueeze(0).to(device)
                    
                    # AI-VAD ì¶”ë¡ 
                    with torch.no_grad():
                        output = model.model(video_clip)
                        
                        # ì¶œë ¥ êµ¬ì¡° í™•ì¸
                        if isinstance(output, list) and len(output) > 0:
                            # íŠ¹ì„± ì¶”ì¶œ ë° density estimator ì—…ë°ì´íŠ¸
                            if hasattr(model.model, 'density_estimator'):
                                model.model.density_estimator.update(output)
                                detection_count += 1
                            
                            clip_count += 1
                            
                            if clip_count == 1:
                                print(f"   ğŸ‰ ì²« ë²ˆì§¸ ê°ì²´ ê°ì§€ ì„±ê³µ!")
                            elif clip_count % 10 == 0:
                                print(f"   âœ… ì²˜ë¦¬ëœ í´ë¦½: {clip_count}")
                        
                except Exception as e:
                    # ì—ëŸ¬ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                    pass
                
                # ë²„í¼ì—ì„œ ì²« ë²ˆì§¸ í”„ë ˆì„ ì œê±°
                frame_buffer.pop(0)
        
        cap.release()
        print(f"   âœ… ì™„ë£Œ: {clip_count}ê°œ í´ë¦½, {detection_count}ê°œ ê°ì§€")
        return clip_count, detection_count
        
    except Exception as e:
        print(f"   âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return 0, 0

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI-VAD ê¹”ë”í•œ ë²„ì „ìœ¼ë¡œ í•™ìŠµ")
    print("=" * 60)
    print("ğŸ’¡ ê·¹ë‹¨ì ì¸ ì„¤ì •:")
    print("   1. box_score_thresh=0.05 (ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ìŒ)")
    print("   2. min_bbox_area=10 (ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ìŒ)")
    print("   3. max_bbox_overlap=0.95 (ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ìŒ)")
    print("   4. foreground_binary_threshold=2 (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°)")
    print("=" * 60)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # 1. ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ì„¤ì •
    video_directory = "/data/DJ/anomalib_DATAPATH/training_videos"
    print(f"\nğŸ“ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬: {video_directory}")
    
    # 2. ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
    video_files = get_video_files_from_directory(video_directory)
    
    if len(video_files) == 0:
        print("âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # 3. AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ì¸ ì„¤ì •)
    print(f"\nğŸ¤– AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ì¸ ì„¤ì •)...")
    try:
        model = AiVad(
            # AI-VAD ê¸°ë³¸ ì„¤ì •
            use_velocity_features=True,
            use_pose_features=True,
            use_deep_features=True,
            # Density estimation ì„¤ì •
            n_components_velocity=2,
            n_neighbors_pose=1,
            n_neighbors_deep=1,
            # ê·¹ë‹¨ì ì¸ ê°ì²´ ê°ì§€ ì„¤ì •
            box_score_thresh=0.05,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì¶¤
            min_bbox_area=10,       # ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ê²Œ
            max_bbox_overlap=0.95,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ê²Œ
            foreground_binary_threshold=2,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°
        )
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™
        if device == "cuda":
            model = model.to(device)
            print(f"âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (GPU: {device})")
        else:
            print("âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (CPU)")
        
    except Exception as e:
        print(f"âŒ AI-VAD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 4. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    checkpoint_path = "aivad_proper_checkpoint.ckpt"
    if os.path.exists(checkpoint_path):
        print(f"\nğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
                if device == "cuda":
                    model = model.to(device)
                print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 5. AI-VAD í•™ìŠµ (ë°°ì¹˜ ì²˜ë¦¬)
    print(f"\nğŸ¯ AI-VAD í•™ìŠµ ì‹œì‘...")
    
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    model.model.eval()
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
    batch_size = 10
    total_batches = (len(video_files) + batch_size - 1) // batch_size
    
    print(f"ğŸ“Š ì²˜ë¦¬ ê³„íš:")
    print(f"   - ì „ì²´ ë¹„ë””ì˜¤: {len(video_files)}ê°œ")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ")
    print(f"   - ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
    
    total_clips_processed = 0
    total_detections = 0
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(video_files))
        batch_videos = video_files[start_idx:end_idx]
        
        print(f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘...")
        print(f"   - ì²˜ë¦¬ ë²”ìœ„: {start_idx + 1}~{end_idx}")
        
        batch_clips = 0
        batch_detections = 0
        
        for video_path in batch_videos:
            clips, detections = process_video_file(video_path, model, device)
            batch_clips += clips
            batch_detections += detections
        
        total_clips_processed += batch_clips
        total_detections += batch_detections
        
        print(f"\nâœ… ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì™„ë£Œ")
        print(f"   - ë°°ì¹˜ í´ë¦½: {batch_clips}")
        print(f"   - ë°°ì¹˜ ê°ì§€: {batch_detections}")
        print(f"   - ì´ í´ë¦½: {total_clips_processed}")
        print(f"   - ì´ ê°ì§€: {total_detections}")
    
    # 6. Density estimator ìµœì¢… í•™ìŠµ
    print(f"\nğŸ“Š ì „ì²´ ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   - ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {len(video_files)}ê°œ")
    print(f"   - ì²˜ë¦¬ëœ í´ë¦½: {total_clips_processed}ê°œ")
    print(f"   - ì´ ê°ì§€ ìˆ˜: {total_detections}ê°œ")
    
    if hasattr(model.model, 'density_estimator') and total_detections > 0:
        print(f"\nğŸ”§ Density Estimator ìµœì¢… í•™ìŠµ...")
        model.model.density_estimator.fit()
        print("âœ… Density Estimator í•™ìŠµ ì™„ë£Œ")
    else:
        print("âš ï¸ ê°ì§€ëœ íŠ¹ì„±ì´ ì—†ì–´ density estimator í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # 7. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = "aivad_clean_learned.ckpt"
    try:
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'clean_density_estimation',
            'total_clips_processed': total_clips_processed,
            'total_detections': total_detections,
            'video_directory': video_directory,
        }, checkpoint_path)
        
        print(f"ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ AI-VAD ê¹”ë”í•œ í•™ìŠµ ì™„ë£Œ!")
    print("ğŸ’¡ í•™ìŠµëœ ë‚´ìš©:")
    print("1. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("2. ëª¨ë“  ì›€ì§ì„ê³¼ ë³€í™” ê°ì§€")
    print("3. 2064ê°œ ë¹„ë””ì˜¤ ëª¨ë‘ ì²˜ë¦¬")
    print("4. Density Estimatorë¡œ ì´ìƒ íƒì§€ ì¤€ë¹„")
    print("5. UIì—ì„œ 'aivad_clean_learned.ckpt' ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()
