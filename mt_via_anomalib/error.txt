PACKAGE LIST
(mt_p310) PS C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib> pip list
Package                Version
---------------------- -----------
aiofiles               24.1.0
aiohappyeyeballs       2.6.1
aiohttp                3.13.0
aiosignal              1.4.0
annotated-types        0.7.0
anomalib               2.2.0
antlr4-python3-runtime 4.9.3
anyio                  4.11.0
async-timeout          5.0.1
attrs                  25.4.0
av                     16.0.1
Brotli                 1.1.0
certifi                2025.10.5
charset-normalizer     3.4.3
click                  8.3.0
clip                   1.0
colorama               0.4.6
contourpy              1.3.2
cycler                 0.12.1
docstring_parser       0.17.0
einops                 0.8.1
exceptiongroup         1.3.0
faiss-cpu              1.12.0
fastapi                0.119.0
ffmpy                  0.6.3
filelock               3.20.0
fonttools              4.60.1
FrEIA                  0.2
frozenlist             1.8.0
fsspec                 2025.9.0
ftfy                   6.3.1
gradio                 5.49.1
gradio_client          1.13.3
groovy                 0.1.2
h11                    0.16.0
httpcore               1.0.9
httpx                  0.28.1
huggingface-hub        0.35.3
idna                   3.11
imagecodecs            2025.3.30
imageio                2.37.0
importlib_resources    6.5.2
Jinja2                 3.1.6
joblib                 1.5.2
jsonargparse           4.41.0
kiwisolver             1.4.9
kornia                 0.8.1
kornia_rs              0.1.9
lazy_loader            0.4
lightning              2.5.5
lightning-utilities    0.15.2
markdown-it-py         4.0.0
MarkupSafe             3.0.3
matplotlib             3.10.7
mdurl                  0.1.2
mpmath                 1.3.0
multidict              6.7.0
narwhals               2.8.0
networkx               3.4.2
numpy                  2.2.6
omegaconf              2.3.0
opencv-contrib-python  4.12.0.88
opencv-python          4.12.0.88
orjson                 3.11.3
packaging              25.0
pandas                 2.3.3
pillow                 11.3.0
pip                    25.2
plotly                 6.3.1
propcache              0.4.1
pydantic               2.11.10
pydantic_core          2.33.2
pydub                  0.25.1
Pygments               2.19.2
pyparsing              3.2.5
PySide6                6.10.0
PySide6_Addons         6.10.0
PySide6_Essentials     6.10.0
python-dateutil        2.9.0.post0
python-multipart       0.0.20
pytorch-lightning      2.5.5
pytz                   2025.2
PyYAML                 6.0.3
regex                  2025.9.18
requests               2.32.5
rich                   14.2.0
rich-argparse          1.7.1
ruff                   0.14.0
safehttpx              0.1.6
safetensors            0.6.2
scikit-image           0.25.2
scikit-learn           1.7.2
scipy                  1.15.3
semantic-version       2.10.0
setuptools             80.9.0
shellingham            1.5.4
shiboken6              6.10.0
six                    1.17.0
sniffio                1.3.1
starlette              0.48.0
sympy                  1.14.0
threadpoolctl          3.6.0
tifffile               2025.5.10
timm                   1.0.20
tomlkit                0.13.3
torch                  2.8.0
torchaudio             2.8.0
torchmetrics           1.8.2
torchvision            0.23.0
tqdm                   4.67.1
typer                  0.19.2
typeshed_client        2.8.2
typing_extensions      4.15.0
typing-inspection      0.4.2
tzdata                 2025.2
urllib3                2.5.0
uvicorn                0.37.0
wcwidth                0.2.14
websockets             15.0.1
wheel                  0.45.1
yarl                   1.22.0
(mt_p310) PS C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib> conda list
# packages in environment at C:\Users\User\anaconda3\envs\mt_p310:
#
# Name                    Version                   Build  Channel
aiofiles                  24.1.0                   pypi_0    pypi
aiohappyeyeballs          2.6.1                    pypi_0    pypi
aiohttp                   3.13.0                   pypi_0    pypi
aiosignal                 1.4.0                    pypi_0    pypi
annotated-types           0.7.0                    pypi_0    pypi
anomalib                  2.2.0                    pypi_0    pypi
antlr4-python3-runtime    4.9.3                    pypi_0    pypi
anyio                     4.11.0                   pypi_0    pypi
async-timeout             5.0.1                    pypi_0    pypi
attrs                     25.4.0                   pypi_0    pypi
av                        16.0.1                   pypi_0    pypi
brotli                    1.1.0                    pypi_0    pypi
bzip2                     1.0.8                h2bbff1b_6
ca-certificates           2025.9.9             haa95532_0
certifi                   2025.10.5                pypi_0    pypi
charset-normalizer        3.4.3                    pypi_0    pypi
click                     8.3.0                    pypi_0    pypi
clip                      1.0                      pypi_0    pypi
colorama                  0.4.6                    pypi_0    pypi
contourpy                 1.3.2                    pypi_0    pypi
cycler                    0.12.1                   pypi_0    pypi
docstring-parser          0.17.0                   pypi_0    pypi
einops                    0.8.1                    pypi_0    pypi
exceptiongroup            1.3.0                    pypi_0    pypi
expat                     2.7.1                h8ddb27b_0
faiss-cpu                 1.12.0                   pypi_0    pypi
fastapi                   0.119.0                  pypi_0    pypi
ffmpy                     0.6.3                    pypi_0    pypi
filelock                  3.20.0                   pypi_0    pypi
fonttools                 4.60.1                   pypi_0    pypi
freia                     0.2                      pypi_0    pypi
frozenlist                1.8.0                    pypi_0    pypi
fsspec                    2025.9.0                 pypi_0    pypi
ftfy                      6.3.1                    pypi_0    pypi
gradio                    5.49.1                   pypi_0    pypi
gradio-client             1.13.3                   pypi_0    pypi
groovy                    0.1.2                    pypi_0    pypi
h11                       0.16.0                   pypi_0    pypi
httpcore                  1.0.9                    pypi_0    pypi
httpx                     0.28.1                   pypi_0    pypi
huggingface-hub           0.35.3                   pypi_0    pypi
idna                      3.11                     pypi_0    pypi
imagecodecs               2025.3.30                pypi_0    pypi
imageio                   2.37.0                   pypi_0    pypi
importlib-resources       6.5.2                    pypi_0    pypi
jinja2                    3.1.6                    pypi_0    pypi
joblib                    1.5.2                    pypi_0    pypi
jsonargparse              4.41.0                   pypi_0    pypi
kiwisolver                1.4.9                    pypi_0    pypi
kornia                    0.8.1                    pypi_0    pypi
kornia-rs                 0.1.9                    pypi_0    pypi
lazy-loader               0.4                      pypi_0    pypi
libffi                    3.4.4                hd77b12b_1
libzlib                   1.3.1                h02ab6af_0
lightning                 2.5.5                    pypi_0    pypi
lightning-utilities       0.15.2                   pypi_0    pypi
markdown-it-py            4.0.0                    pypi_0    pypi
markupsafe                3.0.3                    pypi_0    pypi
matplotlib                3.10.7                   pypi_0    pypi
mdurl                     0.1.2                    pypi_0    pypi
mpmath                    1.3.0                    pypi_0    pypi
multidict                 6.7.0                    pypi_0    pypi
narwhals                  2.8.0                    pypi_0    pypi
networkx                  3.4.2                    pypi_0    pypi
numpy                     2.2.6                    pypi_0    pypi
omegaconf                 2.3.0                    pypi_0    pypi
opencv-contrib-python     4.12.0.88                pypi_0    pypi
opencv-python             4.12.0.88                pypi_0    pypi
openssl                   3.0.18               h543e019_0
orjson                    3.11.3                   pypi_0    pypi
packaging                 25.0                     pypi_0    pypi
pandas                    2.3.3                    pypi_0    pypi
pillow                    11.3.0                   pypi_0    pypi
pip                       25.2               pyhc872135_0
plotly                    6.3.1                    pypi_0    pypi
propcache                 0.4.1                    pypi_0    pypi
pydantic                  2.11.10                  pypi_0    pypi
pydantic-core             2.33.2                   pypi_0    pypi
pydub                     0.25.1                   pypi_0    pypi
pygments                  2.19.2                   pypi_0    pypi
pyparsing                 3.2.5                    pypi_0    pypi
pyside6                   6.10.0                   pypi_0    pypi
pyside6-addons            6.10.0                   pypi_0    pypi
pyside6-essentials        6.10.0                   pypi_0    pypi
python                    3.10.18              h981015d_0
python-dateutil           2.9.0.post0              pypi_0    pypi
python-multipart          0.0.20                   pypi_0    pypi
pytorch-lightning         2.5.5                    pypi_0    pypi
pytz                      2025.2                   pypi_0    pypi
pyyaml                    6.0.3                    pypi_0    pypi
regex                     2025.9.18                pypi_0    pypi
requests                  2.32.5                   pypi_0    pypi
rich                      14.2.0                   pypi_0    pypi
rich-argparse             1.7.1                    pypi_0    pypi
ruff                      0.14.0                   pypi_0    pypi
safehttpx                 0.1.6                    pypi_0    pypi
safetensors               0.6.2                    pypi_0    pypi
scikit-image              0.25.2                   pypi_0    pypi
scikit-learn              1.7.2                    pypi_0    pypi
scipy                     1.15.3                   pypi_0    pypi
semantic-version          2.10.0                   pypi_0    pypi
setuptools                80.9.0          py310haa95532_0
shellingham               1.5.4                    pypi_0    pypi
shiboken6                 6.10.0                   pypi_0    pypi
six                       1.17.0                   pypi_0    pypi
sniffio                   1.3.1                    pypi_0    pypi
sqlite                    3.50.2               hda9a48d_1
starlette                 0.48.0                   pypi_0    pypi
sympy                     1.14.0                   pypi_0    pypi
threadpoolctl             3.6.0                    pypi_0    pypi
tifffile                  2025.5.10                pypi_0    pypi
timm                      1.0.20                   pypi_0    pypi
tk                        8.6.15               hf199647_0
tomlkit                   0.13.3                   pypi_0    pypi
torch                     2.8.0                    pypi_0    pypi
torchaudio                2.8.0                    pypi_0    pypi
torchmetrics              1.8.2                    pypi_0    pypi
torchvision               0.23.0                   pypi_0    pypi
tqdm                      4.67.1                   pypi_0    pypi
typer                     0.19.2                   pypi_0    pypi
typeshed-client           2.8.2                    pypi_0    pypi
typing-extensions         4.15.0                   pypi_0    pypi
typing-inspection         0.4.2                    pypi_0    pypi
tzdata                    2025.2                   pypi_0    pypi
ucrt                      10.0.22621.0         haa95532_0
urllib3                   2.5.0                    pypi_0    pypi
uvicorn                   0.37.0                   pypi_0    pypi
vc                        14.3                h2df5915_10
vc14_runtime              14.44.35208         h4927774_10
vs2015_runtime            14.44.35208         ha6b5a95_10
wcwidth                   0.2.14                   pypi_0    pypi
websockets                15.0.1                   pypi_0    pypi
wheel                     0.45.1          py310haa95532_0
xz                        5.6.4                h4754444_1
yarl                      1.22.0                   pypi_0    pypi
zlib                      1.3.1                h02ab6af_0


""ERROR LOGS""
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_custom_ultimate.py
============================================================
ğŸ† ê¶ê·¹ì˜ ì•ˆì • ë²„ì „ - ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ AI-VAD í•™ìŠµ
============================================================
ğŸš€ ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ AI-VAD ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ê¶ê·¹ì˜ ì•ˆì • ë²„ì „)...
ğŸ“ ì»¤ìŠ¤í…€ Avenue í˜•ì‹ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...
âœ… ê¸°ì¡´ ë°ì´í„°ì…‹ ì •ë¦¬ ì™„ë£Œ
âœ… ë³µì‚¬ ì™„ë£Œ: normal_video.mp4 -> 01.avi
âœ… ë³µì‚¬ ì™„ë£Œ: unknown_video.mp4 -> 02.avi
ğŸ“ í…ŒìŠ¤íŠ¸ìš© ë¹„ë””ì˜¤ ë³µì‚¬ ì¤‘...
âœ… í…ŒìŠ¤íŠ¸ ë³µì‚¬: 01.avi
âœ… í…ŒìŠ¤íŠ¸ ë³µì‚¬: 02.avi
ğŸ“ Avenue ground truth êµ¬ì¡° ìƒì„± ì¤‘...
âœ… Ground truth ìƒì„±: 1_label (100ê°œ ë§ˆìŠ¤í¬)
âœ… Ground truth ìƒì„±: 2_label (100ê°œ ë§ˆìŠ¤í¬)
âœ… Avenue ground truth êµ¬ì¡° ìƒì„± ì™„ë£Œ
âœ… Avenue í˜•ì‹ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: 2ê°œ íŒŒì¼ ì²˜ë¦¬ë¨
CPU ì‚¬ìš©
ğŸ“ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...
ğŸ“‚ ë°ì´í„°ì…‹ ê²½ë¡œ: C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\custom_avenue_ultimate
ğŸ“‚ Ground truth ê²½ë¡œ: C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\custom_avenue_ultimate\ground_truth_demo
âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ
ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...
âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ
âš™ï¸  í•™ìŠµ ì—”ì§„ ì„¤ì •...
âœ… í•™ìŠµ ì—”ì§„ ì„¤ì • ì™„ë£Œ
ğŸ¯ í•™ìŠµ ì‹œì‘!
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
  0%|                                                                                            | 0/1 [00:00<?, ?it/s]C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\io\_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.95it/s]
  0%|                                                                                            | 0/1 [00:00<?, ?it/s]C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\io\_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.67it/s]
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\core\optimizer.py:183: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name           | Type          | Params | Mode
---------------------------------------------------------
0 | pre_processor  | PreProcessor  | 0      | train
1 | post_processor | PostProcessor | 0      | train
2 | evaluator      | Evaluator     | 0      | train
3 | model          | AiVadModel    | 260 M  | train
---------------------------------------------------------
259 M     Trainable params
447 K     Non-trainable params
260 M     Total params
1,041.500 Total estimated model params size (MB)
670       Modules in train mode
227       Modules in eval mode
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py:527: Found 227 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.
Epoch 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                           | 2/10 [00:31<02:04,  0.06it/Epoch 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                       | 2/10 [36:26<2:25:45,  0.00it/s] Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [38:21<00:00,  0.00it/s] âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: Caught IndexError in DataLoader worker process 0.                       | 0/? [00:00<?, ?it/s]
Original Traceback (most recent call last):
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\base\video.py", line 207, in __getitem__
    item = self.indexer.get_item(index)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\utils\video.py", line 132, in get_item
    gt_mask=self.get_mask(idx),
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in get_mask
    mask_paths = [mask_frames[idx] for idx in frames.int()]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in <listcomp>
    mask_paths = [mask_frames[idx] for idx in frames.int()]
IndexError: list index out of range

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_custom_ultimate.py", line 285, in main
    engine.fit(model=model, datamodule=datamodule)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\engine\engine.py", line 412, in fit
    self.trainer.fit(model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1011, in _run
    results = self._run_stage()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 216, in run
    self.advance()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 138, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fetchers.py", line 134, in __next__
    batch = super().__next__()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fetchers.py", line 61, in __next__
    batch = next(self.iterator)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\utilities\combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\utilities\combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\dataloader.py", line 734, in __next__
    data = self._next_data()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\dataloader.py", line 1516, in _next_data
    return self._process_data(data, worker_id)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\dataloader.py", line 1551, in _process_data
    data.reraise()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\_utils.py", line 769, in reraise
    raise exception
IndexError: Caught IndexError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\base\video.py", line 207, in __getitem__
    item = self.indexer.get_item(index)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\utils\video.py", line 132, in get_item
    gt_mask=self.get_mask(idx),
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in get_mask
    mask_paths = [mask_frames[idx] for idx in frames.int()]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in <listcomp>
    mask_paths = [mask_frames[idx] for idx in frames.int()]
IndexError: list index out of range


ğŸ’¥ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.

ğŸ“‹ í•´ê²° ë°©ë²•:
1. video_files ë¦¬ìŠ¤íŠ¸ì— ì˜¬ë°”ë¥¸ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ê°€í•˜ì„¸ìš”
2. ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
3. ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš” (.mp4, .avi, .mov, .mkv, .flv, .wmv)
4. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [38:31<00:00,  0.00it/s]

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>


---- 2025.10.15. 19:19 ---

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_custom_ultimate.py
============================================================
ğŸ† ê¶ê·¹ì˜ ì•ˆì • ë²„ì „ - ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ AI-VAD í•™ìŠµ
============================================================
ğŸš€ ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ AI-VAD ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ê¶ê·¹ì˜ ì•ˆì • ë²„ì „)...
ğŸ“ ì»¤ìŠ¤í…€ Avenue í˜•ì‹ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...
âœ… ê¸°ì¡´ ë°ì´í„°ì…‹ ì •ë¦¬ ì™„ë£Œ
âœ… ë³µì‚¬ ì™„ë£Œ: normal_video.mp4 -> 01.avi
âœ… ë³µì‚¬ ì™„ë£Œ: unknown_video.mp4 -> 02.avi
ğŸ“ í…ŒìŠ¤íŠ¸ìš© ë¹„ë””ì˜¤ ë³µì‚¬ ì¤‘...
âœ… í…ŒìŠ¤íŠ¸ ë³µì‚¬: 01.avi
âœ… í…ŒìŠ¤íŠ¸ ë³µì‚¬: 02.avi
ğŸ“ Avenue ground truth êµ¬ì¡° ìƒì„± ì¤‘...
âœ… Ground truth ìƒì„±: 1_label (100ê°œ ë§ˆìŠ¤í¬)
âœ… Ground truth ìƒì„±: 2_label (100ê°œ ë§ˆìŠ¤í¬)
âœ… Avenue ground truth êµ¬ì¡° ìƒì„± ì™„ë£Œ
âœ… Avenue í˜•ì‹ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: 2ê°œ íŒŒì¼ ì²˜ë¦¬ë¨
GPU ì‚¬ìš©: NVIDIA GeForce RTX 4090
GPU ë©”ëª¨ë¦¬: 24.0 GB
ğŸ“ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...
ğŸ“‚ ë°ì´í„°ì…‹ ê²½ë¡œ: C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\custom_avenue_ultimate
ğŸ“‚ Ground truth ê²½ë¡œ: C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\custom_avenue_ultimate\ground_truth_demo
âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ
ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...
âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ
âš™ï¸  í•™ìŠµ ì—”ì§„ ì„¤ì •...
âœ… í•™ìŠµ ì—”ì§„ ì„¤ì • ì™„ë£Œ
ğŸ¯ í•™ìŠµ ì‹œì‘!
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
  0%|                                                                                         | 0/1 [00:00<?, ?it/s]C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\io\_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.54it/s]
  0%|                                                                                         | 0/1 [00:00<?, ?it/s]C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\io\_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.28it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\core\optimizer.py:183: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name           | Type          | Params | Mode
---------------------------------------------------------
0 | pre_processor  | PreProcessor  | 0      | train
1 | post_processor | PostProcessor | 0      | train
2 | evaluator      | Evaluator     | 0      | train
3 | model          | AiVadModel    | 260 M  | train
---------------------------------------------------------
259 M     Trainable params
447 K     Non-trainable params
260 M     Total params
1,041.500 Total estimated model params size (MB)
670       Modules in train mode
227       Modules in eval mode
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py:527: Found 227 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  0.92it/s] âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: Caught IndexError in DataLoader worker process 0.                       | 0/? [00:00<?, ?it/s]
Original Traceback (most recent call last):
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\base\video.py", line 207, in __getitem__
    item = self.indexer.get_item(index)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\utils\video.py", line 132, in get_item
    gt_mask=self.get_mask(idx),
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in get_mask
    mask_paths = [mask_frames[idx] for idx in frames.int()]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in <listcomp>
    mask_paths = [mask_frames[idx] for idx in frames.int()]
IndexError: list index out of range

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_custom_ultimate.py", line 285, in main
    engine.fit(model=model, datamodule=datamodule)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\engine\engine.py", line 412, in fit
    self.trainer.fit(model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1011, in _run
    results = self._run_stage()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 216, in run
    self.advance()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 138, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fetchers.py", line 134, in __next__
    batch = super().__next__()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fetchers.py", line 61, in __next__
    batch = next(self.iterator)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\utilities\combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\utilities\combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\dataloader.py", line 733, in __next__
    data = self._next_data()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\dataloader.py", line 1515, in _next_data
    return self._process_data(data, worker_id)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\dataloader.py", line 1550, in _process_data
    data.reraise()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\_utils.py", line 750, in reraise
    raise exception
IndexError: Caught IndexError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\base\video.py", line 207, in __getitem__
    item = self.indexer.get_item(index)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\utils\video.py", line 132, in get_item
    gt_mask=self.get_mask(idx),
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in get_mask
    mask_paths = [mask_frames[idx] for idx in frames.int()]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 203, in <listcomp>
    mask_paths = [mask_frames[idx] for idx in frames.int()]
IndexError: list index out of range


ğŸ’¥ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.

ğŸ“‹ í•´ê²° ë°©ë²•:
1. video_files ë¦¬ìŠ¤íŠ¸ì— ì˜¬ë°”ë¥¸ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ê°€í•˜ì„¸ìš”
2. ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
3. ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš” (.mp4, .avi, .mov, .mkv, .flv, .wmv)
4. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  0.47it/s]

--- ISSUE 1 : no gpu memory allocated ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_custom_final_fixed.py
ğŸ”§ GPU ë° cuDNN ì„¤ì • ìµœì í™” ì¤‘...
âœ… GPU ì‚¬ìš© ê°€ëŠ¥: NVIDIA GeForce RTX 4090
âœ… GPU ë©”ëª¨ë¦¬: 24.0 GB
============================================================
ğŸ† ìµœì¢… ì™„ë²½ ìˆ˜ì • ë²„ì „ - ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ AI-VAD í•™ìŠµ
============================================================
ğŸš€ ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ AI-VAD ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ìµœì¢… ì™„ë²½ ìˆ˜ì • ë²„ì „)...
âœ… video_files_listì—ì„œ 20ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ë¡œë“œ
ğŸ” GPU ìƒíƒœ ì§„ë‹¨ ì¤‘...
CUDA ê°€ìš©ì„±: True
GPU ê°œìˆ˜: 2
GPU 0: NVIDIA GeForce RTX 4090 (24.0 GB)
GPU 1: NVIDIA GeForce RTX 4090 (24.0 GB)
âœ… GPU ì‚¬ìš© ì„¤ì •: NVIDIA GeForce RTX 4090
GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.0 GB (í• ë‹¹ë¨), 0.0 GB (ìºì‹œë¨)
ğŸ“ ê°„ë‹¨í•œ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...
ğŸ“ ê°„ë‹¨í•œ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_0.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_1.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_2.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_3.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_4.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_5.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_6.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_7.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_8.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_9.avi (1000 í”„ë ˆì„)


--- ISSUE 2 : tensor size ---

ğŸš€ ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ AI-VAD ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ìµœì¢… ì™„ë²½ ìˆ˜ì • ë²„ì „)...
âœ… video_files_listì—ì„œ 20ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ë¡œë“œ
ğŸ” GPU ìƒíƒœ ì§„ë‹¨ ì¤‘...
CUDA ê°€ìš©ì„±: True
GPU ê°œìˆ˜: 2
GPU 0: NVIDIA GeForce RTX 4090 (24.0 GB)
GPU 1: NVIDIA GeForce RTX 4090 (24.0 GB)
âœ… GPU ì‚¬ìš© ì„¤ì •: NVIDIA GeForce RTX 4090
GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.0 GB (í• ë‹¹ë¨), 0.0 GB (ìºì‹œë¨)
ğŸ“ ê°„ë‹¨í•œ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...
ğŸ“ ê°„ë‹¨í•œ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_0.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_1.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_2.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_3.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_4.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_5.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_6.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_7.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_8.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_9.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_10.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_11.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_12.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_13.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_14.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_15.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_16.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_17.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_18.avi (1000 í”„ë ˆì„)
âœ… ë¹„ë””ì˜¤ ë¡œë“œ: normal_19.avi (478 í”„ë ˆì„)
âœ… 20ê°œ ë¹„ë””ì˜¤ ë¡œë“œ ì™„ë£Œ
ğŸ“ ê°„ë‹¨í•œ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...
ğŸ“Š ë¹„ë””ì˜¤ 1: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_0.avi -> video_01.mp4
ğŸ“Š ë¹„ë””ì˜¤ 2: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_1.avi -> video_02.mp4
ğŸ“Š ë¹„ë””ì˜¤ 3: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_2.avi -> video_03.mp4
ğŸ“Š ë¹„ë””ì˜¤ 4: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_3.avi -> video_04.mp4
ğŸ“Š ë¹„ë””ì˜¤ 5: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_4.avi -> video_05.mp4
ğŸ“Š ë¹„ë””ì˜¤ 6: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_5.avi -> video_06.mp4
ğŸ“Š ë¹„ë””ì˜¤ 7: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_6.avi -> video_07.mp4
ğŸ“Š ë¹„ë””ì˜¤ 8: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_7.avi -> video_08.mp4
ğŸ“Š ë¹„ë””ì˜¤ 9: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_8.avi -> video_09.mp4
ğŸ“Š ë¹„ë””ì˜¤ 10: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_9.avi -> video_10.mp4
ğŸ“Š ë¹„ë””ì˜¤ 11: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_10.avi -> video_11.mp4
ğŸ“Š ë¹„ë””ì˜¤ 12: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_11.avi -> video_12.mp4
ğŸ“Š ë¹„ë””ì˜¤ 13: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_12.avi -> video_13.mp4
ğŸ“Š ë¹„ë””ì˜¤ 14: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_13.avi -> video_14.mp4
ğŸ“Š ë¹„ë””ì˜¤ 15: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_14.avi -> video_15.mp4
ğŸ“Š ë¹„ë””ì˜¤ 16: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_15.avi -> video_16.mp4
ğŸ“Š ë¹„ë””ì˜¤ 17: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_16.avi -> video_17.mp4
ğŸ“Š ë¹„ë””ì˜¤ 18: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_17.avi -> video_18.mp4
ğŸ“Š ë¹„ë””ì˜¤ 19: 1000 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_18.avi -> video_19.mp4
ğŸ“Š ë¹„ë””ì˜¤ 20: 478 í”„ë ˆì„
âœ… ë³µì‚¬ ì™„ë£Œ: normal_19.avi -> video_20.mp4
âœ… ê°„ë‹¨í•œ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: 20ê°œ íŒŒì¼
ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...
âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ
âš™ï¸  ê°„ë‹¨í•œ í•™ìŠµ ì„¤ì •...
ğŸš€ GPU ê°€ì† í•™ìŠµ ì—”ì§„ ì„¤ì •...
âœ… ê°„ë‹¨í•œ í•™ìŠµ ì—”ì§„ ì„¤ì • ì™„ë£Œ
ğŸ¯ ê°„ë‹¨í•œ í•™ìŠµ ì‹œì‘!
âš ï¸  ë°ì´í„° ëª¨ë“ˆ ì—†ì´ ê¸°ë³¸ ëª¨ë¸ í•™ìŠµì„ ì‹œë„í•©ë‹ˆë‹¤...
âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_custom_final_fixed.py", line 304, in main
    output = model(dummy_data)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\components\base\anomalib_module.py", line 200, in forward
    batch = self.model(batch)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 192, in forward
    flows = self.flow_extractor(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\flow.py", line 71, in forward
    first_frame, last_frame = self.pre_process(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\flow.py", line 56, in pre_process
    return self.transforms(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\transforms\_presets.py", line 201, in forward
    img1 = F.normalize(img1, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\transforms\functional.py", line 350, in normalize
    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\transforms\_functional_tensor.py", line 928, in normalize
    return tensor.sub_(mean).div_(std)
RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0

ğŸ’¥ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.

--- ISSUE : ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_working.py
ğŸ”§ GPU ë° cuDNN ì„¤ì • ìµœì í™” ì¤‘...
âœ… GPU ì‚¬ìš© ê°€ëŠ¥: NVIDIA GeForce RTX 4090
âœ… GPU ë©”ëª¨ë¦¬: 24.0 GB
============================================================
ğŸ† ì‘ë™í•˜ëŠ” AI-VAD ëª¨ë¸ í•™ìŠµ (Tensor í¬ê¸° ë¬¸ì œ í•´ê²°)
============================================================
ğŸš€ ì‘ë™í•˜ëŠ” AI-VAD ëª¨ë¸ í•™ìŠµ ì‹œì‘...
ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.000 GB (í• ë‹¹ë¨), 0.000 GB (ìºì‹œë¨)
ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...
âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ
âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì™„ë£Œ
ğŸ“Š ëª¨ë¸ ë¡œë“œ í›„ GPU ë©”ëª¨ë¦¬: 0.755 GB (í• ë‹¹ë¨), 0.766 GB (ìºì‹œë¨)

ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸...
ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ” Flow extractor í…ŒìŠ¤íŠ¸...
âœ… Flow extractor ì„±ê³µ: torch.Size([1, 2, 224, 224])

ğŸ§ª ëª¨ë¸ forward pass í…ŒìŠ¤íŠ¸...
ğŸ§ª ëª¨ë¸ forward pass í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ“Š AI-VADìš© ì •í™•í•œ ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì¤‘...
âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì™„ë£Œ: torch.Size([2, 2, 3, 224, 224])
   - ë°°ì¹˜ í¬ê¸°: 2
   - í”„ë ˆì„ ìˆ˜: 2
   - ì±„ë„ ìˆ˜: 3 (RGB)
   - í•´ìƒë„: 224x224
âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ë¥¼ GPUë¡œ ì´ë™ ì™„ë£Œ
âŒ Forward pass ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_aivad_working.py", line 70, in test_model_forward_pass
    output = model(video_batch)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\components\base\anomalib_module.py", line 201, in forward
    return self.post_processor(batch) if self.post_processor else batch
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\post_processing\post_processor.py", line 210, in forward
    if predictions.pred_score is None and predictions.anomaly_map is None:
AttributeError: 'list' object has no attribute 'pred_score'

âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨

ğŸ’¥ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.

ğŸ“‹ í•´ê²° ë°©ë²•:
1. GPU ë“œë¼ì´ë²„ ë° CUDA ì„¤ì¹˜ í™•ì¸
2. PyTorch GPU ë²„ì „ ì„¤ì¹˜ í™•ì¸
3. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰

--- training success ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_success.py
ğŸ”§ GPU ë° cuDNN ì„¤ì • ìµœì í™” ì¤‘...
âœ… GPU ì‚¬ìš© ê°€ëŠ¥: NVIDIA GeForce RTX 4090
âœ… GPU ë©”ëª¨ë¦¬: 24.0 GB
============================================================
ğŸ† ì„±ê³µí•˜ëŠ” AI-VAD ëª¨ë¸ í•™ìŠµ (Post-processor ì˜¤ë¥˜ í•´ê²°)
============================================================
ğŸš€ ì„±ê³µí•˜ëŠ” AI-VAD ëª¨ë¸ í•™ìŠµ ì‹œì‘...
ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.000 GB (í• ë‹¹ë¨), 0.000 GB (ìºì‹œë¨)
ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...
âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ
âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì™„ë£Œ
ğŸ“Š ëª¨ë¸ ë¡œë“œ í›„ GPU ë©”ëª¨ë¦¬: 0.755 GB (í• ë‹¹ë¨), 0.766 GB (ìºì‹œë¨)

ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸...
ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ” Flow extractor í…ŒìŠ¤íŠ¸...
âœ… Flow extractor ì„±ê³µ: torch.Size([1, 2, 224, 224])

ğŸ” ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸...
ğŸ” ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ” ëª¨ë¸ í•µì‹¬ forward í…ŒìŠ¤íŠ¸...
âœ… ëª¨ë¸ í•µì‹¬ forward ì„±ê³µ!
   - ì¶œë ¥ íƒ€ì…: <class 'list'>

ğŸ§ª Post-processor ìš°íšŒ í…ŒìŠ¤íŠ¸...
ğŸ§ª Post-processor ìš°íšŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ“Š AI-VADìš© ì •í™•í•œ ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì¤‘...
âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì™„ë£Œ: torch.Size([2, 2, 3, 224, 224])
   - ë°°ì¹˜ í¬ê¸°: 2
   - í”„ë ˆì„ ìˆ˜: 2
   - ì±„ë„ ìˆ˜: 3 (RGB)
   - í•´ìƒë„: 224x224
âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ë¥¼ GPUë¡œ ì´ë™ ì™„ë£Œ
âœ… Forward pass ì„±ê³µ!
   - ì…ë ¥ í˜•íƒœ: torch.Size([2, 2, 3, 224, 224])
   - ì¶œë ¥ í˜•íƒœ: list with 2 elements
     - Element 0: <class 'dict'>
     - Element 1: <class 'dict'>

âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!
ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: aivad_success_checkpoint.ckpt
ğŸ“Š ì²´í¬í¬ì¸íŠ¸ í¬ê¸°: 757.9 MB

ğŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
ì´ì œ realtime_ui_advanced_windows.pyì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì²´í¬í¬ì¸íŠ¸ íŒŒì¼:
- aivad_success_checkpoint.ckpt

ğŸ’¡ ì´ ë²„ì „ì˜ íŠ¹ì§•:
- Post-processor ì˜¤ë¥˜ ì™„ì „ í•´ê²°
- Tensor í¬ê¸° ë¬¸ì œ í•´ê²°
- GPU ê°€ì† ì§€ì›
- ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
- Post-processor ìš°íšŒ ê¸°ëŠ¥
- ì•ˆì •ì ì¸ ëª¨ë¸ í•µì‹¬ í…ŒìŠ¤íŠ¸


--- new issue UI ADAPTATION ---

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>pyton realtime_ui_advanced_windows.py
'pyton'ì€(ëŠ”) ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ ëª…ë ¹, ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨, ë˜ëŠ”
ë°°ì¹˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python realtime_ui_advanced_windows.py
C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py:651: DeprecationWarning: Enum value 'Qt::ApplicationAttribute.AA_EnableHighDpiScaling' is marked as deprecated, please check the documentation for more information.
  QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling, True)
C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py:652: DeprecationWarning: Enum value 'Qt::ApplicationAttribute.AA_UseHighDpiPixmaps' is marked as deprecated, please check the documentation for more information.
  QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_UseHighDpiPixmaps, True)
ìœˆë„ìš°ì¦ˆ GPU ì‚¬ìš©: NVIDIA GeForce RTX 4090
ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: 'pytorch-lightning_version'

--- ISSUE ---

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python create_ui_checkpoint.py
ğŸ”§ GPU ì„¤ì • ì¤‘...
âœ… GPU ì‚¬ìš©: NVIDIA GeForce RTX 4090
============================================================
ğŸ† UI í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
============================================================
ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...
âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì™„ë£Œ
âŒ ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨: module 'pytorch_lightning' has no attribute 'save_checkpoint'
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\create_ui_checkpoint.py", line 38, in create_ui_compatible_checkpoint
    pl.save_checkpoint(model, checkpoint_path)
AttributeError: module 'pytorch_lightning' has no attribute 'save_checkpoint'

ğŸ’¥ ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨


--- ISSUE 2025.10.15. 20:07 ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python create_ui_checkpoint.py
ğŸ”§ GPU ì„¤ì • ì¤‘...
âœ… GPU ì‚¬ìš©: NVIDIA GeForce RTX 4090
============================================================
ğŸ† UI í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
============================================================
ğŸ¤– AI-VAD ëª¨ë¸ ì´ˆê¸°í™”...
ğŸ“Š PyTorch Lightning ë²„ì „: 2.5.5
âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì™„ë£Œ
ğŸ’¾ UI í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: aivad_ui_compatible_checkpoint.ckpt
ğŸ“Š ì²´í¬í¬ì¸íŠ¸ í¬ê¸°: 757.9 MB
ğŸ” ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© í™•ì¸:
âŒ ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.
        (1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
        (2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
        WeightsUnpickler error: Unsupported global: GLOBAL lightning.fabric.utilities.data.AttributeDict was not an allowed global by default. Please use `torch.serialization.add_safe_globals([lightning.fabric.utilities.data.AttributeDict])` or the `torch.serialization.safe_globals([lightning.fabric.utilities.data.AttributeDict])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\create_ui_checkpoint.py", line 59, in create_ui_compatible_checkpoint
    loaded_checkpoint = torch.load(checkpoint_path, map_location='cpu')
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.
        (1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
        (2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
        WeightsUnpickler error: Unsupported global: GLOBAL lightning.fabric.utilities.data.AttributeDict was not an allowed global by default. Please use `torch.serialization.add_safe_globals([lightning.fabric.utilities.data.AttributeDict])` or the `torch.serialization.safe_globals([lightning.fabric.utilities.data.AttributeDict])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.

ğŸ’¥ ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨

-- ISSUE 2025.10.15. 20:16 --

ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸...
ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ” Flow extractor í…ŒìŠ¤íŠ¸...
âœ… Flow extractor ì„±ê³µ: torch.Size([1, 2, 224, 224])

ğŸ” ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸...
ğŸ” ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ” ëª¨ë¸ í•µì‹¬ forward í…ŒìŠ¤íŠ¸...
âœ… ëª¨ë¸ í•µì‹¬ forward ì„±ê³µ!
   - ì¶œë ¥ íƒ€ì…: <class 'list'>

ğŸ§ª Post-processor ìš°íšŒ í…ŒìŠ¤íŠ¸...
ğŸ§ª Post-processor ìš°íšŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...
ğŸ“Š AI-VADìš© ì •í™•í•œ ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì¤‘...
âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ ìƒì„± ì™„ë£Œ: torch.Size([2, 2, 3, 224, 224])
   - ë°°ì¹˜ í¬ê¸°: 2
   - í”„ë ˆì„ ìˆ˜: 2
   - ì±„ë„ ìˆ˜: 3 (RGB)
   - í•´ìƒë„: 224x224
âœ… ë¹„ë””ì˜¤ ë°°ì¹˜ë¥¼ GPUë¡œ ì´ë™ ì™„ë£Œ
âœ… Forward pass ì„±ê³µ!
   - ì…ë ¥ í˜•íƒœ: torch.Size([2, 2, 3, 224, 224])
   - ì¶œë ¥ í˜•íƒœ: list with 2 elements
     - Element 0: <class 'dict'>
     - Element 1: <class 'dict'>

âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!
ğŸ’¾ UI í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: aivad_ui_ready_checkpoint.ckpt
ğŸ“Š ì²´í¬í¬ì¸íŠ¸ í¬ê¸°: 757.9 MB
ğŸ§ª UI í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...
âœ… UI í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ
   - í¬í•¨ëœ í‚¤: ['state_dict', 'pytorch-lightning_version', 'model_class']

ğŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
ì´ì œ realtime_ui_advanced_windows.pyì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì²´í¬í¬ì¸íŠ¸ íŒŒì¼:
- aivad_ui_ready_checkpoint.ckpt

ğŸ’¡ ì´ ë²„ì „ì˜ íŠ¹ì§•:
- Post-processor ì˜¤ë¥˜ ì™„ì „ í•´ê²°
- Tensor í¬ê¸° ë¬¸ì œ í•´ê²°
- GPU ê°€ì† ì§€ì›
- ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
- Post-processor ìš°íšŒ ê¸°ëŠ¥
- ì•ˆì •ì ì¸ ëª¨ë¸ í•µì‹¬ í…ŒìŠ¤íŠ¸

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python realtime_ui_advanced_windows.py
C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py:651: DeprecationWarning: Enum value 'Qt::ApplicationAttribute.AA_EnableHighDpiScaling' is marked as deprecated, please check the documentation for more information.
  QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling, True)
C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py:652: DeprecationWarning: Enum value 'Qt::ApplicationAttribute.AA_UseHighDpiPixmaps' is marked as deprecated, please check the documentation for more information.
  QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_UseHighDpiPixmaps, True)
ìœˆë„ìš°ì¦ˆ GPU ì‚¬ìš©: NVIDIA GeForce RTX 4090
ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: C:/Users/User/PycharmProjects/pythonProject1/mt_via_anomalib/mt_via_anomalib/aivad_ui_ready_checkpoint.ckpt
QWindowsWindow::setGeometry: Unable to set geometry 1920x1013+0+23 (frame: 1936x1052-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1490x1013 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1506, y=1052)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1017+0+23 (frame: 1936x1056-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1495x1017 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1511, y=1056)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1021+0+23 (frame: 1936x1060-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1502x1021 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1518, y=1060)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1025+0+23 (frame: 1936x1064-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1510x1025 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1526, y=1064)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1029+0+23 (frame: 1936x1068-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1517x1029 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1533, y=1068)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1033+0+23 (frame: 1936x1072-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1524x1033 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1540, y=1072)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1037+0+23 (frame: 1936x1076-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1531x1037 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1547, y=1076)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1041+0+23 (frame: 1936x1080-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1538x1041 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1554, y=1080)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1045+0+23 (frame: 1936x1084-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1545x1045 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1561, y=1084)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1049+0+23 (frame: 1936x1088-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1552x1049 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1568, y=1088)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1053+0+23 (frame: 1936x1092-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1559x1053 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1575, y=1092)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1057+0+23 (frame: 1936x1096-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1566x1057 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1582, y=1096)))
QWindowsWindow::setGeometry: Unable to set geometry 1920x1061+0+23 (frame: 1936x1100-8-8) on QWidgetWindow/"MainWindowClassWindow" on "MACROSILICON". Resulting geometry: 1920x1009+0+23 (frame: 1936x1048-8-8) margins: 8, 31, 8, 8 minimum size: 1573x1061 MINMAXINFO(maxSize=POINT(x=0, y=0), maxpos=POINT(x=0, y=0), maxtrack=POINT(x=0, y=0), mintrack=POINT(x=1589, y=1100)))


--- ISSUE 2025.10.15. 20:45 ---
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 664, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 270, in infer_on_frame
    flows, regions = self._extract_regions_and_flows(t0.unsqueeze(0), t1.unsqueeze(0))
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 185, in _extract_regions_and_flows
    flows = self.core.flow_extractor(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\flow.py", line 75, in forward
    flows = self.model(first_frame, last_frame)[-1]
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\models\optical_flow\raft.py", line 492, in forward
    fmaps = self.feature_encoder(torch.cat([image1, image2], dim=0))
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\models\optical_flow\raft.py", line 160, in forward
    x = self.convnormrelu(x)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\container.py", line 240, in forward
    input = module(input)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\conv.py", line 549, in _conv_forward
    return F.conv2d(
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor

--- ISSUE 2025.10.15 20:49 ---
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: amax(): Expected reduction dim 0 to have non-zero size.
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 692, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 299, in infer_on_frame
    output = self.core(batch)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 210, in forward
    [
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 211, in <listcomp>
    torch.amax(region["masks"] * scores.view(-1, 1, 1, 1), dim=0)
IndexError: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” Flow extractor ì…ë ¥ ë””ë°”ì´ìŠ¤: cuda:0
ğŸ” Core ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0

--- ISSUE 2025.10.15 20:57 ---
ìœˆë„ìš°ì¦ˆ GPU ì‚¬ìš©: NVIDIA GeForce RTX 4090
ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„: C:/Users/User/PycharmProjects/pythonProject1/mt_via_anomalib/mt_via_anomalib/aivad_ui_ready_checkpoint.ckpt
ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: ['state_dict', 'pytorch-lightning_version', 'model_class']
âœ… state_dict ë¡œë“œ ì„±ê³µ
âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: C:/Users/User/PycharmProjects/pythonProject1/mt_via_anomalib/mt_via_anomalib/aivad_ui_ready_checkpoint.ckpt
âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0
âœ… Core ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0
âœ… ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: cuda
âš ï¸  ëª¨ë¸ì´ cuda:0ì— ìˆì§€ë§Œ ì„¤ì •ì€ cuda
âœ… ëª¨ë¸ ë¡œë“œ í™•ì¸ë¨

âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: 'DummyOutput' object has no attribute 'device'
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 300, in infer_on_frame
    output = self.core(batch)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 210, in forward
    [
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 211, in <listcomp>
    torch.amax(region["masks"] * scores.view(-1, 1, 1, 1), dim=0)
IndexError: amax(): Expected reduction dim 0 to have non-zero size.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 706, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 309, in infer_on_frame
    output = DummyOutput()
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 307, in __init__
    self.pred_score = torch.tensor([0.5], device=self.device)
AttributeError: 'DummyOutput' object has no attribute 'device'
ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 720, 1280])
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: 'DummyOutput' object has no attribute 'device'
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 300, in infer_on_frame
    output = self.core(batch)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 210, in forward
    [
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 211, in <listcomp>
    torch.amax(region["masks"] * scores.view(-1, 1, 1, 1), dim=0)
IndexError: amax(): Expected reduction dim 0 to have non-zero size.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 706, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 309, in infer_on_frame
    output = DummyOutput()
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 307, in __init__
    self.pred_score = torch.tensor([0.5], device=self.device)
AttributeError: 'DummyOutput' object has no attribute 'device'
ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 720, 1280])
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: 'DummyOutput' object has no attribute 'device'

--- ISSUE 2025.10.15. 21:03 ---
mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python realtime_ui_advanced_windows.py
C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py:799: DeprecationWarning: Enum value 'Qt::ApplicationAttribute.AA_EnableHighDpiScaling' is marked as deprecated, please check the documentation for more information.
  QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling, True)
C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py:800: DeprecationWarning: Enum value 'Qt::ApplicationAttribute.AA_UseHighDpiPixmaps' is marked as deprecated, please check the documentation for more information.
  QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_UseHighDpiPixmaps, True)
ìœˆë„ìš°ì¦ˆ GPU ì‚¬ìš©: NVIDIA GeForce RTX 4090
ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„: C:/Users/User/PycharmProjects/pythonProject1/mt_via_anomalib/mt_via_anomalib/aivad_ui_ready_checkpoint.ckpt
ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: ['state_dict', 'pytorch-lightning_version', 'model_class']
âœ… state_dict ë¡œë“œ ì„±ê³µ
âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: C:/Users/User/PycharmProjects/pythonProject1/mt_via_anomalib/mt_via_anomalib/aivad_ui_ready_checkpoint.ckpt
âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0
âœ… Core ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0
âœ… ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: cuda
âš ï¸  ëª¨ë¸ì´ cuda:0ì— ìˆì§€ë§Œ ì„¤ì •ì€ cuda
âœ… ëª¨ë¸ ë¡œë“œ í™•ì¸ë¨
ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 224, 224])
ğŸ” ì˜ˆìƒ í¬ê¸°: [1, 2, 3, 224, 224]
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: <class '__main__.AiVadInferencer.infer_on_frame.<locals>.DummyOutput'>
ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: ['pred_score', 'anomaly_map']
âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: 0.500, ë§µ í¬ê¸°: (1, 224, 224)
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 709, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 355, in infer_on_frame
    overlay = self._create_advanced_overlay(frame_bgr, anomaly_map, regions, box_scores)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 221, in _create_advanced_overlay
    heatmap = cv2.applyColorMap((norm_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
cv2.error: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 224, 224])
ğŸ” ì˜ˆìƒ í¬ê¸°: [1, 2, 3, 224, 224]
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: <class '__main__.AiVadInferencer.infer_on_frame.<locals>.DummyOutput'>
ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: ['pred_score', 'anomaly_map']
âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: 0.500, ë§µ í¬ê¸°: (1, 224, 224)
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 709, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 355, in infer_on_frame
    overlay = self._create_advanced_overlay(frame_bgr, anomaly_map, regions, box_scores)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 221, in _create_advanced_overlay
    heatmap = cv2.applyColorMap((norm_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
cv2.error: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 224, 224])
ğŸ” ì˜ˆìƒ í¬ê¸°: [1, 2, 3, 224, 224]
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: <class '__main__.AiVadInferencer.infer_on_frame.<locals>.DummyOutput'>
ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: ['pred_score', 'anomaly_map']
âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: 0.500, ë§µ í¬ê¸°: (1, 224, 224)
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 709, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 355, in infer_on_frame
    overlay = self._create_advanced_overlay(frame_bgr, anomaly_map, regions, box_scores)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 221, in _create_advanced_overlay
    heatmap = cv2.applyColorMap((norm_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
cv2.error: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 224, 224])
ğŸ” ì˜ˆìƒ í¬ê¸°: [1, 2, 3, 224, 224]
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: <class '__main__.AiVadInferencer.infer_on_frame.<locals>.DummyOutput'>
ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: ['pred_score', 'anomaly_map']
âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: 0.500, ë§µ í¬ê¸°: (1, 224, 224)
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 709, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 355, in infer_on_frame
    overlay = self._create_advanced_overlay(frame_bgr, anomaly_map, regions, box_scores)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 221, in _create_advanced_overlay
    heatmap = cv2.applyColorMap((norm_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
cv2.error: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 224, 224])
ğŸ” ì˜ˆìƒ í¬ê¸°: [1, 2, 3, 224, 224]
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: <class '__main__.AiVadInferencer.infer_on_frame.<locals>.DummyOutput'>
ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: ['pred_score', 'anomaly_map']
âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: 0.500, ë§µ í¬ê¸°: (1, 224, 224)
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 709, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 355, in infer_on_frame
    overlay = self._create_advanced_overlay(frame_bgr, anomaly_map, regions, box_scores)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 221, in _create_advanced_overlay
    heatmap = cv2.applyColorMap((norm_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
cv2.error: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 224, 224])
ğŸ” ì˜ˆìƒ í¬ê¸°: [1, 2, 3, 224, 224]
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: <class '__main__.AiVadInferencer.infer_on_frame.<locals>.DummyOutput'>
ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: ['pred_score', 'anomaly_map']
âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: 0.500, ë§µ í¬ê¸°: (1, 224, 224)
âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 709, in on_frame
    overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 355, in infer_on_frame
    overlay = self._create_advanced_overlay(frame_bgr, anomaly_map, regions, box_scores)
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\realtime_ui_advanced_windows.py", line 221, in _create_advanced_overlay
    heatmap = cv2.applyColorMap((norm_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
cv2.error: OpenCV(4.12.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'

ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0=cuda:0, t1=cuda:0, batch=cuda:0
ğŸ” ë°°ì¹˜ í¬ê¸°: torch.Size([1, 2, 3, 224, 224])
ğŸ” ì˜ˆìƒ í¬ê¸°: [1, 2, 3, 224, 224]
âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: amax(): Expected reduction dim 0 to have non-zero size.
ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: <class '__main__.AiVadInferencer.infer_on_frame.<locals>.DummyOutput'>
ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: ['pred_score', 'anomaly_map']
âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: 0.500, ë§µ í¬ê¸°: (1, 224, 224)

--- model check result 2025.10.15 21:40 ---

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python check_model_structure.py
============================================================
ğŸ” AI-VAD ëª¨ë¸ êµ¬ì¡° í™•ì¸
============================================================
ğŸ“Š Anomalib ë²„ì „: 2.2.0
âœ… AI-VAD í´ë˜ìŠ¤: <class 'anomalib.models.video.ai_vad.lightning_model.AiVad'>
ğŸ” AI-VAD ëª¨ë¸ êµ¬ì¡° í™•ì¸ ì¤‘...
âœ… AI-VAD ëª¨ë¸ ìƒì„± ì„±ê³µ

ğŸ“Š ëª¨ë¸ êµ¬ì¡°:
AiVad(
  (pre_processor): PreProcessor()
  (post_processor): PostProcessor(
    (_image_threshold_metric): F1AdaptiveThreshold()
    (_pixel_threshold_metric): F1AdaptiveThreshold()
    (_image_min_max_metric): MinMax()
    (_pixel_min_max_metric): MinMax()
  )
  (evaluator): Evaluator(
    (val_metrics): ModuleList()
    (test_metrics): ModuleList(
      (0): AUROC()
      (1): F1Score()
      (2): AUROC()
      (3): F1Score()
    )
  )
  (model): AiVadModel(
    (flow_extractor): FlowExtractor(
      (model): RAFT(
        (feature_encoder): FeatureEncoder(
          (convnormrelu): Conv2dNormActivation(
            (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
          (layer1): Sequential(
            (0): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
            (1): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
          )
          (layer2): Sequential(
            (0): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (downsample): Conv2dNormActivation(
                (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))
                (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              )
              (relu): ReLU(inplace=True)
            )
            (1): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
          )
          (layer3): Sequential(
            (0): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (downsample): Conv2dNormActivation(
                (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              )
              (relu): ReLU(inplace=True)
            )
            (1): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
          )
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (context_encoder): FeatureEncoder(
          (convnormrelu): Conv2dNormActivation(
            (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (layer1): Sequential(
            (0): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
            (1): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
          )
          (layer2): Sequential(
            (0): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (downsample): Conv2dNormActivation(
                (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (relu): ReLU(inplace=True)
            )
            (1): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
          )
          (layer3): Sequential(
            (0): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (downsample): Conv2dNormActivation(
                (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (relu): ReLU(inplace=True)
            )
            (1): ResidualBlock(
              (convnormrelu1): Conv2dNormActivation(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (convnormrelu2): Conv2dNormActivation(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (downsample): Identity()
              (relu): ReLU(inplace=True)
            )
          )
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (corr_block): CorrBlock()
        (update_block): UpdateBlock(
          (motion_encoder): MotionEncoder(
            (convcorr1): Conv2dNormActivation(
              (0): Conv2d(324, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU(inplace=True)
            )
            (convcorr2): Conv2dNormActivation(
              (0): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): ReLU(inplace=True)
            )
            (convflow1): Conv2dNormActivation(
              (0): Conv2d(2, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
              (1): ReLU(inplace=True)
            )
            (convflow2): Conv2dNormActivation(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): ReLU(inplace=True)
            )
            (conv): Conv2dNormActivation(
              (0): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): ReLU(inplace=True)
            )
          )
          (recurrent_block): RecurrentBlock(
            (convgru1): ConvGRU(
              (convz): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))
              (convr): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))
              (convq): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))
            )
            (convgru2): ConvGRU(
              (convz): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))
              (convr): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))
              (convq): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))
            )
          )
          (flow_head): FlowHead(
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (relu): ReLU(inplace=True)
          )
        )
        (mask_predictor): MaskPredictor(
          (convrelu): Conv2dNormActivation(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (conv): Conv2d(256, 576, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (transforms): OpticalFlow()
    )
    (region_extractor): RegionExtractor(
      (backbone): MaskRCNN(
        (transform): GeneralizedRCNNTransform(
            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            Resize(min_size=(800,), max_size=1333, mode='bilinear')
        )
        (backbone): BackboneWithFPN(
          (body): IntermediateLayerGetter(
            (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
            (layer1): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
            )
            (layer2): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (3): Bottleneck(
                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
            )
            (layer3): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (3): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (4): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (5): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
            )
            (layer4): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
              )
            )
          )
          (fpn): FeaturePyramidNetwork(
            (inner_blocks): ModuleList(
              (0): Conv2dNormActivation(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): Conv2dNormActivation(
                (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): Conv2dNormActivation(
                (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): Conv2dNormActivation(
                (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer_blocks): ModuleList(
              (0-3): 4 x Conv2dNormActivation(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (extra_blocks): LastLevelMaxPool()
          )
        )
        (rpn): RegionProposalNetwork(
          (anchor_generator): AnchorGenerator()
          (head): RPNHead(
            (conv): Sequential(
              (0): Conv2dNormActivation(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
              (1): Conv2dNormActivation(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
            (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (roi_heads): RoIHeads(
          (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)
          (box_head): FastRCNNConvFCHead(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (4): Flatten(start_dim=1, end_dim=-1)
            (5): Linear(in_features=12544, out_features=1024, bias=True)
            (6): ReLU(inplace=True)
          )
          (box_predictor): FastRCNNPredictor(
            (cls_score): Linear(in_features=1024, out_features=91, bias=True)
            (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)
          )
          (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)
          (mask_head): MaskRCNNHeads(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (mask_predictor): MaskRCNNPredictor(
            (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
            (relu): ReLU(inplace=True)
            (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))
          )
        )
      )
    )
    (feature_extractor): VideoRegionFeatureExtractor(
      (deep_extractor): DeepExtractor(
        (encoder): CLIP(
          (visual): VisionTransformer(
            (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
            (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (transformer): Transformer(
              (resblocks): Sequential(
                (0): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (1): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (2): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (3): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (4): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (5): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (6): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (7): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (8): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (9): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (10): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
                (11): ResidualAttentionBlock(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                  )
                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (mlp): Sequential(
                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                    (gelu): QuickGELU()
                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                  )
                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (token_embedding): Embedding(49408, 512)
          (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (transform): Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
      )
      (velocity_extractor): VelocityExtractor()
      (pose_extractor): PoseExtractor(
        (model): KeypointRCNN(
          (transform): GeneralizedRCNNTransform(
              Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
              Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')
          )
          (backbone): BackboneWithFPN(
            (body): IntermediateLayerGetter(
              (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
              (bn1): FrozenBatchNorm2d(64, eps=0.0)
              (relu): ReLU(inplace=True)
              (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
              (layer1): Sequential(
                (0): Bottleneck(
                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(64, eps=0.0)
                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(64, eps=0.0)
                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(256, eps=0.0)
                  (relu): ReLU(inplace=True)
                  (downsample): Sequential(
                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d(256, eps=0.0)
                  )
                )
                (1): Bottleneck(
                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(64, eps=0.0)
                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(64, eps=0.0)
                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(256, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (2): Bottleneck(
                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(64, eps=0.0)
                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(64, eps=0.0)
                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(256, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
              )
              (layer2): Sequential(
                (0): Bottleneck(
                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(128, eps=0.0)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(128, eps=0.0)
                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(512, eps=0.0)
                  (relu): ReLU(inplace=True)
                  (downsample): Sequential(
                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                    (1): FrozenBatchNorm2d(512, eps=0.0)
                  )
                )
                (1): Bottleneck(
                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(128, eps=0.0)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(128, eps=0.0)
                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(512, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (2): Bottleneck(
                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(128, eps=0.0)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(128, eps=0.0)
                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(512, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (3): Bottleneck(
                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(128, eps=0.0)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(128, eps=0.0)
                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(512, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
              )
              (layer3): Sequential(
                (0): Bottleneck(
                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(256, eps=0.0)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(256, eps=0.0)
                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                  (relu): ReLU(inplace=True)
                  (downsample): Sequential(
                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                    (1): FrozenBatchNorm2d(1024, eps=0.0)
                  )
                )
                (1): Bottleneck(
                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(256, eps=0.0)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(256, eps=0.0)
                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (2): Bottleneck(
                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(256, eps=0.0)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(256, eps=0.0)
                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (3): Bottleneck(
                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(256, eps=0.0)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(256, eps=0.0)
                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (4): Bottleneck(
                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(256, eps=0.0)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(256, eps=0.0)
                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (5): Bottleneck(
                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(256, eps=0.0)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(256, eps=0.0)
                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
              )
              (layer4): Sequential(
                (0): Bottleneck(
                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(512, eps=0.0)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(512, eps=0.0)
                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(2048, eps=0.0)
                  (relu): ReLU(inplace=True)
                  (downsample): Sequential(
                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                    (1): FrozenBatchNorm2d(2048, eps=0.0)
                  )
                )
                (1): Bottleneck(
                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(512, eps=0.0)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(512, eps=0.0)
                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(2048, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
                (2): Bottleneck(
                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d(512, eps=0.0)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d(512, eps=0.0)
                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (bn3): FrozenBatchNorm2d(2048, eps=0.0)
                  (relu): ReLU(inplace=True)
                )
              )
            )
            (fpn): FeaturePyramidNetwork(
              (inner_blocks): ModuleList(
                (0): Conv2dNormActivation(
                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                )
                (1): Conv2dNormActivation(
                  (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                )
                (2): Conv2dNormActivation(
                  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                )
                (3): Conv2dNormActivation(
                  (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (layer_blocks): ModuleList(
                (0-3): 4 x Conv2dNormActivation(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (extra_blocks): LastLevelMaxPool()
            )
          )
          (rpn): RegionProposalNetwork(
            (anchor_generator): AnchorGenerator()
            (head): RPNHead(
              (conv): Sequential(
                (0): Conv2dNormActivation(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (1): ReLU(inplace=True)
                )
              )
              (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
              (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (roi_heads): RoIHeads(
            (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)
            (box_head): TwoMLPHead(
              (fc6): Linear(in_features=12544, out_features=1024, bias=True)
              (fc7): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (box_predictor): FastRCNNPredictor(
              (cls_score): Linear(in_features=1024, out_features=2, bias=True)
              (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)
            )
            (keypoint_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)
            (keypoint_head): KeypointRCNNHeads(
              (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): ReLU(inplace=True)
              (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (3): ReLU(inplace=True)
              (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (5): ReLU(inplace=True)
              (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (7): ReLU(inplace=True)
              (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (9): ReLU(inplace=True)
              (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (11): ReLU(inplace=True)
              (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (13): ReLU(inplace=True)
              (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (15): ReLU(inplace=True)
            )
            (keypoint_predictor): KeypointRCNNPredictor(
              (kps_score_lowres): ConvTranspose2d(512, 17, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            )
          )
        )
        (transform): GeneralizedRCNNTransform(
            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')
        )
        (backbone): BackboneWithFPN(
          (body): IntermediateLayerGetter(
            (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            (bn1): FrozenBatchNorm2d(64, eps=0.0)
            (relu): ReLU(inplace=True)
            (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
            (layer1): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(64, eps=0.0)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(64, eps=0.0)
                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(256, eps=0.0)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d(256, eps=0.0)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(64, eps=0.0)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(64, eps=0.0)
                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(256, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(64, eps=0.0)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(64, eps=0.0)
                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(256, eps=0.0)
                (relu): ReLU(inplace=True)
              )
            )
            (layer2): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(128, eps=0.0)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(128, eps=0.0)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(512, eps=0.0)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): FrozenBatchNorm2d(512, eps=0.0)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(128, eps=0.0)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(128, eps=0.0)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(512, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(128, eps=0.0)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(128, eps=0.0)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(512, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (3): Bottleneck(
                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(128, eps=0.0)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(128, eps=0.0)
                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(512, eps=0.0)
                (relu): ReLU(inplace=True)
              )
            )
            (layer3): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(256, eps=0.0)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(256, eps=0.0)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): FrozenBatchNorm2d(1024, eps=0.0)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(256, eps=0.0)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(256, eps=0.0)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(256, eps=0.0)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(256, eps=0.0)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (3): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(256, eps=0.0)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(256, eps=0.0)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (4): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(256, eps=0.0)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(256, eps=0.0)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (5): Bottleneck(
                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(256, eps=0.0)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(256, eps=0.0)
                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(1024, eps=0.0)
                (relu): ReLU(inplace=True)
              )
            )
            (layer4): Sequential(
              (0): Bottleneck(
                (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(512, eps=0.0)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(512, eps=0.0)
                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(2048, eps=0.0)
                (relu): ReLU(inplace=True)
                (downsample): Sequential(
                  (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): FrozenBatchNorm2d(2048, eps=0.0)
                )
              )
              (1): Bottleneck(
                (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(512, eps=0.0)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(512, eps=0.0)
                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(2048, eps=0.0)
                (relu): ReLU(inplace=True)
              )
              (2): Bottleneck(
                (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn1): FrozenBatchNorm2d(512, eps=0.0)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): FrozenBatchNorm2d(512, eps=0.0)
                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn3): FrozenBatchNorm2d(2048, eps=0.0)
                (relu): ReLU(inplace=True)
              )
            )
          )
          (fpn): FeaturePyramidNetwork(
            (inner_blocks): ModuleList(
              (0): Conv2dNormActivation(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): Conv2dNormActivation(
                (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
              )
              (2): Conv2dNormActivation(
                (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              )
              (3): Conv2dNormActivation(
                (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              )
            )
            (layer_blocks): ModuleList(
              (0-3): 4 x Conv2dNormActivation(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (extra_blocks): LastLevelMaxPool()
          )
        )
        (roi_heads): RoIHeads(
          (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)
          (box_head): TwoMLPHead(
            (fc6): Linear(in_features=12544, out_features=1024, bias=True)
            (fc7): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (box_predictor): FastRCNNPredictor(
            (cls_score): Linear(in_features=1024, out_features=2, bias=True)
            (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)
          )
          (keypoint_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)
          (keypoint_head): KeypointRCNNHeads(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (3): ReLU(inplace=True)
            (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (5): ReLU(inplace=True)
            (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (7): ReLU(inplace=True)
            (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (9): ReLU(inplace=True)
            (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (11): ReLU(inplace=True)
            (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (13): ReLU(inplace=True)
            (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (15): ReLU(inplace=True)
          )
          (keypoint_predictor): KeypointRCNNPredictor(
            (kps_score_lowres): ConvTranspose2d(512, 17, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          )
        )
      )
    )
    (density_estimator): CombinedDensityEstimator(
      (velocity_estimator): GMMEstimator(
        (gmm): GaussianMixture()
      )
      (appearance_estimator): GroupedKNNEstimator()
      (pose_estimator): GroupedKNNEstimator()
    )
  )
)

ğŸ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í™•ì¸:
âœ… Core ëª¨ë¸ íƒ€ì…: <class 'anomalib.models.video.ai_vad.torch_model.AiVadModel'>
âœ… flow_extractor: <class 'anomalib.models.video.ai_vad.flow.FlowExtractor'>
âœ… region_extractor: <class 'anomalib.models.video.ai_vad.regions.RegionExtractor'>
âŒ clip_extractor: ì—†ìŒ
âœ… feature_extractor: <class 'anomalib.models.video.ai_vad.features.VideoRegionFeatureExtractor'>

ğŸ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸: aivad_ui_ready_checkpoint.ckpt
ğŸ“Š ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: ['state_dict', 'pytorch-lightning_version', 'model_class']
ğŸ“Š State dict í‚¤ ê°œìˆ˜: 1541
ğŸ” AI-VAD ê´€ë ¨ í‚¤ë“¤:
   - model.flow_extractor.model.feature_encoder.convnormrelu.0.weight: torch.Size([64, 3, 7, 7])
   - model.flow_extractor.model.feature_encoder.convnormrelu.0.weight: torch.Size([64, 3, 7, 7])
   - model.flow_extractor.model.feature_encoder.convnormrelu.0.bias: torch.Size([64])
   - model.flow_extractor.model.feature_encoder.convnormrelu.0.bias: torch.Size([64])
   - model.flow_extractor.model.feature_encoder.layer1.0.convnormrelu1.0.weight: torch.Size([64, 64, 3, 3])
   - model.flow_extractor.model.feature_encoder.layer1.0.convnormrelu1.0.weight: torch.Size([64, 64, 3, 3])
   - model.flow_extractor.model.feature_encoder.layer1.0.convnormrelu1.0.bias: torch.Size([64])
   - model.flow_extractor.model.feature_encoder.layer1.0.convnormrelu1.0.bias: torch.Size([64])
   - model.flow_extractor.model.feature_encoder.layer1.0.convnormrelu2.0.weight: torch.Size([64, 64, 3, 3])
   - model.flow_extractor.model.feature_encoder.layer1.0.convnormrelu2.0.weight: torch.Size([64, 64, 3, 3])

ğŸ—ï¸  ì˜¬ë°”ë¥¸ AI-VAD ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì¤‘...
âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ
âœ… Core ëª¨ë¸: <class 'anomalib.models.video.ai_vad.torch_model.AiVadModel'>
âœ… Flow extractor ì¡´ì¬
âœ… Region extractor ì¡´ì¬
âš ï¸  CLIP extractor ì—†ìŒ - ì´ëŠ” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
ğŸ’¾ ì˜¬ë°”ë¥¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: aivad_proper_checkpoint.ckpt
ğŸ“Š ì²´í¬í¬ì¸íŠ¸ í¬ê¸°: 757.8 MB

ğŸ‰ ì˜¬ë°”ë¥¸ AI-VAD ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì™„ë£Œ!

ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:
1. UIì—ì„œ 'aivad_proper_checkpoint.ckpt' ë¡œë“œ
2. ì‹¤ì œ AI-VAD êµ¬ì¡°ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
3. CLIP ë“±ì˜ ì»´í¬ë„ŒíŠ¸ í¬í•¨ ì—¬ë¶€ í™•ì¸

--- ISSUE 2025.10.15. 21:52 ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_finetune.py
ğŸš€ AI-VAD ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090
GPU ë©”ëª¨ë¦¬: 24.0 GB
ğŸ“ í›ˆë ¨í•  ë¹„ë””ì˜¤ íŒŒì¼: 56ê°œ
  1. normal_0.avi
  2. normal_1.avi
  3. normal_2.avi
  ... ì™¸ 53ê°œ

ğŸ“Š ë°ì´í„° ëª¨ë“ˆ ì„¤ì •...
ğŸ“ ë¹„ë””ì˜¤ íŒŒì¼ 56ê°œ ë¡œë“œ ì¤‘...
  - normal_0.avi: 1802 frames
  - normal_1.avi: 1674 frames
  - normal_2.avi: 273 frames
  - normal_3.avi: 198 frames
  - normal_4.avi: 802 frames
  - normal_5.avi: 2094 frames
  - normal_6.avi: 527 frames
  - normal_7.avi: 511 frames
  - normal_8.avi: 122 frames
  - normal_9.avi: 1485 frames
  - normal_10.avi: 769 frames
  - normal_11.avi: 984 frames
  - normal_12.avi: 845 frames
  - normal_13.avi: 72 frames
  - normal_14.avi: 408 frames
  - normal_15.avi: 82 frames
  - normal_16.avi: 491 frames
  - normal_17.avi: 440 frames
  - normal_18.avi: 484 frames
  - normal_19.avi: 324 frames
  - normal_20.avi: 672 frames
  - normal_21.avi: 2460 frames
  - normal_22.avi: 1959 frames
  - rest_0.avi: 214 frames
  - rest_1.avi: 57 frames
  - rest_2.avi: 19 frames
  - rest_3.avi: 76 frames
  - rest_4.avi: 24 frames
  - rest_5.avi: 145 frames
  - rest_6.avi: 42 frames
  - rest_7.avi: 777 frames
  - rest_8.avi: 80 frames
  - rest_9.avi: 247 frames
  - rest_10.avi: 212 frames
  - rest_11.avi: 260 frames
  - rest_12.avi: 210 frames
  - rest_13.avi: 81 frames
  - rest_14.avi: 226 frames
  - rest_15.avi: 193 frames
  - rest_16.avi: 223 frames
  - rest_17.avi: 202 frames
  - rest_18.avi: 10 frames
  - rest_19.avi: 120 frames
  - rest_20.avi: 139 frames
  - rest_21.avi: 279 frames
  - rest_22.avi: 145 frames
  - rest_23.avi: 136 frames
  - rest_24.avi: 96 frames
  - rest_25.avi: 185 frames
  - rest_26.avi: 474 frames
  - rest_27.avi: 358 frames
  - rest_28.avi: 201 frames
  - rest_29.avi: 106 frames
  - rest_30.avi: 78 frames
  - rest_31.avi: 203 frames
  - rest_32.avi: 438 frames
âœ… ì´ 25734 í”„ë ˆì„ ë¡œë“œ ì™„ë£Œ

ğŸ¤– ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ...
ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ: aivad_proper_checkpoint.ckpt
INFO:anomalib.models.components.base.anomalib_module:Initializing AiVad model.
âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ

âš™ï¸ íŒŒì¸íŠœë‹ ì„¤ì •...
âš™ï¸ íŒŒì¸íŠœë‹ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (LR: 1e-05)

ğŸ”§ í•™ìŠµ ì—”ì§„ ì„¤ì •...

ğŸ¯ íŒŒì¸íŠœë‹ ì‹œì‘!
ğŸ“Š ì‹¤ì œ ë¹„ë””ì˜¤ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ ì‹œì‘...
âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜: 'CustomVideoDataModule' object has no attribute 'name'
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_aivad_finetune.py", line 253, in main
    engine.fit(model=model, datamodule=datamodule)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\engine\engine.py", line 400, in fit
    self._setup_workspace(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\engine\engine.py", line 270, in _setup_workspace
    dataset_name = datamodule.name
AttributeError: 'CustomVideoDataModule' object has no attribute 'name'

--- ISSUE 2025.10.16. ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_finetune.py
ğŸš€ AI-VAD ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090
GPU ë©”ëª¨ë¦¬: 24.0 GB
ğŸ“ í›ˆë ¨í•  ë¹„ë””ì˜¤ íŒŒì¼: 56ê°œ
  1. normal_0.avi
  2. normal_1.avi
  3. normal_2.avi
  ... ì™¸ 53ê°œ

ğŸ“Š ë°ì´í„° ëª¨ë“ˆ ì„¤ì •...
âœ… Avenue ë°ì´í„° ëª¨ë“ˆ ì‚¬ìš©

ğŸ¤– ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ...
ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ: aivad_proper_checkpoint.ckpt
INFO:anomalib.models.components.base.anomalib_module:Initializing AiVad model.
âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ

âš™ï¸ íŒŒì¸íŠœë‹ ì„¤ì •...
âš™ï¸ íŒŒì¸íŠœë‹ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (LR: 1e-05)

ğŸ”§ í•™ìŠµ ì—”ì§„ ì„¤ì •...

ğŸ¯ íŒŒì¸íŠœë‹ ì‹œì‘!
ğŸ“Š ì‹¤ì œ ë¹„ë””ì˜¤ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ ì‹œì‘...
INFO:anomalib.engine.engine:Overriding gradient_clip_val from 1.0 with 0 for AiVad
INFO:anomalib.engine.engine:Overriding max_epochs from 5 with 1 for AiVad
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:anomalib.data.datamodules.video.avenue:Found the dataset.
âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜: cannot set a frame with no defined index and a scalar
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_aivad_finetune.py", line 276, in main
    engine.fit(model=model, datamodule=datamodule)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\engine\engine.py", line 412, in fit
    self.trainer.fit(model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 108, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 199, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datamodules\base\image.py", line 154, in setup
    self._setup(stage)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datamodules\video\avenue.py", line 178, in _setup
    self.train_data = AvenueDataset(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 115, in __init__
    self.samples = make_avenue_dataset(self.root, self.gt_dir, self.split)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\video\avenue.py", line 164, in make_avenue_dataset
    samples.loc[samples.folder == "training_videos", "split"] = "train"
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\pandas\core\indexing.py", line 912, in __setitem__
    iloc._setitem_with_indexer(indexer, value, self.name)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\pandas\core\indexing.py", line 1848, in _setitem_with_indexer
    raise ValueError(
ValueError: cannot set a frame with no defined index and a scalar


--- ISSUE : 2025.10.16. ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_simple_finetune.py
ğŸš€ ê°„ë‹¨í•œ AI-VAD íŒŒì¸íŠœë‹ ì‹œì‘
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090
GPU ë©”ëª¨ë¦¬: 24.0 GB

ğŸ“ í›ˆë ¨í•  ë¹„ë””ì˜¤ íŒŒì¼: 56ê°œ
  1. normal_0.avi
  2. normal_1.avi
  3. normal_2.avi
  4. normal_3.avi
  5. normal_4.avi
  ... ì™¸ 51ê°œ

ğŸ“Š ë°ì´í„°ì…‹ ìƒì„±...
  ğŸ“¹ normal_0.avi: 1802 frames
  ğŸ“¹ normal_1.avi: 1674 frames
  ğŸ“¹ normal_2.avi: 273 frames
  ğŸ“¹ normal_3.avi: 198 frames
  ğŸ“¹ normal_4.avi: 802 frames
  ğŸ“¹ normal_5.avi: 2094 frames
  ğŸ“¹ normal_6.avi: 527 frames
  ğŸ“¹ normal_7.avi: 511 frames
  ğŸ“¹ normal_8.avi: 122 frames
  ğŸ“¹ normal_9.avi: 1485 frames
  ğŸ“¹ normal_10.avi: 769 frames
  ğŸ“¹ normal_11.avi: 984 frames
  ğŸ“¹ normal_12.avi: 845 frames
  ğŸ“¹ normal_13.avi: 72 frames
  ğŸ“¹ normal_14.avi: 408 frames
  ğŸ“¹ normal_15.avi: 82 frames
  ğŸ“¹ normal_16.avi: 491 frames
  ğŸ“¹ normal_17.avi: 440 frames
  ğŸ“¹ normal_18.avi: 484 frames
  ğŸ“¹ normal_19.avi: 324 frames
  ğŸ“¹ normal_20.avi: 672 frames
  ğŸ“¹ normal_21.avi: 2460 frames
  ğŸ“¹ normal_22.avi: 1959 frames
  ğŸ“¹ rest_0.avi: 214 frames
  ğŸ“¹ rest_1.avi: 57 frames
  ğŸ“¹ rest_2.avi: 19 frames
  ğŸ“¹ rest_3.avi: 76 frames
  ğŸ“¹ rest_4.avi: 24 frames
  ğŸ“¹ rest_5.avi: 145 frames
  ğŸ“¹ rest_6.avi: 42 frames
  ğŸ“¹ rest_7.avi: 777 frames
  ğŸ“¹ rest_8.avi: 80 frames
  ğŸ“¹ rest_9.avi: 247 frames
  ğŸ“¹ rest_10.avi: 212 frames
  ğŸ“¹ rest_11.avi: 260 frames
  ğŸ“¹ rest_12.avi: 210 frames
  ğŸ“¹ rest_13.avi: 81 frames
  ğŸ“¹ rest_14.avi: 226 frames
  ğŸ“¹ rest_15.avi: 193 frames
  ğŸ“¹ rest_16.avi: 223 frames
  ğŸ“¹ rest_17.avi: 202 frames
  ğŸ“¹ rest_18.avi: 10 frames
  ğŸ“¹ rest_19.avi: 120 frames
  ğŸ“¹ rest_20.avi: 139 frames
  ğŸ“¹ rest_21.avi: 279 frames
  ğŸ“¹ rest_22.avi: 145 frames
  ğŸ“¹ rest_23.avi: 136 frames
  ğŸ“¹ rest_24.avi: 96 frames
  ğŸ“¹ rest_25.avi: 185 frames
  ğŸ“¹ rest_26.avi: 474 frames
  ğŸ“¹ rest_27.avi: 358 frames
  ğŸ“¹ rest_28.avi: 201 frames
  ğŸ“¹ rest_29.avi: 106 frames
  ğŸ“¹ rest_30.avi: 78 frames
  ğŸ“¹ rest_31.avi: 203 frames
  ğŸ“¹ rest_32.avi: 438 frames
ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:
  - ë¹„ë””ì˜¤ íŒŒì¼: 56ê°œ
  - ìƒì„±ëœ í´ë¦½: 2586ê°œ
âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ (ë°°ì¹˜ ìˆ˜: 2586)

ğŸ¤– ëª¨ë¸ ë¡œë“œ...
ğŸ”„ ëª¨ë¸ ë¡œë“œ: aivad_proper_checkpoint.ckpt
INFO:anomalib.models.components.base.anomalib_module:Initializing AiVad model.
âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ

ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...
ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ì—í¬í¬: 3, í•™ìŠµë¥ : 1e-05)
  âŒ ë°°ì¹˜ 0 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 1 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 2 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 3 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 4 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 5 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 6 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 7 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 8 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 9 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 10 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 11 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 12 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 13 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 14 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 15 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 16 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 17 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 18 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 19 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 20 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 21 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 22 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 23 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 24 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 25 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 26 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 27 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 28 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 29 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 30 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 31 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 32 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 33 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 34 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 35 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 36 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 37 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 38 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 39 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 40 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 41 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 42 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 43 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 44 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 45 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 46 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 47 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
  âŒ ë°°ì¹˜ 48 ì²˜ë¦¬ ì‹¤íŒ¨: 'list' object has no attribute 'pred_score'
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_simple_finetune.py", line 251, in <module>
    main()
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_simple_finetune.py", line 225, in main
    train_model(
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_simple_finetune.py", line 151, in train_model
    output = model(batch_data)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\components\base\anomalib_module.py", line 200, in forward
    batch = self.model(batch)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 193, in forward
    regions = self.region_extractor(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\regions.py", line 115, in forward
    regions = self._add_foreground_boxes(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\regions.py", line 181, in _add_foreground_boxes
    batch_boxes, _ = masks_to_boxes(foreground_map)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\utils\boxes.py", line 56, in masks_to_boxes
    batch_comps = connected_components_gpu(masks).squeeze(1)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\utils\cv\connected_components.py", line 63, in connected_components_gpu
    components = connected_components(image, num_iterations=num_iterations)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\kornia\contrib\connected_components.py", line 73, in connected_components
    out = F.max_pool2d(out, kernel_size=3, stride=1, padding=1)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\_jit_internal.py", line 622, in fní–£ 
    return if_false(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\functional.py", line 830, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
KeyboardInterrupt
^C



--- ISSUE 2025.10.16. ---

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_official.py
ğŸš€ Anomalib ê³µì‹ ë°©ë²•ìœ¼ë¡œ AI-VAD íŒŒì¸íŠœë‹
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090

ğŸ“ í›ˆë ¨í•  ë¹„ë””ì˜¤ íŒŒì¼: 56ê°œ
ğŸ“ ë¹„ë””ì˜¤ í´ë” êµ¬ì¡° ì¤€ë¹„: custom_video_dataset
  ğŸ“‚ ì •ìƒ ë¹„ë””ì˜¤ í´ë”: custom_video_dataset\train\good
    ğŸ“¹ normal_000_normal_0.avi
    ğŸ“¹ normal_001_normal_1.avi
    ğŸ“¹ normal_002_normal_2.avi
    ğŸ“¹ normal_003_normal_3.avi
    ğŸ“¹ normal_004_normal_4.avi
  âœ… 56ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ

ğŸ“Š Anomalib Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„±...
âŒ Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„± ì‹¤íŒ¨: Folder.__init__() got an unexpected keyword argument 'task'

--- ISSUE 2025.10.16 12:44 ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_official.py
ğŸš€ Anomalib ê³µì‹ ë°©ë²•ìœ¼ë¡œ AI-VAD íŒŒì¸íŠœë‹
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090

ğŸ“ í›ˆë ¨í•  ë¹„ë””ì˜¤ íŒŒì¼: 56ê°œ
ğŸ“ ë¹„ë””ì˜¤ í´ë” êµ¬ì¡° ì¤€ë¹„: custom_video_dataset
  ğŸ“‚ ì •ìƒ ë¹„ë””ì˜¤ í´ë”: custom_video_dataset\train\good
    ğŸ“¹ normal_000_normal_0.avi
    ğŸ“¹ normal_001_normal_1.avi
    ğŸ“¹ normal_002_normal_2.avi
    ğŸ“¹ normal_003_normal_3.avi
    ğŸ“¹ normal_004_normal_4.avi
  âœ… 56ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ

ğŸ“Š Anomalib Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„±...
âŒ Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„± ì‹¤íŒ¨: Folder.__init__() missing 1 required positional argument: 'name'

--- ISSUE 2025.10.16. 12:46 ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_official.py
ğŸš€ Anomalib ê³µì‹ ë°©ë²•ìœ¼ë¡œ AI-VAD íŒŒì¸íŠœë‹
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090

ğŸ“ í›ˆë ¨í•  ë¹„ë””ì˜¤ íŒŒì¼: 56ê°œ
ğŸ“ ë¹„ë””ì˜¤ í´ë” êµ¬ì¡° ì¤€ë¹„: custom_video_dataset
  ğŸ“‚ ì •ìƒ ë¹„ë””ì˜¤ í´ë”: custom_video_dataset\train\good
    ğŸ“¹ normal_000_normal_0.avi
    ğŸ“¹ normal_001_normal_1.avi
    ğŸ“¹ normal_002_normal_2.avi
    ğŸ“¹ normal_003_normal_3.avi
    ğŸ“¹ normal_004_normal_4.avi
  âœ… 56ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ

ğŸ“Š Anomalib Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„±...
âœ… Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„± ì™„ë£Œ

ğŸ¤– AI-VAD ëª¨ë¸ ìƒì„±...
âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ

ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: aivad_proper_checkpoint.ckpt
âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ

ğŸ”§ Anomalib Engine ìƒì„±...
âœ… Engine ìƒì„± ì™„ë£Œ

ğŸ¯ AI-VAD íŒŒì¸íŠœë‹ ì‹œì‘...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: Found 0 normal images in C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\custom_video_dataset\train\good with extensions ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_aivad_official.py", line 140, in main
    engine.fit(model=model, datamodule=datamodule)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\engine\engine.py", line 412, in fit
    self.trainer.fit(model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 108, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 199, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datamodules\base\image.py", line 154, in setup
    self._setup(stage)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datamodules\image\folder.py", line 165, in _setup
    self.train_data = FolderDataset(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\image\folder.py", line 112, in __init__
    self.samples = make_folder_dataset(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\datasets\image\folder.py", line 226, in make_folder_dataset
    filename, label = _prepare_files_labels(path, dir_type, extensions)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\data\utils\path.py", line 121, in _prepare_files_labels
    raise RuntimeError(msg)
RuntimeError: Found 0 normal images in C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\custom_video_dataset\train\good with extensions ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')



--- ISSUE 2025.10.16 18:11 ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_with_frames.py
ğŸš€ ì´ë¯¸ì§€ í”„ë ˆì„ì„ ì‚¬ìš©í•œ AI-VAD íŒŒì¸íŠœë‹
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090
ğŸ“ ì´ë¯¸ì§€ í”„ë ˆì„ ì¶”ì¶œ: image_segments.json
  ğŸ“‚ ì •ìƒ ì´ë¯¸ì§€ í´ë”: frame_dataset\train\good
  ğŸ“Š JSON ë¡œë“œ ì™„ë£Œ: 144ê°œ ì„¸ê·¸ë¨¼íŠ¸
    ğŸ“¸ normal_001_00_frame_20250728-143012_273.jpg
    ğŸ“¸ normal_001_01_frame_20250728-143012_308.jpg
    ğŸ“¸ normal_001_02_frame_20250728-143012_340.jpg
    ğŸ“¸ normal_001_03_frame_20250728-143012_373.jpg
    ğŸ“¸ normal_001_04_frame_20250728-143012_408.jpg
    ğŸ“¸ normal_002_00_frame_20250728-143442_128.jpg
    ğŸ“¸ normal_002_01_frame_20250728-143442_161.jpg
    ğŸ“¸ normal_002_02_frame_20250728-143442_194.jpg
    ğŸ“¸ normal_002_03_frame_20250728-143442_228.jpg
    ğŸ“¸ normal_002_04_frame_20250728-143442_261.jpg
  âœ… ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸: 23ê°œ
  âœ… ë³µì‚¬ëœ ì´ë¯¸ì§€: 115ê°œ

ğŸ“Š Anomalib Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„±...
âŒ Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„± ì‹¤íŒ¨: Folder.__init__() got an unexpected keyword argument 'image_size'


--- ISSUE 18:19 ---
(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_with_frames.py
ğŸš€ ì´ë¯¸ì§€ í”„ë ˆì„ì„ ì‚¬ìš©í•œ AI-VAD íŒŒì¸íŠœë‹
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090
ğŸ“ ì´ë¯¸ì§€ í”„ë ˆì„ ì¶”ì¶œ: image_segments.json
  ğŸ“‚ ì •ìƒ ì´ë¯¸ì§€ í´ë”: frame_dataset\train\good
  ğŸ“Š JSON ë¡œë“œ ì™„ë£Œ: 144ê°œ ì„¸ê·¸ë¨¼íŠ¸
    ğŸ“¸ normal_001_00_frame_20250728-143012_273.jpg
    ğŸ“¸ normal_001_01_frame_20250728-143012_308.jpg
    ğŸ“¸ normal_001_02_frame_20250728-143012_340.jpg
    ğŸ“¸ normal_001_03_frame_20250728-143012_373.jpg
    ğŸ“¸ normal_001_04_frame_20250728-143012_408.jpg
    ğŸ“¸ normal_002_00_frame_20250728-143442_128.jpg
    ğŸ“¸ normal_002_01_frame_20250728-143442_161.jpg
    ğŸ“¸ normal_002_02_frame_20250728-143442_194.jpg
    ğŸ“¸ normal_002_03_frame_20250728-143442_228.jpg
    ğŸ“¸ normal_002_04_frame_20250728-143442_261.jpg
  âœ… ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸: 23ê°œ
  âœ… ë³µì‚¬ëœ ì´ë¯¸ì§€: 115ê°œ

ğŸ“Š Anomalib Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„±...
âœ… Folder ë°ì´í„° ëª¨ë“ˆ ìƒì„± ì™„ë£Œ

ğŸ¤– AI-VAD ëª¨ë¸ ìƒì„±...
âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ

ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: aivad_proper_checkpoint.ckpt
âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ

ğŸ”§ Anomalib Engine ìƒì„±...
âœ… Engine ìƒì„± ì™„ë£Œ

ğŸ¯ AI-VAD íŒŒì¸íŠœë‹ ì‹œì‘...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\core\optimizer.py:183: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\utilities\model_summary\model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name           | Type          | Params | Mode
---------------------------------------------------------
0 | pre_processor  | PreProcessor  | 0      | train
1 | post_processor | PostProcessor | 0      | train
2 | evaluator      | Evaluator     | 0      | train
3 | model          | AiVadModel    | 260 M  | train
---------------------------------------------------------
259 M     Trainable params
447 K     Non-trainable params
260 M     Total params
1,041.500 Total estimated model params size (MB)
670       Modules in train mode
227       Modules in eval mode
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.
C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py:527: Found 227 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.
Epoch 0:   0%|                                                                               | 0/12 [00:00<?, ?it/s] âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 0
Traceback (most recent call last):
  File "C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\train_aivad_with_frames.py", line 157, in main
    engine.fit(model=model, datamodule=datamodule)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\engine\engine.py", line 412, in fit
    self.trainer.fit(model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1011, in _run
    results = self._run_stage()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 216, in run
    self.advance()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\core\module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\core\optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\plugins\precision\amp.py", line 79, in optimizer_step
    closure_result = closure()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\trainer\call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\lightning_model.py", line 181, in training_step
    features_per_batch = self.model(batch.image)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\torch_model.py", line 192, in forward
    flows = self.flow_extractor(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\flow.py", line 71, in forward
    first_frame, last_frame = self.pre_process(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\anomalib\models\video\ai_vad\flow.py", line 56, in pre_process
    return self.transforms(first_frame, last_frame)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\transforms\_presets.py", line 201, in forward
    img1 = F.normalize(img1, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\transforms\functional.py", line 350, in normalize
    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)
  File "C:\Users\User\anaconda3\envs\mt_p310\lib\site-packages\torchvision\transforms\_functional_tensor.py", line 928, in normalize
    return tensor.sub_(mean).div_(std)
RuntimeError: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 0
Epoch 0:   0%|                                                                               | 0/12 [00:00<?, ?it/s]


--- ISSUE 18:22 ---

(mt_p310) C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib>python train_aivad_video_sequence.py
ğŸš€ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ AI-VAD íŒŒì¸íŠœë‹
==================================================
ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda
GPU: NVIDIA GeForce RTX 4090
ğŸ“ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ìƒì„±: image_segments.json
  ğŸ“‚ ì •ìƒ ì´ë¯¸ì§€ í´ë”: video_sequence_dataset\train\good
  ğŸ“Š JSON ë¡œë“œ ì™„ë£Œ: 144ê°œ ì„¸ê·¸ë¨¼íŠ¸
    ğŸ¬ seq_001_00_frame1_frame_20250728-143012_273.jpg
    ğŸ¬ seq_001_00_frame2_frame_20250728-143012_308.jpg
    ğŸ¬ seq_001_01_frame1_frame_20250728-143012_308.jpg
    ğŸ¬ seq_001_01_frame2_frame_20250728-143012_340.jpg
    ğŸ¬ seq_001_02_frame1_frame_20250728-143012_340.jpg
    ğŸ¬ seq_001_02_frame2_frame_20250728-143012_373.jpg
    ğŸ¬ seq_002_00_frame1_frame_20250728-143442_128.jpg
    ğŸ¬ seq_002_00_frame2_frame_20250728-143442_161.jpg
    ğŸ¬ seq_002_01_frame1_frame_20250728-143442_161.jpg
    ğŸ¬ seq_002_01_frame2_frame_20250728-143442_194.jpg
    ğŸ¬ seq_002_02_frame1_frame_20250728-143442_194.jpg
    ğŸ¬ seq_002_02_frame2_frame_20250728-143442_228.jpg
    ğŸ¬ seq_003_00_frame1_frame_20250728-143600_291.jpg
    ğŸ¬ seq_003_00_frame2_frame_20250728-143600_325.jpg
    ğŸ¬ seq_003_01_frame1_frame_20250728-143600_325.jpg
    ğŸ¬ seq_003_01_frame2_frame_20250728-143600_360.jpg
    ğŸ¬ seq_003_02_frame1_frame_20250728-143600_360.jpg
    ğŸ¬ seq_003_02_frame2_frame_20250728-143600_391.jpg
    ğŸ¬ seq_004_00_frame1_frame_20250728-143626_423.jpg
    ğŸ¬ seq_004_00_frame2_frame_20250728-143626_457.jpg
  âœ… ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸: 23ê°œ
  âœ… ë³µì‚¬ëœ ì´ë¯¸ì§€: 138ê°œ

ğŸ“Š ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±...
ğŸ“¸ ì´ 138ê°œ ì´ë¯¸ì§€ ë°œê²¬
ğŸ¬ 137ê°œ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ìƒì„±
âœ… ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ
   ğŸ“Š ë°°ì¹˜ í¬ê¸°: 2
   ğŸ“Š ì´ ë°°ì¹˜ ìˆ˜: 68

ğŸ¤– AI-VAD ëª¨ë¸ ìƒì„±...
âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ

ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: aivad_proper_checkpoint.ckpt
âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ

ğŸ¯ AI-VAD íŒŒì¸íŠœë‹ ì‹œì‘...

ğŸ“ˆ Epoch 1/2
  âš ï¸ Batch 1 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 2 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 3 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 4 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 5 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  ğŸ“Š í‰ê·  ì†ì‹¤: 0.0000

ğŸ“ˆ Epoch 2/2
  âš ï¸ Batch 1 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 2 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 3 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 4 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  âš ï¸ Batch 5 ì‹¤íŒ¨: element 0 of tensors does not require grad and does not have a grad_fn
  ğŸ“Š í‰ê·  ì†ì‹¤: 0.0000
âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!
ğŸ’¾ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥: aivad_video_sequence_finetuned.ckpt
ğŸ“Š íŒŒì¼ í¬ê¸°: 757.9 MB

ğŸ‰ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ AI-VAD íŒŒì¸íŠœë‹ ì™„ë£Œ!
ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:
1. UIì—ì„œ 'aivad_video_sequence_finetuned.ckpt' ë¡œë“œ
2. ì‹¤ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
3. í•„ìš”ì‹œ ì¶”ê°€ íŒŒì¸íŠœë‹