"""
GPU ì§„ë‹¨ ë„êµ¬ - Windows í™˜ê²½ì—ì„œ GPU ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤
"""

import os
import subprocess
import sys

def check_nvidia_driver():
    """NVIDIA ë“œë¼ì´ë²„ í™•ì¸"""
    print("ğŸ” NVIDIA ë“œë¼ì´ë²„ í™•ì¸ ì¤‘...")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            print("ğŸ“Š GPU ìƒíƒœ:")
            print(result.stdout)
            return True
        else:
            print("âŒ NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ nvidia-smi ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
    except subprocess.TimeoutExpired:
        print("âŒ nvidia-smi ëª…ë ¹ì–´ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
        return False
    except FileNotFoundError:
        print("âŒ nvidia-smi ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âŒ NVIDIA ë“œë¼ì´ë²„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def check_cuda_installation():
    """CUDA ì„¤ì¹˜ í™•ì¸"""
    print("\nğŸ” CUDA ì„¤ì¹˜ í™•ì¸ ì¤‘...")
    
    # CUDA í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    cuda_path = os.environ.get('CUDA_PATH')
    if cuda_path:
        print(f"âœ… CUDA_PATH í™˜ê²½ ë³€ìˆ˜: {cuda_path}")
    else:
        print("âš ï¸  CUDA_PATH í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # CUDA ë²„ì „ í™•ì¸
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… CUDA ì»´íŒŒì¼ëŸ¬ (nvcc)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            print(f"ğŸ“Š CUDA ë²„ì „ ì •ë³´:\n{result.stdout}")
            return True
        else:
            print("âŒ CUDA ì»´íŒŒì¼ëŸ¬ (nvcc)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
    except FileNotFoundError:
        print("âŒ CUDA ì»´íŒŒì¼ëŸ¬ (nvcc)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CUDA íˆ´í‚·ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âŒ CUDA í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def check_pytorch_gpu():
    """PyTorch GPU ì§€ì› í™•ì¸"""
    print("\nğŸ” PyTorch GPU ì§€ì› í™•ì¸ ì¤‘...")
    
    try:
        import torch
        print(f"âœ… PyTorch ë²„ì „: {torch.__version__}")
        
        # CUDA ê°€ìš©ì„± í™•ì¸
        cuda_available = torch.cuda.is_available()
        print(f"CUDA ê°€ìš©ì„±: {cuda_available}")
        
        if cuda_available:
            device_count = torch.cuda.device_count()
            print(f"GPU ê°œìˆ˜: {device_count}")
            
            for i in range(device_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                gpu_capability = torch.cuda.get_device_capability(i)
                print(f"GPU {i}: {gpu_name}")
                print(f"  - ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
                print(f"  - Compute Capability: {gpu_capability[0]}.{gpu_capability[1]}")
            
            # GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
            print("\nğŸ§ª GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                test_tensor = torch.randn(1000, 1000).cuda()
                gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"âœ… GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                print(f"  - í• ë‹¹ëœ ë©”ëª¨ë¦¬: {gpu_memory_allocated:.3f} GB")
                print(f"  - ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {gpu_memory_reserved:.3f} GB")
                del test_tensor
                torch.cuda.empty_cache()
                return True
            except Exception as e:
                print(f"âŒ GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                return False
        else:
            print("âŒ PyTorchì—ì„œ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except ImportError:
        print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âŒ PyTorch GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def check_anomalib_gpu():
    """Anomalib GPU ì§€ì› í™•ì¸"""
    print("\nğŸ” Anomalib GPU ì§€ì› í™•ì¸ ì¤‘...")
    
    try:
        from anomalib.models.video import AiVad
        print("âœ… Anomalibê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # ê°„ë‹¨í•œ ëª¨ë¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        try:
            model = AiVad()
            print("âœ… AI-VAD ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            if torch.cuda.is_available():
                model = model.cuda()
                print("âœ… AI-VAD ëª¨ë¸ì´ GPUë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            print(f"âŒ AI-VAD ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
            
    except ImportError:
        print("âŒ Anomalibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âŒ Anomalib GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def provide_gpu_solution():
    """GPU ì‚¬ìš©ì„ ìœ„í•œ í•´ê²°ì±… ì œì‹œ"""
    print("\n" + "="*60)
    print("ğŸ’¡ GPU ì‚¬ìš©ì„ ìœ„í•œ í•´ê²°ì±…")
    print("="*60)
    
    print("\n1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜:")
    print("   - NVIDIA ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ")
    print("   - https://www.nvidia.com/drivers/")
    
    print("\n2. CUDA íˆ´í‚· ì„¤ì¹˜:")
    print("   - CUDA 11.8 ë˜ëŠ” 12.1 ë²„ì „ ì„¤ì¹˜ ê¶Œì¥")
    print("   - https://developer.nvidia.com/cuda-downloads")
    
    print("\n3. PyTorch GPU ë²„ì „ ì„¤ì¹˜:")
    print("   # CUDA 11.8ìš©")
    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    print("   ")
    print("   # CUDA 12.1ìš©")
    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    
    print("\n4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:")
    print("   - CUDA_PATH í™˜ê²½ ë³€ìˆ˜ ì„¤ì •")
    print("   - PATHì— CUDA bin ë””ë ‰í† ë¦¬ ì¶”ê°€")
    
    print("\n5. ì‹œìŠ¤í…œ ì¬ë¶€íŒ…:")
    print("   - ë“œë¼ì´ë²„ ì„¤ì¹˜ í›„ ì‹œìŠ¤í…œ ì¬ë¶€íŒ… í•„ìš”")

def main():
    print("ğŸ”§ GPU ì§„ë‹¨ ë„êµ¬")
    print("="*60)
    
    # ê° ë‹¨ê³„ë³„ í™•ì¸
    nvidia_ok = check_nvidia_driver()
    cuda_ok = check_cuda_installation()
    pytorch_ok = check_pytorch_gpu()
    anomalib_ok = check_anomalib_gpu()
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ì§„ë‹¨ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    print(f"NVIDIA ë“œë¼ì´ë²„: {'âœ… OK' if nvidia_ok else 'âŒ FAIL'}")
    print(f"CUDA ì„¤ì¹˜: {'âœ… OK' if cuda_ok else 'âŒ FAIL'}")
    print(f"PyTorch GPU: {'âœ… OK' if pytorch_ok else 'âŒ FAIL'}")
    print(f"Anomalib GPU: {'âœ… OK' if anomalib_ok else 'âŒ FAIL'}")
    
    if all([nvidia_ok, cuda_ok, pytorch_ok, anomalib_ok]):
        print("\nğŸ‰ ëª¨ë“  GPU êµ¬ì„± ìš”ì†Œê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        print("GPU ê°€ì† í•™ìŠµì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸  GPU êµ¬ì„± ìš”ì†Œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        provide_gpu_solution()

if __name__ == "__main__":
    main()
