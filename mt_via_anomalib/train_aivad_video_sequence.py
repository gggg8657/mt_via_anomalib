"""
ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ AI-VAD íŒŒì¸íŠœë‹
ì´ë¯¸ì§€ í”„ë ˆì„ë“¤ì„ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import os
import json
import torch
import cv2
import numpy as np
from pathlib import Path
from anomalib.data import Folder
from anomalib.models.video import AiVad
from anomalib.engine import Engine
import shutil
from torch.utils.data import Dataset, DataLoader

class VideoSequenceDataset(Dataset):
    """ì´ë¯¸ì§€ë“¤ì„ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë°ì´í„°ì…‹"""
    
    def __init__(self, image_folder, sequence_length=2, transform=None):
        self.image_folder = Path(image_folder)
        self.sequence_length = sequence_length
        self.transform = transform
        
        # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
        self.image_files = sorted([f for f in self.image_folder.glob("*.jpg")])
        print(f"ğŸ“¸ ì´ {len(self.image_files)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        
        # ì‹œí€€ìŠ¤ ìƒì„± (ì—°ì†ëœ í”„ë ˆì„ë“¤)
        self.sequences = []
        for i in range(len(self.image_files) - sequence_length + 1):
            sequence = self.image_files[i:i + sequence_length]
            self.sequences.append(sequence)
        
        print(f"ğŸ¬ {len(self.sequences)}ê°œ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ìƒì„±")
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence_paths = self.sequences[idx]
        frames = []
        
        # ê° ì‹œí€€ìŠ¤ì—ì„œ í”„ë ˆì„ ë¡œë“œ
        for path in sequence_paths:
            frame = cv2.imread(str(path))
            if frame is None:
                # ë¹ˆ í”„ë ˆì„ ìƒì„±
                frame = np.zeros((224, 224, 3), dtype=np.uint8)
            else:
                # 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                frame = cv2.resize(frame, (224, 224))
            
            # BGR to RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ì •ê·œí™” [0, 1]
            frame = frame.astype(np.float32) / 255.0
            
            # CHW í˜•íƒœë¡œ ë³€í™˜
            frame = np.transpose(frame, (2, 0, 1))
            frames.append(frame)
        
        # í…ì„œë¡œ ë³€í™˜ [sequence_length, 3, 224, 224]
        video_tensor = torch.from_numpy(np.array(frames))
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ [1, sequence_length, 3, 224, 224]
        video_tensor = video_tensor.unsqueeze(0)
        
        return {
            'image': video_tensor,
            'label': torch.tensor(0),  # ì •ìƒ ë°ì´í„°
            'image_path': str(sequence_paths[0])  # ì²« ë²ˆì§¸ í”„ë ˆì„ ê²½ë¡œ
        }

def create_video_sequences_from_json(json_path="image_segments.json", target_dir="video_sequence_dataset"):
    """JSONì—ì„œ ì •ìƒ í”„ë ˆì„ë“¤ì„ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
    print(f"ğŸ“ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ìƒì„±: {json_path}")
    
    # í´ë” ìƒì„±
    normal_dir = Path(target_dir) / "train" / "good"
    normal_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"  ğŸ“‚ ì •ìƒ ì´ë¯¸ì§€ í´ë”: {normal_dir}")
    
    # JSON íŒŒì¼ ë¡œë“œ
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            segments = json.load(f)
        print(f"  ğŸ“Š JSON ë¡œë“œ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    except Exception as e:
        print(f"âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    # ì •ìƒ í”„ë ˆì„ë“¤ ì¶”ì¶œ
    copied_count = 0
    normal_count = 0
    
    for i, segment in enumerate(segments):
        if segment.get('category') == 'normal' and 'images' in segment:
            normal_count += 1
            images = segment['images']
            
            # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì—°ì†ëœ í”„ë ˆì„ë“¤ ì‚¬ìš© (ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ìš©)
            for j in range(len(images) - 1):  # ì—°ì†ëœ 2ê°œì”©
                if j < 3:  # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìµœëŒ€ 3ê°œ ì‹œí€€ìŠ¤
                    img1_path = images[j]
                    img2_path = images[j + 1]
                    
                    if os.path.exists(img1_path) and os.path.exists(img2_path):
                        # íŒŒì¼ëª… ìƒì„±
                        name1 = f"seq_{normal_count:03d}_{j:02d}_frame1_{Path(img1_path).name}"
                        name2 = f"seq_{normal_count:03d}_{j:02d}_frame2_{Path(img2_path).name}"
                        
                        target1 = normal_dir / name1
                        target2 = normal_dir / name2
                        
                        try:
                            # íŒŒì¼ ë³µì‚¬
                            shutil.copy2(img1_path, target1)
                            shutil.copy2(img2_path, target2)
                            copied_count += 2
                            
                            if copied_count <= 20:  # ì²˜ìŒ 20ê°œë§Œ í‘œì‹œ
                                print(f"    ğŸ¬ {name1}")
                                print(f"    ğŸ¬ {name2}")
                                
                        except Exception as e:
                            print(f"    âš ï¸ {img1_path} ë³µì‚¬ ì‹¤íŒ¨: {e}")
    
    print(f"  âœ… ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸: {normal_count}ê°œ")
    print(f"  âœ… ë³µì‚¬ëœ ì´ë¯¸ì§€: {copied_count}ê°œ")
    
    if copied_count == 0:
        print("âŒ ë³µì‚¬ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    return str(Path(target_dir).absolute())

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ AI-VAD íŒŒì¸íŠœë‹")
    print("=" * 50)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # 1. ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ìƒì„±
    dataset_root = create_video_sequences_from_json()
    if dataset_root is None:
        print("âŒ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨")
        return
    
    # 2. ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ“Š ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±...")
    try:
        normal_dir = Path(dataset_root) / "train" / "good"
        video_dataset = VideoSequenceDataset(normal_dir, sequence_length=2)
        
        # DataLoader ìƒì„±
        dataloader = DataLoader(
            video_dataset,
            batch_size=2,  # ì‘ì€ ë°°ì¹˜ í¬ê¸°
            shuffle=True,
            num_workers=0,
            drop_last=True
        )
        
        print(f"âœ… ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
        print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {dataloader.batch_size}")
        print(f"   ğŸ“Š ì´ ë°°ì¹˜ ìˆ˜: {len(dataloader)}")
        
    except Exception as e:
        print(f"âŒ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 3. AI-VAD ëª¨ë¸ ìƒì„±
    print(f"\nğŸ¤– AI-VAD ëª¨ë¸ ìƒì„±...")
    try:
        model = AiVad()
        print("âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ AI-VAD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 4. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    checkpoint_path = "aivad_proper_checkpoint.ckpt"
    if os.path.exists(checkpoint_path):
        print(f"\nğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, weights_only=False)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
                print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 5. ê°„ë‹¨í•œ í›ˆë ¨ (PyTorch Lightning ì—†ì´)
    print(f"\nğŸ¯ AI-VAD íŒŒì¸íŠœë‹ ì‹œì‘...")
    try:
        model.to(device)
        
        # ëª¨ë¸ì„ train ëª¨ë“œë¡œ ì„¤ì • (gradient í™œì„±í™”)
        model.train()
        model.model.train()  # ë‚´ë¶€ ëª¨ë¸ë„ train ëª¨ë“œ
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ gradient í™œì„±í™”
        for param in model.parameters():
            param.requires_grad = True
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
        
        # í›ˆë ¨ ë£¨í”„
        for epoch in range(10):  # 10 ì—í¬í¬ë¡œ ì¦ê°€
            print(f"\nğŸ“ˆ Epoch {epoch + 1}/10")
            
            total_loss = 0
            batch_count = 0
            
            for batch_idx, batch in enumerate(dataloader):
                # ë” ë§ì€ ë°°ì¹˜ í›ˆë ¨ (ì „ì²´ ë°ì´í„° ì‚¬ìš©)
                if batch_idx >= len(dataloader):  # ì „ì²´ ë°°ì¹˜ ì‚¬ìš©
                    break
                
                # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                video_tensor = batch['image'].to(device)  # [batch_size, 1, 2, 3, 224, 224]
                
                # ë°°ì¹˜ ì°¨ì› ì¡°ì • [batch_size, 2, 3, 224, 224]
                video_tensor = video_tensor.squeeze(1)
                
                try:
                    # ëª¨ë¸ ìˆœì „íŒŒ
                    optimizer.zero_grad()
                    
                    # AI-VAD ëª¨ë¸ í˜¸ì¶œ
                    output = model.model(video_tensor)
                    
                    # AI-VAD ì¶œë ¥ êµ¬ì¡° ë””ë²„ê¹…
                    print(f"    ğŸ” Output type: {type(output)}")
                    if hasattr(output, '__dict__'):
                        print(f"    ğŸ” Output attributes: {list(output.__dict__.keys())}")
                    elif isinstance(output, (list, tuple)):
                        print(f"    ğŸ” Output length: {len(output)}")
                        for i, item in enumerate(output):
                            print(f"      [{i}] Type: {type(item)}, Shape: {getattr(item, 'shape', 'No shape')}")
                    
                    # ì‹¤ì œ ì†ì‹¤ ê³„ì‚°
                    loss = None
                    
                    # Case 1: outputì´ dict í˜•íƒœì¸ ê²½ìš°
                    if isinstance(output, dict):
                        if 'pred_score' in output:
                            pred_score = output['pred_score'].mean()
                            loss = torch.abs(pred_score - 0.1)
                            print(f"    ğŸ“Š Dict pred_score: {pred_score.item():.4f}")
                        elif 'anomaly_score' in output:
                            anomaly_score = output['anomaly_score'].mean()
                            loss = torch.abs(anomaly_score - 0.1)
                            print(f"    ğŸ“Š Dict anomaly_score: {anomaly_score.item():.4f}")
                    
                    # Case 2: outputì´ tensorì¸ ê²½ìš°
                    elif torch.is_tensor(output):
                        loss = torch.abs(output.mean() - 0.1)
                        print(f"    ğŸ“Š Tensor output: {output.mean().item():.4f}")
                    
                    # Case 3: outputì´ list/tupleì¸ ê²½ìš°
                    elif isinstance(output, (list, tuple)) and len(output) > 0:
                        first_item = output[0]
                        if torch.is_tensor(first_item):
                            loss = torch.abs(first_item.mean() - 0.1)
                            print(f"    ğŸ“Š List[0] tensor: {first_item.mean().item():.4f}")
                    
                    # Case 4: outputì´ ê°ì²´ì¸ ê²½ìš°
                    elif hasattr(output, 'pred_score'):
                        pred_score = output.pred_score.mean()
                        loss = torch.abs(pred_score - 0.1)
                        print(f"    ğŸ“Š Object pred_score: {pred_score.item():.4f}")
                    
                    # Case 5: ëª¨ë“  ê²½ìš° ì‹¤íŒ¨ì‹œ ë”ë¯¸ ì†ì‹¤
                    if loss is None:
                        print(f"    âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì¶œë ¥ í˜•íƒœ, ë”ë¯¸ ì†ì‹¤ ì‚¬ìš©")
                        # ì‹¤ì œ gradientê°€ ìˆëŠ” ë”ë¯¸ ì†ì‹¤
                        video_mean = video_tensor.mean()
                        loss = torch.abs(video_mean - 0.5)  # ì´ë¯¸ì§€ í‰ê· ê°’ ê¸°ë°˜
                        print(f"    ğŸ“Š ë”ë¯¸ ì†ì‹¤ (ì´ë¯¸ì§€ í‰ê·  ê¸°ë°˜): {loss.item():.4f}")
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    batch_count += 1
                    
                    # ì§„í–‰ë¥  í‘œì‹œ (10ë°°ì¹˜ë§ˆë‹¤)
                    if batch_idx % 10 == 0:
                        print(f"  Batch {batch_idx + 1}/{len(dataloader)}: Loss = {loss.item():.4f}")
                    
                except Exception as e:
                    print(f"  âš ï¸ Batch {batch_idx + 1} ì‹¤íŒ¨: {e}")
                    continue
            
            if batch_count > 0:
                avg_loss = total_loss / batch_count
                print(f"  ğŸ“Š í‰ê·  ì†ì‹¤: {avg_loss:.4f} (ì´ {batch_count}ê°œ ë°°ì¹˜)")
            else:
                print(f"  âš ï¸ ì„±ê³µí•œ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 6. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = "aivad_video_sequence_finetuned.ckpt"
    try:
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'video_sequence_finetuned'
        }, checkpoint_path)
        
        print(f"ğŸ’¾ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ AI-VAD íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. UIì—ì„œ 'aivad_video_sequence_finetuned.ckpt' ë¡œë“œ")
    print("2. ì‹¤ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("3. í•„ìš”ì‹œ ì¶”ê°€ íŒŒì¸íŠœë‹")

if __name__ == "__main__":
    main()
