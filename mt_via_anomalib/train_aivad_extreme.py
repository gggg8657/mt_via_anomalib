"""
AI-VAD ê·¹ë‹¨ì ì¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
ëª¨ë“  ë¹„ë””ì˜¤ì—ì„œ ê°ì²´ ê°ì§€ ì‹¤íŒ¨ ë¬¸ì œ í•´ê²°
"""

import os
import torch
import cv2
import numpy as np
from pathlib import Path
from anomalib.models.video import AiVad

def analyze_video_content_detailed(video_path):
    """ë¹„ë””ì˜¤ ë‚´ìš© ìƒì„¸ ë¶„ì„"""
    print(f"ğŸ” ìƒì„¸ ë¹„ë””ì˜¤ ë¶„ì„: {Path(video_path).name}")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"   âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
        return False, 0, []
    
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"   ğŸ“Š í•´ìƒë„: {width}x{height}, í”„ë ˆì„: {frame_count}, FPS: {fps:.1f}")
    
    # ë” ìƒì„¸í•œ ì›€ì§ì„ ê°ì§€
    prev_frame = None
    motion_scores = []
    edge_scores = []
    
    for i in range(min(20, frame_count)):  # ì²˜ìŒ 20í”„ë ˆì„ ë¶„ì„
        ret, frame = cap.read()
        if not ret:
            break
        
        # í”„ë ˆì„ ì „ì²˜ë¦¬
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ì—ì§€ ê°ì§€
        edges = cv2.Canny(frame_gray, 50, 150)
        edge_score = np.mean(edges)
        edge_scores.append(edge_score)
        
        if prev_frame is not None:
            # ì›€ì§ì„ ê°ì§€
            diff = cv2.absdiff(prev_frame, frame_gray)
            motion_score = np.mean(diff)
            motion_scores.append(motion_score)
        
        prev_frame = frame_gray.copy()
    
    cap.release()
    
    avg_motion = np.mean(motion_scores) if motion_scores else 0
    avg_edge = np.mean(edge_scores) if edge_scores else 0
    max_motion = np.max(motion_scores) if motion_scores else 0
    
    print(f"   ğŸƒ í‰ê·  ì›€ì§ì„: {avg_motion:.2f}, ìµœëŒ€ ì›€ì§ì„: {max_motion:.2f}")
    print(f"   ğŸ“ í‰ê·  ì—ì§€: {avg_edge:.2f}")
    print(f"   ğŸ¯ ì›€ì§ì„ ê°ì§€: {'âœ…' if max_motion > 2 else 'âŒ'}")
    
    return max_motion > 2, max_motion, motion_scores

def get_video_files_from_directory(video_dir):
    """ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°"""
    print(f"ğŸ“ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {video_dir}")
    
    if not os.path.exists(video_dir):
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_dir}")
        return []
    
    video_extensions = ['.avi', '.mp4', '.mov', '.mkv', '.flv', '.wmv']
    video_files = []
    
    # ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    for file_path in Path(video_dir).rglob('*'):
        if file_path.is_file() and file_path.suffix.lower() in video_extensions:
            video_files.append(str(file_path))
    
    print(f"âœ… ë°œê²¬ëœ ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
    
    # ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì •ë ¬
    video_files.sort()
    
    return video_files

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI-VAD ê·¹ë‹¨ì ì¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ")
    print("=" * 60)
    print("ğŸ’¡ ê·¹ë‹¨ì ì¸ ì„¤ì •:")
    print("   1. box_score_thresh=0.05 (ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ìŒ)")
    print("   2. min_bbox_area=10 (ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ìŒ)")
    print("   3. max_bbox_overlap=0.95 (ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ìŒ)")
    print("   4. foreground_binary_threshold=2 (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°)")
    print("   5. ìƒì„¸í•œ ë¹„ë””ì˜¤ ë¶„ì„ ë° ì›€ì§ì„ ê°ì§€")
    print("=" * 60)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # 1. ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ì„¤ì •
    video_directory = input("\nğŸ“ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    if not video_directory:
        # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        video_directory = r"C:\Users\User\PycharmProjects\pythonProject1\mt_via_anomalib\mt_via_anomalib\cropped_videos\normal"
        print(f"ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {video_directory}")
    
    # 2. ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
    video_files = get_video_files_from_directory(video_directory)
    
    if len(video_files) == 0:
        print("âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # 3. ëª¨ë“  ë¹„ë””ì˜¤ í™œìš© (ë¶„ì„ ì—†ì´ ì§ì ‘ ì²˜ë¦¬)
    print(f"\nğŸ” ëª¨ë“  ë¹„ë””ì˜¤ í™œìš© ëª¨ë“œ...")
    print(f"ğŸ’¡ ë¶„ì„ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ëª¨ë“  ë¹„ë””ì˜¤ë¥¼ ì§ì ‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    
    # ëª¨ë“  ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
    motion_videos = [(vf, 1.0) for vf in video_files]
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ:")
    print(f"   - ì „ì²´ ë¹„ë””ì˜¤: {len(video_files)}ê°œ")
    print(f"   - ì²˜ë¦¬í•  ë¹„ë””ì˜¤: {len(motion_videos)}ê°œ")
    print(f"   - ì²˜ë¦¬ ë°©ì‹: ëª¨ë“  ë¹„ë””ì˜¤ ì§ì ‘ ì²˜ë¦¬")
    
    # 4. AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ì¸ ì„¤ì •)
    print(f"\nğŸ¤– AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ì¸ ì„¤ì •)...")
    try:
        model = AiVad(
            # AI-VAD ê¸°ë³¸ ì„¤ì •
            use_velocity_features=True,
            use_pose_features=True,
            use_deep_features=True,
            # Density estimation ì„¤ì •
            n_components_velocity=2,
            n_neighbors_pose=1,
            n_neighbors_deep=1,
            # ê·¹ë‹¨ì ì¸ ê°ì²´ ê°ì§€ ì„¤ì •
            box_score_thresh=0.05,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì¶¤ (0.1 -> 0.05)
            min_bbox_area=10,       # ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ê²Œ (25 -> 10)
            max_bbox_overlap=0.95,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ê²Œ (0.9 -> 0.95)
            foreground_binary_threshold=2,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê° (5 -> 2)
        )
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™
        if device == "cuda":
            model = model.to(device)
            print(f"âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (GPU: {device})")
        else:
            print("âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (CPU)")
        
    except Exception as e:
        print(f"âŒ AI-VAD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 5. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    checkpoint_path = "aivad_proper_checkpoint.ckpt"
    if os.path.exists(checkpoint_path):
        print(f"\nğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
                if device == "cuda":
                    model = model.to(device)
                print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 6. AI-VAD ì§ì ‘ í•™ìŠµ (ê·¹ë‹¨ì ì¸ ì„¤ì •)
    print(f"\nğŸ¯ AI-VAD ê·¹ë‹¨ì ì¸ í•™ìŠµ ì‹œì‘...")
    print("ğŸ’¡ í•™ìŠµ ê³¼ì •:")
    print("   1. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("   2. ëª¨ë“  ì›€ì§ì„ê³¼ ë³€í™” ê°ì§€")
    print("   3. Density Update: ì •ìƒ íŠ¹ì„±ë“¤ì„ density estimatorì— ëˆ„ì ")
    print("   4. Density Fit: ëª¨ë“  íŠ¹ì„±ìœ¼ë¡œ ë¶„í¬ ëª¨ë¸ í•™ìŠµ")
    
    try:
        # ì§ì ‘ì ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° AI-VAD í•™ìŠµ
        model.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        
        # ëª¨ë¸ì´ GPUì— ìˆëŠ”ì§€ í™•ì¸
        print(f"ğŸ” ëª¨ë¸ ë””ë°”ì´ìŠ¤ í™•ì¸:")
        print(f"   - ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.model.parameters()).device}")
        print(f"   - íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤: {device}")
        
        if device == "cuda" and next(model.model.parameters()).device.type != "cuda":
            print("âš ï¸ ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì¤‘...")
            model.model = model.model.to(device)
        
        total_clips_processed = 0
        total_detections = 0
        
        # ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬ (ì²˜ë¦¬ ì‹œê°„ì„ ê³ ë ¤í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)
        batch_size = 10  # í•œ ë²ˆì— 10ê°œì”© ì²˜ë¦¬
        total_batches = (len(motion_videos) + batch_size - 1) // batch_size
        
        print(f"ğŸ“Š ì²˜ë¦¬ ê³„íš:")
        print(f"   - ì „ì²´ ë¹„ë””ì˜¤: {len(motion_videos)}ê°œ")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ")
        print(f"   - ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(motion_videos))
            batch_videos = motion_videos[start_idx:end_idx]
            
            print(f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            print(f"   - ì²˜ë¦¬ ë²”ìœ„: {start_idx + 1}~{end_idx}")
            
            for i, (video_path, motion_score) in enumerate(batch_videos):
                video_idx = start_idx + i + 1
                print(f"\nğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {video_idx}/{len(motion_videos)}")
                print(f"   íŒŒì¼: {Path(video_path).name}")
                print(f"   ì›€ì§ì„ ì ìˆ˜: {motion_score:.2f}")
            
                try:
                    # ë¹„ë””ì˜¤ ë¡œë“œ
                    cap = cv2.VideoCapture(video_path)
                    if not cap.isOpened():
                        print(f"   âš ï¸ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
                        continue
                    
                    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    fps = cap.get(cv2.CAP_PROP_FPS)
                    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    
                    print(f"   ğŸ“Š í•´ìƒë„: {width}x{height}, í”„ë ˆì„: {frame_count}, FPS: {fps:.1f}")
                    
                    # í•´ìƒë„ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê²½ê³ 
                    if width < 224 or height < 224:
                        print(f"   âš ï¸ í•´ìƒë„ê°€ ì‘ìŠµë‹ˆë‹¤: {width}x{height} (ê¶Œì¥: 224x224 ì´ìƒ)")
                    
                    # 2í”„ë ˆì„ì”© í´ë¦½ìœ¼ë¡œ ì²˜ë¦¬
                    clip_count = 0
                    frame_buffer = []
                    
                    while True:
                        ret, frame = cap.read()
                        if not ret:
                            break
                        
                        # í”„ë ˆì„ ì „ì²˜ë¦¬ (ë” í° í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)
                        frame_resized = cv2.resize(frame, (224, 224))
                        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
                        frame_tensor = torch.from_numpy(frame_rgb).float() / 255.0
                        frame_tensor = frame_tensor.permute(2, 0, 1)  # HWC -> CHW
                        
                        frame_buffer.append(frame_tensor)
                        
                        # 2í”„ë ˆì„ì´ ëª¨ì´ë©´ í´ë¦½ ì²˜ë¦¬
                        if len(frame_buffer) == 2:
                            try:
                                # ë¹„ë””ì˜¤ í´ë¦½ ìƒì„± [2, 3, 224, 224]
                                video_clip = torch.stack(frame_buffer).unsqueeze(0)  # [1, 2, 3, 224, 224]
                                
                                # ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì´ë™
                                if device == "cuda":
                                    video_clip = video_clip.to(device)
                                
                                # AI-VAD ì¶”ë¡  (ê·¹ë‹¨ì ì¸ ì„¤ì •)
                                try:
                                    with torch.no_grad():
                                        output = model.model(video_clip)
                                        
                                        # ì¶œë ¥ êµ¬ì¡° í™•ì¸
                                        if isinstance(output, list) and len(output) > 0:
                                            # íŠ¹ì„± ì¶”ì¶œ ë° density estimator ì—…ë°ì´íŠ¸
                                            if hasattr(model.model, 'density_estimator'):
                                                # AI-VADì˜ ë‚´ë¶€ íŠ¹ì„±ë“¤ì„ density estimatorì— ì¶”ê°€
                                                model.model.density_estimator.update(output)
                                                total_detections += 1
                                                
                                            clip_count += 1
                                            total_clips_processed += 1
                                            
                                            if clip_count == 1:  # ì²« ë²ˆì§¸ ì„±ê³µí•œ í´ë¦½
                                                print(f"   ğŸ‰ ì²« ë²ˆì§¸ ê°ì²´ ê°ì§€ ì„±ê³µ!")
                                            elif clip_count % 5 == 0:
                                                print(f"   âœ… ì²˜ë¦¬ëœ í´ë¦½: {clip_count}")
                                            
                                        else:
                                            # ì¶œë ¥ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¦„
                                            if clip_count == 0:  # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ì¶œë ¥
                                                print(f"   âš ï¸ AI-VAD ì¶œë ¥ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¦„: {type(output)}")
                                        
                                except Exception as e:
                                    if "index 0 is out of bounds" in str(e):
                                        # Region Extractorì—ì„œ ê°ì²´ ê°ì§€ ì‹¤íŒ¨
                                        if clip_count == 0:  # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ì¶œë ¥
                                            print(f"   âš ï¸ ê°ì²´ ê°ì§€ ì‹¤íŒ¨: Region Extractorê°€ ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•¨")
                                        elif "amax(): Expected reduction dim 0" in str(e):
                                            # ë¹ˆ í…ì„œ ë¬¸ì œ
                                            if clip_count == 0:
                                                print(f"   âš ï¸ ë¹ˆ í…ì„œ ë¬¸ì œ: amax() ì—ëŸ¬")
                                        else:
                                            if clip_count == 0:
                                                print(f"   âš ï¸ AI-VAD ì¶”ë¡  ì‹¤íŒ¨: {e}")
                                        continue
                                
                            except Exception as e:
                                # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ìƒì„¸ ì—ëŸ¬ ì¶œë ¥
                                if clip_count == 0:
                                    print(f"   âš ï¸ í´ë¦½ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            
                            # ë²„í¼ì—ì„œ ì²« ë²ˆì§¸ í”„ë ˆì„ ì œê±°
                            frame_buffer.pop(0)
                    
                    cap.release()
                    print(f"   âœ… ì™„ë£Œ: {clip_count}ê°œ í´ë¦½ ì²˜ë¦¬")
                    
                except Exception as e:
                    print(f"   âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            # ë°°ì¹˜ ì™„ë£Œ í›„ ì§„í–‰ ìƒí™© ì¶œë ¥
            processed_videos = min(end_idx, len(motion_videos))
            print(f"\nâœ… ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì™„ë£Œ")
            print(f"   - ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {processed_videos}/{len(motion_videos)}")
            print(f"   - ì²˜ë¦¬ëœ í´ë¦½: {total_clips_processed}")
            print(f"   - ì´ ê°ì§€ ìˆ˜: {total_detections}")
        
        print(f"\nğŸ“Š ì „ì²´ ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   - ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {len(motion_videos)}ê°œ")
        print(f"   - ì²˜ë¦¬ëœ í´ë¦½: {total_clips_processed}ê°œ")
        print(f"   - ì´ ê°ì§€ ìˆ˜: {total_detections}ê°œ")
        
        # Density estimator ìµœì¢… í•™ìŠµ
        if hasattr(model.model, 'density_estimator') and total_detections > 0:
            print(f"\nğŸ”§ Density Estimator ìµœì¢… í•™ìŠµ...")
            model.model.density_estimator.fit()
            print("âœ… Density Estimator í•™ìŠµ ì™„ë£Œ")
        else:
            print("âš ï¸ ê°ì§€ëœ íŠ¹ì„±ì´ ì—†ì–´ density estimator í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            print("ğŸ’¡ ê·¼ë³¸ì ì¸ ë¬¸ì œ:")
            print("   1. ë¹„ë””ì˜¤ í•´ìƒë„ê°€ ë„ˆë¬´ ì‘ìŒ (270x480)")
            print("   2. ë¹„ë””ì˜¤ì— ì‹¤ì œ ì›€ì§ì´ëŠ” ê°ì²´ê°€ ì—†ìŒ")
            print("   3. ë¹„ë””ì˜¤ í’ˆì§ˆì´ ë‚®ìŒ (ì••ì¶•, ë…¸ì´ì¦ˆ)")
            print("   4. ì¡°ëª…ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ì¼ì •í•¨")
            print("ğŸ’¡ í•´ê²° ë°©ë²•:")
            print("   1. ë” í° í•´ìƒë„ì˜ ë¹„ë””ì˜¤ ì‚¬ìš© (ìµœì†Œ 640x480)")
            print("   2. ì‹¤ì œ ì›€ì§ì´ëŠ” ì‚¬ëŒ/ê°ì²´ê°€ ìˆëŠ” ë¹„ë””ì˜¤ ì‚¬ìš©")
            print("   3. ì¡°ëª…ì´ ì¶©ë¶„í•˜ê³  ë³€í™”ê°€ ìˆëŠ” í™˜ê²½")
            print("   4. ì••ì¶•ë˜ì§€ ì•Šì€ ê³ í’ˆì§ˆ ë¹„ë””ì˜¤ ì‚¬ìš©")
        
        print("âœ… AI-VAD ê·¹ë‹¨ì ì¸ í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ AI-VAD ê·¹ë‹¨ì ì¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 7. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = "aivad_extreme_learned.ckpt"
    try:
        # AI-VAD ëª¨ë¸ ìƒíƒœ ì €ì¥
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'extreme_density_estimation',
            'total_clips_processed': total_clips_processed,
            'total_detections': total_detections,
            'video_directory': video_directory,
        }, checkpoint_path)
        
        print(f"ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ AI-VAD ê·¹ë‹¨ì ì¸ í•™ìŠµ ì™„ë£Œ!")
    print("ğŸ’¡ í•™ìŠµëœ ë‚´ìš©:")
    print("1. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("2. ëª¨ë“  ì›€ì§ì„ê³¼ ë³€í™” ê°ì§€")
    print("3. ìƒì„¸í•œ ë¹„ë””ì˜¤ ë¶„ì„")
    print("4. Density Estimatorë¡œ ì´ìƒ íƒì§€ ì¤€ë¹„")
    print("5. UIì—ì„œ 'aivad_extreme_learned.ckpt' ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()
