"""
GPU ê°ì§€ ë¬¸ì œ í•´ê²° ë° ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import torch
import subprocess
import sys

def check_nvidia_smi():
    """nvidia-smië¡œ GPU ì •ë³´ í™•ì¸"""
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        print("=== nvidia-smi ì¶œë ¥ ===")
        print(result.stdout)
        return True
    except FileNotFoundError:
        print("âŒ nvidia-smië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

def check_pytorch_gpu():
    """PyTorch GPU ê°ì§€ í™•ì¸"""
    print("\n=== PyTorch GPU ì •ë³´ ===")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"CUDA version: {torch.version.cuda}")
    print(f"cuDNN version: {torch.backends.cudnn.version()}")
    print(f"GPU count: {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  - ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB")
            print(f"  - Compute Capability: {props.major}.{props.minor}")
            print(f"  - ë©€í‹°í”„ë¡œì„¸ì„œ: {props.multi_processor_count}ê°œ")

def fix_gpu_detection():
    """GPU ê°ì§€ ë¬¸ì œ í•´ê²°"""
    print("\n=== GPU ê°ì§€ ë¬¸ì œ í•´ê²° ===")
    
    # 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'  # ëª¨ë“  GPU ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['TORCH_USE_CUDA_DSA'] = '1'
    
    # 2. PyTorch ì¬ì´ˆê¸°í™”
    if 'torch.cuda' in sys.modules:
        del sys.modules['torch.cuda']
        import importlib
        importlib.reload(torch.cuda)
    
    # 3. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    print("âœ… GPU í™˜ê²½ ì„¤ì • ì™„ë£Œ")

def test_gpu_usage():
    """GPU ì‚¬ìš© í…ŒìŠ¤íŠ¸"""
    print("\n=== GPU ì‚¬ìš© í…ŒìŠ¤íŠ¸ ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚°ìœ¼ë¡œ GPU í…ŒìŠ¤íŠ¸
        device = torch.device("cuda:0")
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        z = torch.mm(x, y)
        
        print(f"âœ… GPU 0 í…ŒìŠ¤íŠ¸ ì„±ê³µ: {z.shape}")
        print(f"GPU 0 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB")
        
        # ì—¬ëŸ¬ GPU í…ŒìŠ¤íŠ¸
        for i in range(min(4, torch.cuda.device_count())):
            try:
                device = torch.device(f"cuda:{i}")
                x = torch.randn(100, 100, device=device)
                y = torch.randn(100, 100, device=device)
                z = torch.mm(x, y)
                print(f"âœ… GPU {i} í…ŒìŠ¤íŠ¸ ì„±ê³µ: {z.shape}")
            except Exception as e:
                print(f"âŒ GPU {i} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def optimize_for_h100():
    """H100 GPU ìµœì í™” ì„¤ì •"""
    print("\n=== H100 GPU ìµœì í™” ===")
    
    if not torch.cuda.is_available():
        return
    
    # H100 íŠ¹í™” ì„¤ì •
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # Mixed precision ì„¤ì •
    torch.set_float32_matmul_precision('medium')
    
    # ë©”ëª¨ë¦¬ í’€ ìµœì í™”
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    print("âœ… H100 ìµœì í™” ì„¤ì • ì™„ë£Œ")

def create_gpu_test_script():
    """GPU í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    script_content = '''
import torch
import time

def benchmark_gpu():
    """GPU ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    if not torch.cuda.is_available():
        print("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    device = torch.device("cuda:0")
    print(f"GPU ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {torch.cuda.get_device_name(0)}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    # í° í–‰ë ¬ ê³±ì…ˆ í…ŒìŠ¤íŠ¸
    size = 4096
    a = torch.randn(size, size, device=device, dtype=torch.float32)
    b = torch.randn(size, size, device=device, dtype=torch.float32)
    
    # ì›Œë°ì—…
    for _ in range(3):
        c = torch.mm(a, b)
    torch.cuda.synchronize()
    
    # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
    start_time = time.time()
    for _ in range(10):
        c = torch.mm(a, b)
    torch.cuda.synchronize()
    end_time = time.time()
    
    avg_time = (end_time - start_time) / 10
    peak_memory = torch.cuda.max_memory_allocated() / 1024**2
    
    print(f"í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time*1000:.2f} ms")
    print(f"ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.1f} MB")
    print(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {peak_memory/(size*size*4*2/1024**2):.1f}x")

if __name__ == "__main__":
    benchmark_gpu()
'''
    
    with open("gpu_benchmark.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ… GPU ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: gpu_benchmark.py")

def main():
    print("ğŸ”§ GPU ê°ì§€ ë° ìµœì í™” ë„êµ¬")
    print("=" * 50)
    
    # 1. nvidia-smi í™•ì¸
    check_nvidia_smi()
    
    # 2. PyTorch GPU ì •ë³´
    check_pytorch_gpu()
    
    # 3. GPU ê°ì§€ ë¬¸ì œ í•´ê²°
    fix_gpu_detection()
    
    # 4. H100 ìµœì í™”
    optimize_for_h100()
    
    # 5. GPU ì‚¬ìš© í…ŒìŠ¤íŠ¸
    test_success = test_gpu_usage()
    
    # 6. ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    create_gpu_test_script()
    
    print("\n" + "=" * 50)
    print("ğŸ“‹ ìš”ì•½:")
    
    if test_success:
        print("âœ… GPUê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        print("ì´ì œ AI-VAD í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”:")
        print("python train_aivad.py")
    else:
        print("âŒ GPU ì‚¬ìš©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜")
        print("2. CUDA ì„¤ì¹˜")
        print("3. PyTorch CUDA ë²„ì „ í˜¸í™˜ì„±")
    
    print("\nğŸ’¡ GPU ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸:")
    print("python gpu_benchmark.py")

if __name__ == "__main__":
    main()
