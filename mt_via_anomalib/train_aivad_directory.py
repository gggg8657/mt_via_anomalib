"""
AI-VAD ë””ë ‰í† ë¦¬ ê¸°ë°˜ í•™ìŠµ
íš¨ìœ¨ì ì¸ ë¹„ë””ì˜¤ íŒŒì¼ ë¡œë”©
"""

import os
import torch
import cv2
import numpy as np
from pathlib import Path
from anomalib.models.video import AiVad

def analyze_video_content(video_path):
    """ë¹„ë””ì˜¤ ë‚´ìš© ë¶„ì„"""
    print(f"ğŸ” ë¹„ë””ì˜¤ ë¶„ì„: {Path(video_path).name}")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"   âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
        return False, 0
    
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"   ğŸ“Š í•´ìƒë„: {width}x{height}, í”„ë ˆì„: {frame_count}, FPS: {fps:.1f}")
    
    # ì›€ì§ì„ ê°ì§€
    prev_frame = None
    motion_scores = []
    
    for i in range(min(10, frame_count)):  # ì²˜ìŒ 10í”„ë ˆì„ë§Œ ë¶„ì„
        ret, frame = cap.read()
        if not ret:
            break
        
        # í”„ë ˆì„ ì „ì²˜ë¦¬
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is not None:
            # ì›€ì§ì„ ê°ì§€
            diff = cv2.absdiff(prev_frame, frame_gray)
            motion_score = np.mean(diff)
            motion_scores.append(motion_score)
        
        prev_frame = frame_gray.copy()
    
    cap.release()
    
    avg_motion = np.mean(motion_scores) if motion_scores else 0
    motion_detected = avg_motion > 5
    print(f"   ğŸƒ í‰ê·  ì›€ì§ì„ ì ìˆ˜: {avg_motion:.2f}")
    print(f"   ğŸ¯ ì›€ì§ì„ ê°ì§€: {'âœ…' if motion_detected else 'âŒ'}")
    
    return motion_detected, avg_motion

def get_video_files_from_directory(video_dir):
    """ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°"""
    print(f"ğŸ“ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {video_dir}")
    
    if not os.path.exists(video_dir):
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_dir}")
        return []
    
    video_extensions = ['.avi', '.mp4', '.mov', '.mkv', '.flv', '.wmv']
    video_files = []
    
    # ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    for file_path in Path(video_dir).rglob('*'):
        if file_path.is_file() and file_path.suffix.lower() in video_extensions:
            video_files.append(str(file_path))
    
    print(f"âœ… ë°œê²¬ëœ ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
    
    # ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì •ë ¬
    video_files.sort()
    
    return video_files

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI-VAD ë””ë ‰í† ë¦¬ ê¸°ë°˜ í•™ìŠµ")
    print("=" * 60)
    print("ğŸ’¡ íš¨ìœ¨ì ì¸ ë””ë ‰í† ë¦¬ ê¸°ë°˜ ì²˜ë¦¬:")
    print("   1. ë””ë ‰í† ë¦¬ì—ì„œ ìë™ìœ¼ë¡œ ë¹„ë””ì˜¤ íŒŒì¼ ìŠ¤ìº”")
    print("   2. íŒŒì¼ ë³µì‚¬ ì—†ì´ ì§ì ‘ ì²˜ë¦¬")
    print("   3. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("   4. ë¹„ë””ì˜¤ ë‚´ìš© ë¶„ì„ ë° ì›€ì§ì„ ê°ì§€")
    print("=" * 60)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # 1. ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ì„¤ì •
    video_directory = input("\nğŸ“ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: C:\\Users\\User\\Videos): ").strip()
    
    if not video_directory:
        # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        video_directory = r"C:\Users\User\PycharmProjects\pythonProject1\Accurate-Interpretable-VAD\data\my_camera\training_videos"
        print(f"ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {video_directory}")
    
    # 2. ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
    video_files = get_video_files_from_directory(video_directory)
    
    if len(video_files) == 0:
        print("âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì§€ì›ë˜ëŠ” í˜•ì‹: .avi, .mp4, .mov, .mkv, .flv, .wmv")
        return
    
    # 3. ë¹„ë””ì˜¤ ë‚´ìš© ë¶„ì„
    print(f"\nğŸ” ë¹„ë””ì˜¤ ë‚´ìš© ë¶„ì„ ì‹œì‘...")
    motion_videos = []
    
    for video_file in video_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ ë¶„ì„
        motion_detected, motion_score = analyze_video_content(video_file)
        if motion_detected:
            motion_videos.append((video_file, motion_score))
    
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"   - ì „ì²´ ë¹„ë””ì˜¤: {len(video_files)}ê°œ")
    print(f"   - ì›€ì§ì„ ê°ì§€ëœ ë¹„ë””ì˜¤: {len(motion_videos)}ê°œ")
    
    # 4. AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •)
    print(f"\nğŸ¤– AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •)...")
    try:
        model = AiVad(
            # AI-VAD ê¸°ë³¸ ì„¤ì •
            use_velocity_features=True,
            use_pose_features=True,
            use_deep_features=True,
            # Density estimation ì„¤ì •
            n_components_velocity=2,
            n_neighbors_pose=1,
            n_neighbors_deep=1,
            # ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€ ì„¤ì •
            box_score_thresh=0.1,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì¶¤
            min_bbox_area=25,      # ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ê²Œ
            max_bbox_overlap=0.9,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ê²Œ
            foreground_binary_threshold=5,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°
        )
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™
        if device == "cuda":
            model = model.to(device)
            print(f"âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (GPU: {device})")
        else:
            print("âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (CPU)")
        
    except Exception as e:
        print(f"âŒ AI-VAD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 5. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    checkpoint_path = "aivad_proper_checkpoint.ckpt"
    if os.path.exists(checkpoint_path):
        print(f"\nğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
                if device == "cuda":
                    model = model.to(device)
                print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 6. AI-VAD ì§ì ‘ í•™ìŠµ (ë””ë ‰í† ë¦¬ ê¸°ë°˜)
    print(f"\nğŸ¯ AI-VAD ë””ë ‰í† ë¦¬ ê¸°ë°˜ í•™ìŠµ ì‹œì‘...")
    print("ğŸ’¡ í•™ìŠµ ê³¼ì •:")
    print("   1. ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ì§ì ‘ ë¡œë“œ")
    print("   2. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("   3. Density Update: ì •ìƒ íŠ¹ì„±ë“¤ì„ density estimatorì— ëˆ„ì ")
    print("   4. Density Fit: ëª¨ë“  íŠ¹ì„±ìœ¼ë¡œ ë¶„í¬ ëª¨ë¸ í•™ìŠµ")
    
    try:
        # ì§ì ‘ì ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° AI-VAD í•™ìŠµ
        model.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        
        # ëª¨ë¸ì´ GPUì— ìˆëŠ”ì§€ í™•ì¸
        print(f"ğŸ” ëª¨ë¸ ë””ë°”ì´ìŠ¤ í™•ì¸:")
        print(f"   - ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.model.parameters()).device}")
        print(f"   - íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤: {device}")
        
        if device == "cuda" and next(model.model.parameters()).device.type != "cuda":
            print("âš ï¸ ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì¤‘...")
            model.model = model.model.to(device)
        
        # ì›€ì§ì„ì´ ê°ì§€ëœ ë¹„ë””ì˜¤ë“¤ë§Œ ì²˜ë¦¬ (ë˜ëŠ” ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬)
        videos_to_process = motion_videos if motion_videos else [(vf, 0) for vf in video_files[:5]]
        
        total_clips_processed = 0
        total_detections = 0
        
        for i, (video_path, motion_score) in enumerate(videos_to_process[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬
            print(f"\nğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {i+1}/{min(5, len(videos_to_process))}")
            print(f"   íŒŒì¼: {Path(video_path).name}")
            print(f"   ì›€ì§ì„ ì ìˆ˜: {motion_score:.2f}")
            
            try:
                # ë¹„ë””ì˜¤ ë¡œë“œ
                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    print(f"   âš ï¸ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
                    continue
                
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                
                print(f"   ğŸ“Š í”„ë ˆì„ ìˆ˜: {frame_count}, FPS: {fps:.1f}")
                
                # 2í”„ë ˆì„ì”© í´ë¦½ìœ¼ë¡œ ì²˜ë¦¬
                clip_count = 0
                frame_buffer = []
                
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # í”„ë ˆì„ ì „ì²˜ë¦¬
                    frame_resized = cv2.resize(frame, (224, 224))
                    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
                    frame_tensor = torch.from_numpy(frame_rgb).float() / 255.0
                    frame_tensor = frame_tensor.permute(2, 0, 1)  # HWC -> CHW
                    
                    frame_buffer.append(frame_tensor)
                    
                    # 2í”„ë ˆì„ì´ ëª¨ì´ë©´ í´ë¦½ ì²˜ë¦¬
                    if len(frame_buffer) == 2:
                        try:
                            # ë¹„ë””ì˜¤ í´ë¦½ ìƒì„± [2, 3, 224, 224]
                            video_clip = torch.stack(frame_buffer).unsqueeze(0)  # [1, 2, 3, 224, 224]
                            
                            # ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì´ë™
                            if device == "cuda":
                                video_clip = video_clip.to(device)
                            
                            # AI-VAD ì¶”ë¡  (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •)
                            with torch.no_grad():
                                try:
                                    output = model.model(video_clip)
                                    
                                    # ì¶œë ¥ êµ¬ì¡° í™•ì¸
                                    if isinstance(output, list) and len(output) > 0:
                                        # íŠ¹ì„± ì¶”ì¶œ ë° density estimator ì—…ë°ì´íŠ¸
                                        if hasattr(model.model, 'density_estimator'):
                                            # AI-VADì˜ ë‚´ë¶€ íŠ¹ì„±ë“¤ì„ density estimatorì— ì¶”ê°€
                                            model.model.density_estimator.update(output)
                                            total_detections += 1
                                            
                                        clip_count += 1
                                        total_clips_processed += 1
                                        
                                        if clip_count == 1:  # ì²« ë²ˆì§¸ ì„±ê³µí•œ í´ë¦½
                                            print(f"   ğŸ‰ ì²« ë²ˆì§¸ ê°ì²´ ê°ì§€ ì„±ê³µ!")
                                        
                                    else:
                                        # ì¶œë ¥ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¦„
                                        if clip_count == 0:  # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ì¶œë ¥
                                            print(f"   âš ï¸ AI-VAD ì¶œë ¥ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¦„: {type(output)}")
                                        
                                except Exception as e:
                                    if "index 0 is out of bounds" in str(e):
                                        # Region Extractorì—ì„œ ê°ì²´ ê°ì§€ ì‹¤íŒ¨
                                        if clip_count == 0:  # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ì¶œë ¥
                                            print(f"   âš ï¸ ê°ì²´ ê°ì§€ ì‹¤íŒ¨: Region Extractorê°€ ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•¨")
                                    elif "amax(): Expected reduction dim 0" in str(e):
                                        # ë¹ˆ í…ì„œ ë¬¸ì œ
                                        if clip_count == 0:
                                            print(f"   âš ï¸ ë¹ˆ í…ì„œ ë¬¸ì œ: amax() ì—ëŸ¬")
                                    else:
                                        if clip_count == 0:
                                            print(f"   âš ï¸ AI-VAD ì¶”ë¡  ì‹¤íŒ¨: {e}")
                                    continue
                            
                        except Exception as e:
                            # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ìƒì„¸ ì—ëŸ¬ ì¶œë ¥
                            if clip_count == 0:
                                print(f"   âš ï¸ í´ë¦½ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        
                        # ë²„í¼ì—ì„œ ì²« ë²ˆì§¸ í”„ë ˆì„ ì œê±°
                        frame_buffer.pop(0)
                
                cap.release()
                print(f"   âœ… ì™„ë£Œ: {clip_count}ê°œ í´ë¦½ ì²˜ë¦¬")
                
            except Exception as e:
                print(f"   âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nğŸ“Š ì „ì²´ ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   - ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {min(5, len(videos_to_process))}ê°œ")
        print(f"   - ì²˜ë¦¬ëœ í´ë¦½: {total_clips_processed}ê°œ")
        print(f"   - ì´ ê°ì§€ ìˆ˜: {total_detections}ê°œ")
        
        # Density estimator ìµœì¢… í•™ìŠµ
        if hasattr(model.model, 'density_estimator') and total_detections > 0:
            print(f"\nğŸ”§ Density Estimator ìµœì¢… í•™ìŠµ...")
            model.model.density_estimator.fit()
            print("âœ… Density Estimator í•™ìŠµ ì™„ë£Œ")
        else:
            print("âš ï¸ ê°ì§€ëœ íŠ¹ì„±ì´ ì—†ì–´ density estimator í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            print("ğŸ’¡ í•´ê²° ë°©ë²•:")
            print("   1. ë¹„ë””ì˜¤ì— ì›€ì§ì´ëŠ” ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸")
            print("   2. ì¡°ëª…ì´ ì¶©ë¶„í•œì§€ í™•ì¸")
            print("   3. ê°ì²´ê°€ ì¶©ë¶„íˆ í°ì§€ í™•ì¸ (ìµœì†Œ 25x25 í”½ì…€)")
            print("   4. ë‹¤ë¥¸ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ì‹œë„")
        
        print("âœ… AI-VAD ë””ë ‰í† ë¦¬ ê¸°ë°˜ í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ AI-VAD ë””ë ‰í† ë¦¬ ê¸°ë°˜ í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 7. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = "aivad_directory_learned.ckpt"
    try:
        # AI-VAD ëª¨ë¸ ìƒíƒœ ì €ì¥
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'directory_based_density_estimation',
            'total_clips_processed': total_clips_processed,
            'total_detections': total_detections,
            'video_directory': video_directory,
        }, checkpoint_path)
        
        print(f"ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ AI-VAD ë””ë ‰í† ë¦¬ ê¸°ë°˜ í•™ìŠµ ì™„ë£Œ!")
    print("ğŸ’¡ í•™ìŠµëœ ë‚´ìš©:")
    print("1. ë””ë ‰í† ë¦¬ì—ì„œ ìë™ìœ¼ë¡œ ë¹„ë””ì˜¤ íŒŒì¼ ìŠ¤ìº”")
    print("2. íŒŒì¼ ë³µì‚¬ ì—†ì´ ì§ì ‘ ì²˜ë¦¬")
    print("3. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("4. ì›€ì§ì„ ê°ì§€ëœ ë¹„ë””ì˜¤ ìš°ì„  ì²˜ë¦¬")
    print("5. Density Estimatorë¡œ ì´ìƒ íƒì§€ ì¤€ë¹„")
    print("6. UIì—ì„œ 'aivad_directory_learned.ckpt' ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()
