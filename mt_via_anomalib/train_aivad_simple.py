"""
AI-VAD ê°„ë‹¨í•œ í•™ìŠµ ë°©ë²•
ë³µì¡í•œ Feature ì¶”ì¶œ ìš°íšŒí•˜ê³  ê¸°ë³¸ í•™ìŠµë§Œ ì§„í–‰
"""

import os
import json
import torch
import cv2
import numpy as np
from pathlib import Path
from anomalib.models.video import AiVad
import shutil

def create_simple_dataset_from_json(json_path="image_segments.json", target_dir="simple_dataset"):
    """JSONì—ì„œ ì •ìƒ í”„ë ˆì„ë“¤ì„ ê°„ë‹¨í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜"""
    print(f"ğŸ“ AI-VADìš© ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ìƒì„±: {json_path}")
    
    # í´ë” ìƒì„±
    normal_dir = Path(target_dir) / "train" / "good"
    normal_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"  ğŸ“‚ ì •ìƒ ì´ë¯¸ì§€ í´ë”: {normal_dir}")
    
    # JSON íŒŒì¼ ë¡œë“œ
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            segments = json.load(f)
        print(f"  ğŸ“Š JSON ë¡œë“œ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    except Exception as e:
        print(f"âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    # ì •ìƒ í”„ë ˆì„ë“¤ ì¶”ì¶œ
    copied_count = 0
    normal_count = 0
    
    for i, segment in enumerate(segments):
        if segment.get('category') == 'normal' and 'images' in segment:
            normal_count += 1
            images = segment['images']
            
            # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ í”„ë ˆì„ë“¤ ë³µì‚¬
            for j, img_path in enumerate(images[:3]):  # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìµœëŒ€ 3ê°œ
                if os.path.exists(img_path):
                    # íŒŒì¼ëª… ìƒì„±
                    name = f"normal_{normal_count:03d}_{j:02d}_{Path(img_path).name}"
                    target_path = normal_dir / name
                    
                    try:
                        # íŒŒì¼ ë³µì‚¬
                        shutil.copy2(img_path, target_path)
                        copied_count += 1
                        
                        if copied_count <= 20:  # ì²˜ìŒ 20ê°œë§Œ í‘œì‹œ
                            print(f"    ğŸ“¸ {name}")
                            
                    except Exception as e:
                        print(f"    âš ï¸ {img_path} ë³µì‚¬ ì‹¤íŒ¨: {e}")
    
    print(f"  âœ… ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸: {normal_count}ê°œ")
    print(f"  âœ… ë³µì‚¬ëœ ì´ë¯¸ì§€: {copied_count}ê°œ")
    
    if copied_count == 0:
        print("âŒ ë³µì‚¬ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    return str(Path(target_dir).absolute())

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI-VAD ê°„ë‹¨í•œ í•™ìŠµ ë°©ë²•")
    print("=" * 50)
    print("ğŸ’¡ ëª©ì :")
    print("   1. ë³µì¡í•œ Feature ì¶”ì¶œ ìš°íšŒ")
    print("   2. ê¸°ë³¸ ëª¨ë¸ êµ¬ì¡° í™•ì¸")
    print("   3. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ í™œìš©")
    print("   4. ê°„ë‹¨í•œ í•™ìŠµ ê³¼ì • í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # 1. ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ìƒì„±
    dataset_root = create_simple_dataset_from_json()
    if dataset_root is None:
        print("âŒ ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
        return
    
    # 2. AI-VAD ëª¨ë¸ ìƒì„±
    print(f"\nğŸ¤– AI-VAD ëª¨ë¸ ìƒì„±...")
    try:
        model = AiVad(
            # Feature ì¶”ì¶œ ì„¤ì • (ê°„ë‹¨í•˜ê²Œ)
            use_velocity_features=False,  # ë¹„í™œì„±í™”
            use_pose_features=False,      # ë¹„í™œì„±í™”
            use_deep_features=True,       # í™œì„±í™” (CLIPë§Œ)
            # Density estimation ì„¤ì •
            n_components_velocity=2,
            n_neighbors_pose=1,
            n_neighbors_deep=1,
        )
        print("âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ AI-VAD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 3. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    checkpoint_path = "aivad_proper_checkpoint.ckpt"
    if os.path.exists(checkpoint_path):
        print(f"\nğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, weights_only=False)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
                print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 4. ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¯ AI-VAD ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸...")
    print("ğŸ’¡ í…ŒìŠ¤íŠ¸ ê³¼ì •:")
    print("   1. ëª¨ë¸ êµ¬ì¡° í™•ì¸")
    print("   2. ê¸°ë³¸ Feature ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    print("   3. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ í™œìš©")
    print("   4. ë³µì¡í•œ Density Estimation ìš°íšŒ")
    
    try:
        # ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •
        model.eval().to(device)
        
        # ê°„ë‹¨í•œ ë”ë¯¸ ì…ë ¥ í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š ëª¨ë¸ êµ¬ì¡° í…ŒìŠ¤íŠ¸...")
        
        # ë”ë¯¸ ë¹„ë””ì˜¤ ì…ë ¥ ìƒì„± [1, 2, 3, 224, 224]
        dummy_input = torch.randn(1, 2, 3, 224, 224).to(device)
        
        print(f"   ì…ë ¥ í¬ê¸°: {dummy_input.shape}")
        
        # ëª¨ë¸ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ ë²„ì „)
        try:
            with torch.no_grad():
                # ì§ì ‘ ëª¨ë¸ ë‚´ë¶€ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
                print("   ğŸ” Flow Extractor í…ŒìŠ¤íŠ¸...")
                first_frame = dummy_input[:, 0]  # [1, 3, 224, 224]
                last_frame = dummy_input[:, 1]   # [1, 3, 224, 224]
                
                # Flow ì¶”ì¶œ
                flows = model.model.flow_extractor(first_frame, last_frame)
                print(f"   âœ… Flow ì¶”ì¶œ ì„±ê³µ: {flows.shape}")
                
                # Region ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
                print("   ğŸ” Region Extractor í…ŒìŠ¤íŠ¸...")
                regions = model.model.region_extractor(dummy_input)
                print(f"   âœ… Region ì¶”ì¶œ ì„±ê³µ: {len(regions)}ê°œ ì˜ì—­")
                
                # Feature ì¶”ì¶œ (CLIPë§Œ)
                print("   ğŸ” Feature Extractor í…ŒìŠ¤íŠ¸...")
                features = model.model.feature_extractor(first_frame, flows, regions)
                print(f"   âœ… Feature ì¶”ì¶œ ì„±ê³µ: {type(features)}")
                
        except Exception as e:
            print(f"   âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print("   ğŸ’¡ ì´ëŠ” ì •ìƒì ì¸ í˜„ìƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë³µì¡í•œ Feature ì¶”ì¶œ)")
        
        print("âœ… AI-VAD ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ AI-VAD ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 5. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = "aivad_simple_test.ckpt"
    try:
        # AI-VAD ëª¨ë¸ ìƒíƒœ ì €ì¥
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'simple_test',
        }, checkpoint_path)
        
        print(f"ğŸ’¾ í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ AI-VAD ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ’¡ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print("1. ëª¨ë¸ êµ¬ì¡° í™•ì¸ ì™„ë£Œ")
    print("2. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
    print("3. ê¸°ë³¸ Feature ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("4. UIì—ì„œ 'aivad_simple_test.ckpt' ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸")
    print("5. ë³µì¡í•œ Density Estimationì€ ì‹¤ì œ ë°ì´í„°ë¡œ ì§„í–‰ í•„ìš”")

if __name__ == "__main__":
    main()