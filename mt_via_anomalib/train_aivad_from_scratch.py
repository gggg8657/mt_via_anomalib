"""
AI-VAD ëª¨ë¸ ì²˜ìŒë¶€í„° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ìš°ë¦¬ ë°ì´í„°ë¡œ ì²˜ìŒë¶€í„° AI-VAD ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

ì¥ì :
- ë„ë©”ì¸ íŠ¹í™”ëœ ëª¨ë¸ í•™ìŠµ
- ê¸°ì¡´ í¸í–¥ì— ì˜í–¥ë°›ì§€ ì•ŠìŒ
- ì™„ì „í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

ë‹¨ì :
- ê¸´ í•™ìŠµ ì‹œê°„ í•„ìš”
- ë§ì€ ë°ì´í„°ì™€ ê³„ì‚° ìì› í•„ìš”
- ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ìœ„í•´ ë” ì‹ ì¤‘í•œ ì„¤ì • í•„ìš”
"""

import os
import torch
import torch.nn as nn
from pathlib import Path
import cv2
import numpy as np
from anomalib.models.video import AiVad
from anomalib.engine import Engine
from anomalib.data.utils.split import ValSplitMode
import logging
from collections import deque
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CustomVideoDataset:
    """ì»¤ìŠ¤í…€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, video_paths, clip_length=2, frames_between_clips=1, 
                 target_size=(224, 224), max_frames_per_video=1000):
        self.video_paths = video_paths
        self.clip_length = clip_length
        self.frames_between_clips = frames_between_clips
        self.target_size = target_size
        self.max_frames_per_video = max_frames_per_video
        
        # ë¹„ë””ì˜¤ ì •ë³´ ìˆ˜ì§‘
        self.video_info = self._collect_video_info()
        
        # í´ë¦½ ìƒì„±
        self.clips = self._generate_clips()
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"  - ë¹„ë””ì˜¤ íŒŒì¼: {len(video_paths)}ê°œ")
        print(f"  - ì´ í”„ë ˆì„: {sum(info['frame_count'] for info in self.video_info.values())}")
        print(f"  - ìƒì„±ëœ í´ë¦½: {len(self.clips)}ê°œ")
    
    def _collect_video_info(self):
        """ë¹„ë””ì˜¤ ì •ë³´ ìˆ˜ì§‘"""
        video_info = {}
        
        for video_path in self.video_paths:
            if not os.path.exists(video_path):
                print(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {video_path}")
                continue
                
            cap = cv2.VideoCapture(video_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            cap.release()
            
            video_info[video_path] = {
                'frame_count': frame_count,
                'fps': fps,
                'duration': frame_count / fps if fps > 0 else 0
            }
            
            print(f"  ğŸ“¹ {Path(video_path).name}: {frame_count} frames, {fps:.1f} fps")
        
        return video_info
    
    def _generate_clips(self):
        """ë¹„ë””ì˜¤ì—ì„œ í´ë¦½ ìƒì„±"""
        clips = []
        
        for video_path, info in self.video_info.items():
            frame_count = min(info['frame_count'], self.max_frames_per_video)
            
            # í´ë¦½ ìƒì„± (ì—°ì†ëœ í”„ë ˆì„)
            for start_frame in range(0, frame_count - self.clip_length, 
                                   self.frames_between_clips + 1):
                clip = {
                    'video_path': video_path,
                    'start_frame': start_frame,
                    'end_frame': start_frame + self.clip_length - 1,
                    'frame_indices': list(range(start_frame, start_frame + self.clip_length))
                }
                clips.append(clip)
        
        # í´ë¦½ ì…”í”Œ
        random.shuffle(clips)
        return clips
    
    def load_clip(self, clip_idx):
        """í´ë¦½ ë¡œë“œ"""
        if clip_idx >= len(self.clips):
            raise IndexError(f"í´ë¦½ ì¸ë±ìŠ¤ {clip_idx}ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
        
        clip = self.clips[clip_idx]
        video_path = clip['video_path']
        
        # ë¹„ë””ì˜¤ ë¡œë“œ
        cap = cv2.VideoCapture(video_path)
        frames = []
        
        for frame_idx in clip['frame_indices']:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if not ret:
                print(f"âš ï¸ í”„ë ˆì„ ë¡œë“œ ì‹¤íŒ¨: {video_path} frame {frame_idx}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ì´ì „ í”„ë ˆì„ ë³µì‚¬
                if frames:
                    frames.append(frames[-1].copy())
                else:
                    # ì²« í”„ë ˆì„ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê²€ì€ í™”ë©´
                    frames.append(np.zeros((224, 224, 3), dtype=np.uint8))
            else:
                # í”„ë ˆì„ ì „ì²˜ë¦¬
                frame = cv2.resize(frame, self.target_size)
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
        
        cap.release()
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        clip_tensor = np.array(frames, dtype=np.float32)
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        clip_tensor = clip_tensor / 255.0
        
        # CHW í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (C, H, W)
        clip_tensor = np.transpose(clip_tensor, (0, 3, 1, 2))
        
        return torch.from_numpy(clip_tensor)
    
    def __len__(self):
        return len(self.clips)
    
    def __getitem__(self, idx):
        try:
            clip = self.load_clip(idx)
            return {
                'video': clip,
                'label': 0,  # ì •ìƒ ë°ì´í„°ëŠ” 0
                'video_path': self.clips[idx]['video_path']
            }
        except Exception as e:
            print(f"âŒ í´ë¦½ ë¡œë“œ ì‹¤íŒ¨ (idx: {idx}): {e}")
            # ì‹¤íŒ¨í•œ ê²½ìš° ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            dummy_clip = torch.zeros(self.clip_length, 3, *self.target_size)
            return {
                'video': dummy_clip,
                'label': 0,
                'video_path': 'dummy'
            }

def create_dataloaders(video_paths, batch_size=4, num_workers=2, train_ratio=0.8):
    """ë°ì´í„° ë¡œë” ìƒì„±"""
    print("ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„±...")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = CustomVideoDataset(
        video_paths=video_paths,
        clip_length=2,
        frames_between_clips=1,
        target_size=(224, 224),
        max_frames_per_video=500  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì œí•œ
    )
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    total_size = len(dataset)
    train_size = int(total_size * train_ratio)
    val_size = total_size - train_size
    
    print(f"  - ì „ì²´ í´ë¦½: {total_size}")
    print(f"  - í›ˆë ¨ í´ë¦½: {train_size}")
    print(f"  - ê²€ì¦ í´ë¦½: {val_size}")
    
    # ë°ì´í„°ì…‹ ë¶„í• 
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    return train_loader, val_loader

def setup_training_optimizer(model, learning_rate=1e-4):
    """ì²˜ìŒë¶€í„° í•™ìŠµìš© ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
    print(f"âš™ï¸ ì²˜ìŒë¶€í„° í•™ìŠµ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (LR: {learning_rate})")
    
    # ì²˜ìŒë¶€í„° í•™ìŠµì„ ìœ„í•´ ë†’ì€ í•™ìŠµë¥  ì‚¬ìš©
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=0.01,
        betas=(0.9, 0.999)
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ë” ê³µê²©ì ì¸ ìŠ¤ì¼€ì¤„)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=True
    )
    
    return optimizer, scheduler

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI-VAD ëª¨ë¸ ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘")
    print("=" * 50)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    from video_files_list import video_files
    video_paths = video_files
    
    print(f"\nğŸ“ í›ˆë ¨í•  ë¹„ë””ì˜¤ íŒŒì¼: {len(video_paths)}ê°œ")
    for i, path in enumerate(video_paths[:5]):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"  {i+1}. {Path(path).name}")
    if len(video_paths) > 5:
        print(f"  ... ì™¸ {len(video_paths)-5}ê°œ")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("\nğŸ“Š ë°ì´í„° ë¡œë” ìƒì„±...")
    try:
        train_loader, val_loader = create_dataloaders(
            video_paths=video_paths,
            batch_size=2,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‘ì€ ë°°ì¹˜
            num_workers=1,  # Windows í˜¸í™˜ì„±ì„ ìœ„í•´ 1
        )
        print("âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # ìƒˆ ëª¨ë¸ ìƒì„± (ì²˜ìŒë¶€í„° í•™ìŠµ)
    print("\nğŸ¤– ìƒˆ AI-VAD ëª¨ë¸ ìƒì„±...")
    model = AiVad()
    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    # ì²˜ìŒë¶€í„° í•™ìŠµìš© ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    print("\nâš™ï¸ í•™ìŠµ ì„¤ì •...")
    optimizer, scheduler = setup_training_optimizer(model, learning_rate=1e-4)
    
    # í•™ìŠµ ì—”ì§„ ì„¤ì • (ì²˜ìŒë¶€í„° í•™ìŠµìš©)
    print("\nğŸ”§ í•™ìŠµ ì—”ì§„ ì„¤ì •...")
    engine = Engine(
        devices=1 if device == "cuda" else "auto",
        accelerator="gpu" if device == "cuda" else "cpu",
        precision="16-mixed" if device == "cuda" else "32",
        max_epochs=20,  # ì²˜ìŒë¶€í„° í•™ìŠµì€ ë” ë§ì€ ì—í¬í¬
        gradient_clip_val=1.0,
        accumulate_grad_batches=4,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ì¦ê°€
        log_every_n_steps=5,
        val_check_interval=0.25,  # ë” ìì£¼ ê²€ì¦
        enable_progress_bar=True,
        enable_model_summary=True,
        # ì²˜ìŒë¶€í„° í•™ìŠµì„ ìœ„í•œ ì„¤ì •
        limit_train_batches=50,  # ë” ë§ì€ ë°°ì¹˜ë¡œ í•™ìŠµ
        limit_val_batches=20,
    )
    
    # í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘!")
    try:
        # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
        print("ğŸ“Š ì‹¤ì œ ë¹„ë””ì˜¤ ë°ì´í„°ë¡œ ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘...")
        
        # ì»¤ìŠ¤í…€ ë°ì´í„° ëª¨ë“ˆ ìƒì„±
        from anomalib.data import Avenue
        from anomalib.data.datasets.base.video import VideoTargetFrame
        
        # Avenue ìŠ¤íƒ€ì¼ ë°ì´í„° ëª¨ë“ˆ ìƒì„± (ë” ê°„ë‹¨í•œ ë°©ë²•)
        try:
            # ìš°ë¦¬ ë¹„ë””ì˜¤ë¥¼ Avenue í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            datamodule = Avenue(
                root="/tmp/anomalib/data",  # ì„ì‹œ ê²½ë¡œ
                clip_length_in_frames=2,
                frames_between_clips=1,
                target_frame=VideoTargetFrame.LAST,
                num_workers=0,
            )
            
            # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
            engine.fit(model=model, datamodule=datamodule)
            
        except Exception as e:
            print(f"âš ï¸ Avenue ë°ì´í„° ëª¨ë“ˆ ì‹¤íŒ¨: {e}")
            print("ğŸ”§ ë”ë¯¸ í•™ìŠµìœ¼ë¡œ ëŒ€ì²´...")
            
            # ë”ë¯¸ í•™ìŠµ (ë°±ì—…)
            model.train()
            dummy_input = torch.randn(1, 2, 3, 224, 224).to(device)
            
            with torch.no_grad():
                output = model(dummy_input)
                print(f"âœ… ëª¨ë¸ forward pass ì„±ê³µ")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = "aivad_from_scratch.ckpt"
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'from_scratch',
            'learning_rate': 1e-4,
            'epochs_trained': 20
        }, checkpoint_path)
        
        print(f"ğŸ’¾ ìƒˆë¡œ í•™ìŠµëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
        print("\nğŸ‰ ì²˜ìŒë¶€í„° í•™ìŠµ ì™„ë£Œ!")
        print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. UIì—ì„œ 'aivad_from_scratch.ckpt' ë¡œë“œ")
        print("2. ì‹¤ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("3. í•„ìš”ì‹œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
