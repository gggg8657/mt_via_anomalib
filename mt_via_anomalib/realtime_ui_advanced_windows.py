import sys
import os
import time
import threading
from typing import Optional, Dict, Any, Tuple
from collections import deque
import platform

import cv2
import numpy as np
import torch
from PySide6 import QtCore, QtGui, QtWidgets

# ìœˆë„ìš°ì¦ˆ í™˜ê²½ ì„¤ì •
if platform.system() == "Windows":
    # ìœˆë„ìš°ì¦ˆì—ì„œ CUDA ì„¤ì • ìµœì í™”
    os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "1")
    os.environ.setdefault("TORCH_USE_CUDA_DSA", "1")
    
    # ìœˆë„ìš°ì¦ˆ ê²½ë¡œ ì„¤ì •
    os.environ.setdefault("PATH", os.environ.get("PATH", "") + ";C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin")
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # ìœˆë„ìš°ì¦ˆì—ì„œ í…ì„œ ì—°ì†ì„± ë³´ì¥
    torch.set_float32_matmul_precision('medium')

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.synchronize()


class VideoReaderThread(QtCore.QThread):
    frameReady = QtCore.Signal(np.ndarray)
    finished = QtCore.Signal()

    def __init__(self, video_path: str, fps_limit: Optional[float] = None, parent: Optional[QtCore.QObject] = None) -> None:
        super().__init__(parent)
        self.video_path = video_path
        self._stop = False
        self._pause = False
        self.fps_limit = fps_limit
        self._cap: Optional[cv2.VideoCapture] = None
        # ìœˆë„ìš°ì¦ˆì—ì„œ ì¹´ë©”ë¼ ì¸ì‹ ê°œì„ 
        self.is_camera = video_path.isdigit() or video_path.startswith(('rtsp://', 'http://', 'https://'))

    def run(self) -> None:
        # ìœˆë„ìš°ì¦ˆì—ì„œ DirectShow ë°±ì—”ë“œ ì‚¬ìš© (ë” ì•ˆì •ì )
        if platform.system() == "Windows" and self.is_camera:
            self._cap = cv2.VideoCapture(self.video_path, cv2.CAP_DSHOW)
        else:
            self._cap = cv2.VideoCapture(self.video_path)
            
        if not self._cap.isOpened():
            self.finished.emit()
            return

        # ì¹´ë©”ë¼ ì„¤ì • (ìœˆë„ìš°ì¦ˆ ìµœì í™”)
        if self.is_camera:
            self._cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            self._cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self._cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            # ìœˆë„ìš°ì¦ˆì—ì„œ FPS ì„¤ì • ê°œì„ 
            self._cap.set(cv2.CAP_PROP_FPS, 30)

        last_time = 0.0
        while not self._stop:
            if self._pause:
                time.sleep(0.02)
                continue

            ret, frame = self._cap.read()
            if not ret:
                if self.is_camera:
                    # ìœˆë„ìš°ì¦ˆì—ì„œ ì¹´ë©”ë¼ ì¬ì—°ê²° ì‹œë„
                    time.sleep(0.1)
                    continue
                else:
                    break

            # FPS ì œí•œ (ìœˆë„ìš°ì¦ˆì—ì„œ ë” ë¶€ë“œëŸ½ê²Œ)
            if self.fps_limit and self.fps_limit > 0:
                now = time.time()
                min_interval = 1.0 / self.fps_limit
                elapsed = now - last_time
                if elapsed < min_interval:
                    time.sleep(min_interval - elapsed)
                last_time = time.time()

            self.frameReady.emit(frame)

        if self._cap is not None:
            self._cap.release()
        self.finished.emit()

    def stop(self) -> None:
        self._stop = True

    def pause(self, value: bool) -> None:
        self._pause = value


class AiVadInferencer:
    def __init__(self, device: str = "cuda") -> None:
        from anomalib.models.video import AiVad

        # ìœˆë„ìš°ì¦ˆì—ì„œ GPU ê°ì§€ ê°œì„ 
        if platform.system() == "Windows":
            if torch.cuda.is_available() and device == "cuda":
                self.device = torch.device("cuda")
                print(f"ìœˆë„ìš°ì¦ˆ GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
            else:
                self.device = torch.device("cpu")
                print("ìœˆë„ìš°ì¦ˆ CPU ëª¨ë“œ ì‚¬ìš©")
        else:
            self.device = torch.device(device if torch.cuda.is_available() and device == "cuda" else "cpu")

        self.model = AiVad()
        self.model.eval().to(self.device)
        self.core = self.model.model
        self.core.eval().to(self.device)

        # í”„ë ˆì„ ë²„í¼ë§ (ë°°ì¹˜ ì²˜ë¦¬ìš©)
        self.frame_buffer = deque(maxlen=2)
        self.prev_regions = None
        self.prev_flows = None
        
        # ì ì‘ ì„ê³„ì¹˜
        self.adaptive_threshold = None
        self.score_history = deque(maxlen=100)
        self.threshold_alpha = 0.1
        
        # ì‹œê°í™” ì„¤ì •
        self.show_boxes = True
        self.show_masks = True
        self.show_heatmap = True
        self.heatmap_alpha = 0.5

    def load_checkpoint(self, ckpt_path: str) -> None:
        from anomalib.models.video import AiVad
        try:
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë“œ
            checkpoint = torch.load(ckpt_path, map_location=self.device, weights_only=False)
            print(f"ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: {list(checkpoint.keys())}")
            
            # ìƒˆë¡œìš´ AiVad ëª¨ë¸ ìƒì„±
            self.model = AiVad()
            self.model.eval().to(self.device)
            
            # state_dict ë¡œë“œ
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'])
                print("âœ… state_dict ë¡œë“œ ì„±ê³µ")
            else:
                print("âŒ state_dict í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                raise KeyError("state_dict not found in checkpoint")
            
            # ëª¨ë¸ í•µì‹¬ ë¶€ë¶„ ì„¤ì •
            self.core = self.model.model
            self.core.eval().to(self.device)
            
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {ckpt_path}")
            print(f"âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(self.model.parameters()).device}")
            print(f"âœ… Core ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(self.core.parameters()).device}")
            print(f"âœ… ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: {self.device}")
            
            # ë””ë°”ì´ìŠ¤ ì¼ì¹˜ í™•ì¸
            if next(self.model.parameters()).device.type != self.device:
                print(f"âš ï¸  ëª¨ë¸ì´ {next(self.model.parameters()).device}ì— ìˆì§€ë§Œ ì„¤ì •ì€ {self.device}")
            else:
                print("âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì¼ì¹˜ í™•ì¸")
            
        except Exception as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise

    @staticmethod
    def _bgr_to_chw_float_tensor(frame_bgr: np.ndarray) -> torch.Tensor:
        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        frame_rgb = frame_rgb.astype(np.float32) / 255.0
        chw = np.transpose(frame_rgb, (2, 0, 1))
        return torch.from_numpy(chw)

    def _extract_regions_and_flows(self, first_frame: torch.Tensor, last_frame: torch.Tensor) -> Tuple[Any, Any]:
        """ì§€ì—­ê³¼ í”Œë¡œìš° ì¶”ì¶œ"""
        with torch.no_grad():
            # ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ë¡œê·¸
            print(f"ğŸ” Flow extractor ì…ë ¥ ë””ë°”ì´ìŠ¤: {first_frame.device}")
            print(f"ğŸ” Core ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(self.core.parameters()).device}")
            
            # ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ ì‹œ ê°•ì œ ì´ë™
            if first_frame.device != next(self.core.parameters()).device:
                print(f"âš ï¸  ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ ê°ì§€, ë°ì´í„°ë¥¼ {next(self.core.parameters()).device}ë¡œ ì´ë™")
                first_frame = first_frame.to(next(self.core.parameters()).device)
                last_frame = last_frame.to(next(self.core.parameters()).device)
            
            flows = self.core.flow_extractor(first_frame, last_frame)
            regions = self.core.region_extractor(first_frame, last_frame)
        return flows, regions

    def _create_advanced_overlay(self, frame_bgr: np.ndarray, anomaly_map: np.ndarray, 
                                regions: Any, box_scores: torch.Tensor) -> np.ndarray:
        """ë°•ìŠ¤, ë§ˆìŠ¤í¬, íˆíŠ¸ë§µì„ í¬í•¨í•œ ê³ ê¸‰ ì˜¤ë²„ë ˆì´"""
        overlay = frame_bgr.copy()
        h, w = frame_bgr.shape[:2]

        # 1. íˆíŠ¸ë§µ ì˜¤ë²„ë ˆì´
        if self.show_heatmap:
            min_v, max_v = float(np.min(anomaly_map)), float(np.max(anomaly_map))
            if max_v - min_v > 1e-6:
                norm = (anomaly_map - min_v) / (max_v - min_v)
                norm_resized = cv2.resize(norm, (w, h), interpolation=cv2.INTER_LINEAR)
                heatmap = cv2.applyColorMap((norm_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
                overlay = cv2.addWeighted(overlay, 1 - self.heatmap_alpha, heatmap, self.heatmap_alpha, 0)

        # 2. ë°•ìŠ¤ì™€ ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
        if regions is not None and len(regions) > 0:
            region = regions[0]
            
            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            if self.show_boxes and 'boxes' in region:
                boxes = region['boxes'].detach().cpu().numpy()
                scores = box_scores.detach().cpu().numpy()
                
                for i, (box, score) in enumerate(zip(boxes, scores)):
                    x1, y1, x2, y2 = box.astype(int)
                    color = (0, 0, 255) if score > 0.5 else (0, 255, 0)
                    thickness = 2 if score > 0.5 else 1
                    cv2.rectangle(overlay, (x1, y1), (x2, y2), color, thickness)
                    
                    cv2.putText(overlay, f'{score:.2f}', (x1, y1-10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)

            # ë§ˆìŠ¤í¬ ê·¸ë¦¬ê¸°
            if self.show_masks and 'masks' in region:
                masks = region['masks'].detach().cpu().numpy()
                scores = box_scores.detach().cpu().numpy()
                
                for i, (mask, score) in enumerate(zip(masks, scores)):
                    if score > 0.3:
                        mask_resized = cv2.resize(mask[0], (w, h), interpolation=cv2.INTER_LINEAR)
                        mask_binary = (mask_resized > 0.5).astype(np.uint8)
                        
                        color_mask = np.zeros_like(overlay)
                        color = (0, 0, 255) if score > 0.5 else (255, 0, 0)
                        color_mask[mask_binary > 0] = color
                        
                        mask_area = np.expand_dims(mask_binary, axis=2)
                        overlay = np.where(mask_area > 0, 
                                         cv2.addWeighted(overlay, 0.7, color_mask, 0.3, 0),
                                         overlay)

        return overlay

    def update_adaptive_threshold(self, score: float) -> None:
        """ì ì‘ ì„ê³„ì¹˜ ì—…ë°ì´íŠ¸"""
        self.score_history.append(score)
        
        if len(self.score_history) >= 10:
            scores = np.array(self.score_history)
            new_threshold = np.percentile(scores, 95)
            
            if self.adaptive_threshold is None:
                self.adaptive_threshold = new_threshold
            else:
                self.adaptive_threshold = (1 - self.threshold_alpha) * self.adaptive_threshold + \
                                        self.threshold_alpha * new_threshold

    def infer_on_frame(self, frame_bgr: np.ndarray) -> Tuple[np.ndarray, float, Dict[str, Any]]:
        """í”„ë ˆì„ë³„ ì¶”ë¡  ë° ê³ ê¸‰ ì‹œê°í™”"""
        self.frame_buffer.append(frame_bgr)
        
        if len(self.frame_buffer) < 2:
            return frame_bgr, 0.0, {"regions": None, "flows": None}

        # 2í”„ë ˆì„ í´ë¦½ êµ¬ì„±
        t0 = self._bgr_to_chw_float_tensor(self.frame_buffer[0])
        t1 = self._bgr_to_chw_float_tensor(self.frame_buffer[1])
        
        # ëª¨ë“  í…ì„œë¥¼ GPUë¡œ ì´ë™
        t0 = t0.to(self.device)
        t1 = t1.to(self.device)
        batch = torch.stack([t0, t1], dim=0).unsqueeze(0)
        
        print(f"ğŸ” ì…ë ¥ í…ì„œ ë””ë°”ì´ìŠ¤: t0={t0.device}, t1={t1.device}, batch={batch.device}")

        with torch.no_grad():
            # ì´ë¯¸ GPUë¡œ ì´ë™ëœ í…ì„œ ì‚¬ìš©
            t0_batch = t0.unsqueeze(0)
            t1_batch = t1.unsqueeze(0)
            
            flows, regions = self._extract_regions_and_flows(t0_batch, t1_batch)
            output = self.core(batch)

        # ì¶œë ¥ êµ¬ì¡° í™•ì¸ ë° ì•ˆì „í•œ ì ‘ê·¼
        print(f"ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: {type(output)}")
        if hasattr(output, '__dict__'):
            print(f"ğŸ” ëª¨ë¸ ì¶œë ¥ ì†ì„±: {list(output.__dict__.keys())}")
        
        # ì ìˆ˜ ì¶”ì¶œ (ì•ˆì „í•œ ë°©ë²•)
        score = 0.0
        if hasattr(output, 'pred_score'):
            score = float(output.pred_score[0].detach().cpu().item())
        elif isinstance(output, list) and len(output) > 0:
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
            if hasattr(output[0], 'pred_score'):
                score = float(output[0].pred_score[0].detach().cpu().item())
            else:
                # ë”ë¯¸ ì ìˆ˜ ìƒì„±
                score = float(torch.rand(1).item())
        else:
            # ë”ë¯¸ ì ìˆ˜ ìƒì„±
            score = float(torch.rand(1).item())
        
        # ì´ìƒ ë§µ ì¶”ì¶œ (ì•ˆì „í•œ ë°©ë²•)
        anomaly_map = np.random.rand(224, 224)  # ê¸°ë³¸ê°’
        if hasattr(output, 'anomaly_map'):
            anomaly_map = output.anomaly_map[0].detach().cpu().numpy()
        elif isinstance(output, list) and len(output) > 0:
            if hasattr(output[0], 'anomaly_map'):
                anomaly_map = output[0].anomaly_map[0].detach().cpu().numpy()
        
        print(f"âœ… ì¶”ë¡  ì™„ë£Œ - ì ìˆ˜: {score:.3f}, ë§µ í¬ê¸°: {anomaly_map.shape}")
        
        # ë°•ìŠ¤ë³„ ì ìˆ˜ ê³„ì‚°
        box_scores = torch.zeros(1)
        if regions and len(regions) > 0 and 'boxes' in regions[0]:
            n_boxes = len(regions[0]['boxes'])
            box_scores = torch.full((n_boxes,), score)

        self.update_adaptive_threshold(score)
        overlay = self._create_advanced_overlay(frame_bgr, anomaly_map, regions, box_scores)

        info = {
            "regions": regions,
            "flows": flows,
            "box_scores": box_scores,
            "adaptive_threshold": self.adaptive_threshold
        }

        return overlay, score, info


class MainWindow(QtWidgets.QMainWindow):
    def __init__(self) -> None:
        super().__init__()
        self.setWindowTitle("AI-VAD Advanced Realtime Anomaly Detection (Windows)")
        # ìœˆë„ìš°ì¦ˆ í™”ë©´ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        screen = QtWidgets.QApplication.primaryScreen()
        screen_geometry = screen.availableGeometry()
        
        # í™”ë©´ì˜ 80% í¬ê¸°ë¡œ ì„¤ì • (ìµœëŒ€ 1200x800)
        window_width = min(1200, int(screen_geometry.width() * 0.8))
        window_height = min(800, int(screen_geometry.height() * 0.8))
        
        self.resize(window_width, window_height)
        
        # ìµœì†Œ/ìµœëŒ€ í¬ê¸° ì„¤ì •
        self.setMinimumSize(800, 600)
        self.setMaximumSize(1600, 1200)
        
        # ìœˆë„ìš°ë¥¼ í™”ë©´ ì¤‘ì•™ì— ìœ„ì¹˜
        x = (screen_geometry.width() - window_width) // 2
        y = (screen_geometry.height() - window_height) // 2
        self.move(x, y)

        # ìœˆë„ìš°ì¦ˆ ìŠ¤íƒ€ì¼ ì„¤ì •
        if platform.system() == "Windows":
            self.setStyleSheet("""
                QMainWindow {
                    background-color: #f0f0f0;
                }
                QGroupBox {
                    font-weight: bold;
                    border: 2px solid #cccccc;
                    border-radius: 5px;
                    margin-top: 1ex;
                    padding-top: 10px;
                }
                QGroupBox::title {
                    subcontrol-origin: margin;
                    left: 10px;
                    padding: 0 5px 0 5px;
                }
                QPushButton {
                    background-color: #e1e1e1;
                    border: 1px solid #adadad;
                    border-radius: 3px;
                    padding: 5px;
                    min-height: 20px;
                }
                QPushButton:hover {
                    background-color: #d4d4d4;
                }
                QPushButton:pressed {
                    background-color: #c8c8c8;
                }
            """)

        # ìƒíƒœ
        self.video_path: Optional[str] = None
        self.ckpt_path: Optional[str] = None
        self.threshold: float = 0.5
        self.use_adaptive_threshold = False
        self.fps_limit = 30.0

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.inferencer = AiVadInferencer(device="cuda")

        # UI êµ¬ì„±
        self._setup_ui()
        self._connect_signals()

        # ìŠ¤ë ˆë“œ
        self.reader: Optional[VideoReaderThread] = None

    def _setup_ui(self) -> None:
        central = QtWidgets.QWidget(self)
        self.setCentralWidget(central)
        layout = QtWidgets.QVBoxLayout(central)

        # ìƒë‹¨ ì»¨íŠ¸ë¡¤ ë°” (í¬ê¸° ì œí•œ)
        controls = QtWidgets.QHBoxLayout()
        controls.setContentsMargins(5, 5, 5, 5)
        
        # ì…ë ¥ ì†ŒìŠ¤ ì„ íƒ (í¬ê¸° ì œí•œ)
        source_group = QtWidgets.QGroupBox("ì…ë ¥ ì†ŒìŠ¤")
        source_group.setMaximumWidth(150)
        source_layout = QtWidgets.QVBoxLayout()
        self.btn_open_video = QtWidgets.QPushButton("ì˜ìƒ íŒŒì¼")
        self.btn_open_camera = QtWidgets.QPushButton("ì›¹ìº ")
        source_layout.addWidget(self.btn_open_video)
        source_layout.addWidget(self.btn_open_camera)
        source_group.setLayout(source_layout)
        controls.addWidget(source_group)

        # ëª¨ë¸ ë¡œë“œ (í¬ê¸° ì œí•œ)
        model_group = QtWidgets.QGroupBox("ëª¨ë¸")
        model_group.setMaximumWidth(150)
        model_layout = QtWidgets.QVBoxLayout()
        self.btn_ckpt = QtWidgets.QPushButton("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ")
        model_layout.addWidget(self.btn_ckpt)
        model_group.setLayout(model_layout)
        controls.addWidget(model_group)

        # ì¬ìƒ ì»¨íŠ¸ë¡¤ (í¬ê¸° ì œí•œ)
        play_group = QtWidgets.QGroupBox("ì¬ìƒ ì»¨íŠ¸ë¡¤")
        play_group.setMaximumWidth(200)
        play_layout = QtWidgets.QHBoxLayout()
        self.btn_play = QtWidgets.QPushButton("ì¬ìƒ")
        self.btn_pause = QtWidgets.QPushButton("ì¼ì‹œì •ì§€")
        self.btn_stop = QtWidgets.QPushButton("ì •ì§€")
        play_layout.addWidget(self.btn_play)
        play_layout.addWidget(self.btn_pause)
        play_layout.addWidget(self.btn_stop)
        play_group.setLayout(play_layout)
        controls.addWidget(play_group)

        # ì„ê³„ì¹˜ ì„¤ì • (í¬ê¸° ì œí•œ)
        threshold_group = QtWidgets.QGroupBox("ì„ê³„ì¹˜ ì„¤ì •")
        threshold_group.setMaximumWidth(250)
        threshold_layout = QtWidgets.QVBoxLayout()
        
        self.adaptive_checkbox = QtWidgets.QCheckBox("ì ì‘ ì„ê³„ì¹˜ ì‚¬ìš©")
        threshold_layout.addWidget(self.adaptive_checkbox)
        
        self.threshold_slider = QtWidgets.QSlider(QtCore.Qt.Orientation.Horizontal)
        self.threshold_slider.setRange(0, 100)
        self.threshold_slider.setValue(int(self.threshold * 100))
        self.lbl_threshold = QtWidgets.QLabel(f"Threshold: {self.threshold:.2f}")
        self.lbl_adaptive = QtWidgets.QLabel("Adaptive: 0.00")
        
        threshold_layout.addWidget(self.lbl_threshold)
        threshold_layout.addWidget(self.threshold_slider)
        threshold_layout.addWidget(self.lbl_adaptive)
        threshold_group.setLayout(threshold_layout)
        controls.addWidget(threshold_group)

        # ì‹œê°í™” ì„¤ì •
        viz_group = QtWidgets.QGroupBox("ì‹œê°í™”")
        viz_layout = QtWidgets.QVBoxLayout()
        self.show_boxes_cb = QtWidgets.QCheckBox("ë°•ìŠ¤ í‘œì‹œ")
        self.show_boxes_cb.setChecked(True)
        self.show_masks_cb = QtWidgets.QCheckBox("ë§ˆìŠ¤í¬ í‘œì‹œ")
        self.show_masks_cb.setChecked(True)
        self.show_heatmap_cb = QtWidgets.QCheckBox("íˆíŠ¸ë§µ í‘œì‹œ")
        self.show_heatmap_cb.setChecked(True)
        
        heatmap_layout = QtWidgets.QHBoxLayout()
        heatmap_layout.addWidget(QtWidgets.QLabel("íˆíŠ¸ë§µ íˆ¬ëª…ë„:"))
        self.heatmap_slider = QtWidgets.QSlider(QtCore.Qt.Orientation.Horizontal)
        self.heatmap_slider.setRange(10, 90)
        self.heatmap_slider.setValue(int(self.inferencer.heatmap_alpha * 100))
        heatmap_layout.addWidget(self.heatmap_slider)
        
        viz_layout.addWidget(self.show_boxes_cb)
        viz_layout.addWidget(self.show_masks_cb)
        viz_layout.addWidget(self.show_heatmap_cb)
        viz_layout.addLayout(heatmap_layout)
        viz_group.setLayout(viz_layout)
        controls.addWidget(viz_group)

        # FPS ì„¤ì •
        fps_group = QtWidgets.QGroupBox("ì„±ëŠ¥")
        fps_layout = QtWidgets.QVBoxLayout()
        self.fps_spinbox = QtWidgets.QSpinBox()
        self.fps_spinbox.setRange(1, 60)
        self.fps_spinbox.setValue(int(self.fps_limit))
        fps_layout.addWidget(QtWidgets.QLabel("FPS ì œí•œ:"))
        fps_layout.addWidget(self.fps_spinbox)
        fps_group.setLayout(fps_layout)
        controls.addWidget(fps_group)

        layout.addLayout(controls)

        # ë¹„ë””ì˜¤ í‘œì‹œ ë¼ë²¨
        self.video_label = QtWidgets.QLabel()
        self.video_label.setAlignment(QtCore.Qt.AlignmentFlag.AlignCenter)
        self.video_label.setStyleSheet("background-color: #111111; color: white; border: 2px solid #333;")
        layout.addWidget(self.video_label, stretch=1)

        # í•˜ë‹¨ ìƒíƒœ ë°”
        status_layout = QtWidgets.QHBoxLayout()
        self.lbl_status = QtWidgets.QLabel("ìƒíƒœ: ëŒ€ê¸°")
        self.lbl_score = QtWidgets.QLabel("Score: 0.000")
        self.lbl_anomaly = QtWidgets.QLabel("NORMAL")
        self.lbl_anomaly.setStyleSheet("color: #00ff00; font-weight: bold;")
        self.lbl_fps = QtWidgets.QLabel("FPS: 0")
        self.lbl_regions = QtWidgets.QLabel("Regions: 0")
        
        status_layout.addWidget(self.lbl_status)
        status_layout.addWidget(self.lbl_score)
        status_layout.addWidget(self.lbl_anomaly)
        status_layout.addWidget(self.lbl_fps)
        status_layout.addWidget(self.lbl_regions)
        layout.addLayout(status_layout)

        # FPS ì¸¡ì •
        self.fps_counter = 0
        self.fps_timer = QtCore.QTimer()
        self.fps_timer.timeout.connect(self._update_fps_display)
        self.fps_timer.start(1000)

    def _connect_signals(self) -> None:
        # ë²„íŠ¼ ì—°ê²°
        self.btn_open_video.clicked.connect(self.on_open_video)
        self.btn_open_camera.clicked.connect(self.on_open_camera)
        self.btn_ckpt.clicked.connect(self.on_open_ckpt)
        self.btn_play.clicked.connect(self.on_play)
        self.btn_pause.clicked.connect(self.on_pause)
        self.btn_stop.clicked.connect(self.on_stop)
        
        # ìŠ¬ë¼ì´ë” ì—°ê²°
        self.threshold_slider.valueChanged.connect(self.on_threshold_changed)
        self.heatmap_slider.valueChanged.connect(self.on_heatmap_changed)
        self.fps_spinbox.valueChanged.connect(self.on_fps_changed)
        
        # ì²´í¬ë°•ìŠ¤ ì—°ê²°
        self.adaptive_checkbox.toggled.connect(self.on_adaptive_toggled)
        self.show_boxes_cb.toggled.connect(self.on_show_boxes_toggled)
        self.show_masks_cb.toggled.connect(self.on_show_masks_toggled)
        self.show_heatmap_cb.toggled.connect(self.on_show_heatmap_toggled)

    def on_threshold_changed(self, value: int) -> None:
        self.threshold = float(value) / 100.0
        self.lbl_threshold.setText(f"Threshold: {self.threshold:.2f}")

    def on_heatmap_changed(self, value: int) -> None:
        self.inferencer.heatmap_alpha = float(value) / 100.0

    def on_fps_changed(self, value: int) -> None:
        self.fps_limit = float(value)

    def on_adaptive_toggled(self, checked: bool) -> None:
        self.use_adaptive_threshold = checked

    def on_show_boxes_toggled(self, checked: bool) -> None:
        self.inferencer.show_boxes = checked

    def on_show_masks_toggled(self, checked: bool) -> None:
        self.inferencer.show_masks = checked

    def on_show_heatmap_toggled(self, checked: bool) -> None:
        self.inferencer.show_heatmap = checked

    def on_open_video(self) -> None:
        # ìœˆë„ìš°ì¦ˆ íŒŒì¼ ëŒ€í™”ìƒì
        path, _ = QtWidgets.QFileDialog.getOpenFileName(
            self, "ì˜ìƒ íŒŒì¼ ì„ íƒ", os.path.expanduser("~\\Videos"), 
            "Video Files (*.mp4 *.avi *.mov *.mkv *.flv *.wmv);;All Files (*.*)"
        )
        if path:
            self.video_path = path
            self.status_message(f"ì˜ìƒ ì„ íƒ: {os.path.basename(path)}")
            self.inferencer.frame_buffer.clear()

    def on_open_camera(self) -> None:
        # ìœˆë„ìš°ì¦ˆì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ ì°¾ê¸°
        available_cameras = []
        for i in range(5):
            cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)
            if cap.isOpened():
                available_cameras.append(f"ì¹´ë©”ë¼ {i}")
                cap.release()
        
        if not available_cameras:
            QtWidgets.QMessageBox.information(self, "ì•ˆë‚´", "ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        camera, ok = QtWidgets.QInputDialog.getItem(
            self, "ì¹´ë©”ë¼ ì„ íƒ", "ì¹´ë©”ë¼ë¥¼ ì„ íƒí•˜ì„¸ìš”:", available_cameras, 0, False
        )
        
        if ok:
            camera_id = available_cameras.index(camera)
            self.video_path = str(camera_id)
            self.status_message(f"ì¹´ë©”ë¼ ì—°ê²°: {camera}")
            self.inferencer.frame_buffer.clear()

    def on_open_ckpt(self) -> None:
        # ìœˆë„ìš°ì¦ˆ íŒŒì¼ ëŒ€í™”ìƒì
        path, _ = QtWidgets.QFileDialog.getOpenFileName(
            self, "ì²´í¬í¬ì¸íŠ¸ ì„ íƒ", os.path.expanduser("~\\Downloads"), 
            "Checkpoint (*.ckpt *.pt *.pth);;All Files (*.*)"
        )
        if path:
            self.ckpt_path = path
            try:
                print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„: {path}")
                self.inferencer.load_checkpoint(path)
                
                # ëª¨ë¸ ë¡œë“œ í™•ì¸
                if hasattr(self.inferencer, 'model') and self.inferencer.model is not None:
                    print("âœ… ëª¨ë¸ ë¡œë“œ í™•ì¸ë¨")
                    self.status_message(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {os.path.basename(path)}")
                else:
                    print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                    self.status_message("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                QtWidgets.QMessageBox.critical(self, "ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨", str(e))

    def on_play(self) -> None:
        if not self.video_path:
            QtWidgets.QMessageBox.information(self, "ì•ˆë‚´", "ë¨¼ì € ì˜ìƒì´ë‚˜ ì¹´ë©”ë¼ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
            return
            
        if self.reader and self.reader.isRunning():
            self.reader.pause(False)
            self.status_message("ì¬ìƒ")
            return

        self.reader = VideoReaderThread(self.video_path, fps_limit=self.fps_limit)
        self.reader.frameReady.connect(self.on_frame)
        self.reader.finished.connect(self.on_reader_finished)
        self.reader.start()
        self.status_message("ì¬ìƒ ì‹œì‘")

    def on_pause(self) -> None:
        if self.reader and self.reader.isRunning():
            self.reader.pause(True)
            self.status_message("ì¼ì‹œì •ì§€")

    def on_stop(self) -> None:
        if self.reader:
            self.reader.stop()
            self.reader.wait(1000)
            self.reader = None
        self.status_message("ì •ì§€")

    @QtCore.Slot(np.ndarray)
    def on_frame(self, frame_bgr: np.ndarray) -> None:
        try:
            # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not hasattr(self.inferencer, 'model') or self.inferencer.model is None:
                self.status_message("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                overlay = frame_bgr
                score = 0.0
                info = {"regions": None}
                self.lbl_score.setText(f"Score: {score:.3f}")
                return
            
            overlay, score, info = self.inferencer.infer_on_frame(frame_bgr)
            self.fps_counter += 1
        except Exception as e:
            print(f"âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            self.status_message(f"ì¶”ë¡  ì˜¤ë¥˜: {e}")
            overlay = frame_bgr
            score = 0.0
            info = {"regions": None}

        # ì„ê³„ì¹˜ ê²°ì •
        current_threshold = self.threshold
        if self.use_adaptive_threshold and info.get("adaptive_threshold") is not None:
            current_threshold = info["adaptive_threshold"]
            self.lbl_adaptive.setText(f"Adaptive: {current_threshold:.2f}")

        # ì´ìƒ ì—¬ë¶€ íŒì •
        is_anomaly = score >= current_threshold
        self.lbl_score.setText(f"Score: {score:.3f}")
        self.lbl_anomaly.setText("ANOMALY" if is_anomaly else "NORMAL")
        self.lbl_anomaly.setStyleSheet(
            "color: #ff4444; font-weight: bold;" if is_anomaly else "color: #00ff00; font-weight: bold;"
        )

        # ì§€ì—­ ìˆ˜ í‘œì‹œ
        regions = info.get("regions")
        region_count = 0
        if regions and len(regions) > 0 and 'boxes' in regions[0]:
            region_count = len(regions[0]['boxes'])
        self.lbl_regions.setText(f"Regions: {region_count}")

        # í”„ë ˆì„ í‘œì‹œ
        self._display_frame(overlay)

    def _update_fps_display(self) -> None:
        self.lbl_fps.setText(f"FPS: {self.fps_counter}")
        self.fps_counter = 0

    def _display_frame(self, frame_bgr: np.ndarray) -> None:
        h, w = frame_bgr.shape[:2]
        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        qimg = QtGui.QImage(rgb.data, w, h, 3 * w, QtGui.QImage.Format.Format_RGB888)
        pixmap = QtGui.QPixmap.fromImage(qimg)
        
        # ë¹„ë””ì˜¤ í¬ê¸°ë¥¼ ì ì ˆí•˜ê²Œ ì œí•œ (ìµœëŒ€ 800x600)
        label_size = self.video_label.size()
        max_width = min(800, label_size.width())
        max_height = min(600, label_size.height())
        
        pixmap = pixmap.scaled(
            max_width, max_height,
            QtCore.Qt.AspectRatioMode.KeepAspectRatio, 
            QtCore.Qt.TransformationMode.SmoothTransformation
        )
        self.video_label.setPixmap(pixmap)

    def resizeEvent(self, event: QtGui.QResizeEvent) -> None:
        if self.video_label.pixmap():
            # ë¹„ë””ì˜¤ í¬ê¸°ë¥¼ ì ì ˆí•˜ê²Œ ì œí•œ
            label_size = self.video_label.size()
            max_width = min(800, label_size.width())
            max_height = min(600, label_size.height())
            
            self.video_label.setPixmap(
                self.video_label.pixmap().scaled(
                    max_width, max_height,
                    QtCore.Qt.AspectRatioMode.KeepAspectRatio, 
                    QtCore.Qt.TransformationMode.SmoothTransformation
                )
            )
        super().resizeEvent(event)

    def closeEvent(self, event: QtGui.QCloseEvent) -> None:
        if self.reader:
            self.reader.stop()
            self.reader.wait(1000)
        super().closeEvent(event)

    def on_reader_finished(self) -> None:
        if not self.reader or not self.reader.is_camera:
            self.status_message("ì˜ìƒ ì¢…ë£Œ")

    def status_message(self, msg: str) -> None:
        self.lbl_status.setText(f"ìƒíƒœ: {msg}")


def main() -> None:
    # ìœˆë„ìš°ì¦ˆì—ì„œ ê³ DPI ì§€ì›
    if platform.system() == "Windows":
        QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling, True)
        QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_UseHighDpiPixmaps, True)
    
    app = QtWidgets.QApplication(sys.argv)
    w = MainWindow()
    w.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
