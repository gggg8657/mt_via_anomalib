"""
AI-VAD ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
ê°ì²´ ê°ì§€ ì‹¤íŒ¨ ë¬¸ì œ í•´ê²°
"""

import os
import torch
import cv2
import numpy as np
from pathlib import Path
from anomalib.models.video import AiVad
import shutil

# video_files_list.pyì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
def get_video_files():
    """video_files_list.pyì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    video_files = [
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_0.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_1.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_2.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_3.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_4.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_5.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_6.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_7.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_8.avi",
        "C:\\Users\\User\\PycharmProjects\\pythonProject1\\Accurate-Interpretable-VAD\\data\\my_camera\\training_videos\\normal_9.avi",
    ]
    return video_files

def analyze_video_content(video_path):
    """ë¹„ë””ì˜¤ ë‚´ìš© ë¶„ì„"""
    print(f"ğŸ” ë¹„ë””ì˜¤ ë¶„ì„: {Path(video_path).name}")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"   âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
        return False
    
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"   ğŸ“Š í•´ìƒë„: {width}x{height}, í”„ë ˆì„: {frame_count}, FPS: {fps:.1f}")
    
    # ì²« ë²ˆì§¸ì™€ ì¤‘ê°„ í”„ë ˆì„ ë¶„ì„
    frames_to_check = [0, frame_count // 2, frame_count - 1]
    motion_detected = False
    
    prev_frame = None
    motion_scores = []
    
    for i in range(min(10, frame_count)):  # ì²˜ìŒ 10í”„ë ˆì„ë§Œ ë¶„ì„
        ret, frame = cap.read()
        if not ret:
            break
        
        # í”„ë ˆì„ ì „ì²˜ë¦¬
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is not None:
            # ì›€ì§ì„ ê°ì§€
            diff = cv2.absdiff(prev_frame, frame_gray)
            motion_score = np.mean(diff)
            motion_scores.append(motion_score)
            
            if motion_score > 5:  # ì„ê³„ê°’
                motion_detected = True
        
        prev_frame = frame_gray.copy()
    
    cap.release()
    
    avg_motion = np.mean(motion_scores) if motion_scores else 0
    print(f"   ğŸƒ í‰ê·  ì›€ì§ì„ ì ìˆ˜: {avg_motion:.2f}")
    print(f"   ğŸ¯ ì›€ì§ì„ ê°ì§€: {'âœ…' if motion_detected else 'âŒ'}")
    
    return motion_detected, avg_motion

def create_ultra_sensitive_dataset(video_files, target_dir="ultra_sensitive_dataset"):
    """ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±"""
    print(f"ğŸ“ ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ë°ì´í„°ì…‹ ìƒì„±: {target_dir}")
    
    # Avenue ë°ì´í„°ì…‹ êµ¬ì¡° ìƒì„±
    train_dir = Path(target_dir) / "train" / "normal"
    train_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"  ğŸ“‚ ì •ìƒ ë¹„ë””ì˜¤ í´ë”: {train_dir}")
    
    # ì‹¤ì œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ë³µì‚¬
    copied_count = 0
    
    for i, video_file in enumerate(video_files):
        if os.path.exists(video_file):
            # ë¹„ë””ì˜¤ íŒŒì¼ëª… ìƒì„±
            video_name = f"normal_{i:03d}.avi"
            target_path = train_dir / video_name
            
            try:
                # íŒŒì¼ ë³µì‚¬
                shutil.copy2(video_file, target_path)
                copied_count += 1
                
                # ë¹„ë””ì˜¤ ë‚´ìš© ë¶„ì„
                analyze_video_content(video_file)
                
            except Exception as e:
                print(f"    âš ï¸ {video_file} ë³µì‚¬ ì‹¤íŒ¨: {e}")
        else:
            print(f"    âš ï¸ íŒŒì¼ ì—†ìŒ: {video_file}")
    
    print(f"  âœ… ë³µì‚¬ëœ ë¹„ë””ì˜¤: {copied_count}ê°œ")
    
    if copied_count == 0:
        print("âŒ ë³µì‚¬ëœ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    return str(Path(target_dir).absolute())

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI-VAD ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ")
    print("=" * 60)
    print("ğŸ’¡ ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •:")
    print("   1. box_score_thresh=0.1 (ë§¤ìš° ë‚®ìŒ)")
    print("   2. min_bbox_area=25 (ë§¤ìš° ì‘ìŒ)")
    print("   3. max_bbox_overlap=0.9 (ë§¤ìš° ë†’ìŒ)")
    print("   4. foreground_binary_threshold=5 (ë§¤ìš° ë¯¼ê°)")
    print("   5. ë¹„ë””ì˜¤ ë‚´ìš© ë¶„ì„ ë° ì›€ì§ì„ ê°ì§€")
    print("=" * 60)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # 1. ì‹¤ì œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
    video_files = get_video_files()
    print(f"\nğŸ“Š ì‹¤ì œ ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
    
    # 2. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ë°ì´í„°ì…‹ ìƒì„±
    dataset_root = create_ultra_sensitive_dataset(video_files)
    if dataset_root is None:
        print("âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
        return
    
    # 3. AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •)
    print(f"\nğŸ¤– AI-VAD ëª¨ë¸ ìƒì„± (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •)...")
    try:
        model = AiVad(
            # AI-VAD ê¸°ë³¸ ì„¤ì •
            use_velocity_features=True,
            use_pose_features=True,
            use_deep_features=True,
            # Density estimation ì„¤ì •
            n_components_velocity=2,
            n_neighbors_pose=1,
            n_neighbors_deep=1,
            # ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€ ì„¤ì •
            box_score_thresh=0.1,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì¶¤ (0.3 -> 0.1)
            min_bbox_area=25,      # ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ê²Œ (50 -> 25)
            max_bbox_overlap=0.9,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ê²Œ (0.8 -> 0.9)
            foreground_binary_threshold=5,  # ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê° (10 -> 5)
        )
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™
        if device == "cuda":
            model = model.to(device)
            print(f"âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (GPU: {device})")
        else:
            print("âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (CPU)")
        
    except Exception as e:
        print(f"âŒ AI-VAD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 4. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    checkpoint_path = "aivad_proper_checkpoint.ckpt"
    if os.path.exists(checkpoint_path):
        print(f"\nğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
                # ê°€ì¤‘ì¹˜ ë¡œë“œ í›„ì—ë„ ëª¨ë¸ì„ GPUë¡œ ì´ë™
                if device == "cuda":
                    model = model.to(device)
                print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 5. ì§ì ‘ì ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° AI-VAD í•™ìŠµ
    print(f"\nğŸ¯ AI-VAD ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ í•™ìŠµ ì‹œì‘...")
    print("ğŸ’¡ í•™ìŠµ ê³¼ì •:")
    print("   1. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("   2. ëª¨ë“  ì›€ì§ì„ê³¼ ë³€í™” ê°ì§€")
    print("   3. Density Update: ì •ìƒ íŠ¹ì„±ë“¤ì„ density estimatorì— ëˆ„ì ")
    print("   4. Density Fit: ëª¨ë“  íŠ¹ì„±ìœ¼ë¡œ ë¶„í¬ ëª¨ë¸ í•™ìŠµ")
    
    try:
        # ì§ì ‘ì ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° AI-VAD í•™ìŠµ
        model.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        
        # ëª¨ë¸ì´ GPUì— ìˆëŠ”ì§€ í™•ì¸
        print(f"ğŸ” ëª¨ë¸ ë””ë°”ì´ìŠ¤ í™•ì¸:")
        print(f"   - ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.model.parameters()).device}")
        print(f"   - íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤: {device}")
        
        if device == "cuda" and next(model.model.parameters()).device.type != "cuda":
            print("âš ï¸ ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì¤‘...")
            model.model = model.model.to(device)
        
        # ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì§ì ‘ ì²˜ë¦¬
        video_files_available = []
        train_dir = Path(dataset_root) / "train" / "normal"
        
        for video_file in train_dir.glob("*.avi"):
            if video_file.exists():
                video_files_available.append(str(video_file))
        
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë¹„ë””ì˜¤: {len(video_files_available)}ê°œ")
        
        total_clips_processed = 0
        total_detections = 0
        
        for i, video_path in enumerate(video_files_available[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬
            print(f"\nğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {i+1}/{min(5, len(video_files_available))}")
            print(f"   íŒŒì¼: {Path(video_path).name}")
            
            try:
                # ë¹„ë””ì˜¤ ë¡œë“œ
                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    print(f"   âš ï¸ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
                    continue
                
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                
                print(f"   ğŸ“Š í”„ë ˆì„ ìˆ˜: {frame_count}, FPS: {fps:.1f}")
                
                # 2í”„ë ˆì„ì”© í´ë¦½ìœ¼ë¡œ ì²˜ë¦¬
                clip_count = 0
                frame_buffer = []
                
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # í”„ë ˆì„ ì „ì²˜ë¦¬
                    frame_resized = cv2.resize(frame, (224, 224))
                    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
                    frame_tensor = torch.from_numpy(frame_rgb).float() / 255.0
                    frame_tensor = frame_tensor.permute(2, 0, 1)  # HWC -> CHW
                    
                    frame_buffer.append(frame_tensor)
                    
                    # 2í”„ë ˆì„ì´ ëª¨ì´ë©´ í´ë¦½ ì²˜ë¦¬
                    if len(frame_buffer) == 2:
                        try:
                            # ë¹„ë””ì˜¤ í´ë¦½ ìƒì„± [2, 3, 224, 224]
                            video_clip = torch.stack(frame_buffer).unsqueeze(0)  # [1, 2, 3, 224, 224]
                            
                            # ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì´ë™
                            if device == "cuda":
                                video_clip = video_clip.to(device)
                            
                            # AI-VAD ì¶”ë¡  (ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ì„¤ì •)
                            with torch.no_grad():
                                try:
                                    output = model.model(video_clip)
                                    
                                    # ì¶œë ¥ êµ¬ì¡° í™•ì¸
                                    if isinstance(output, list) and len(output) > 0:
                                        # íŠ¹ì„± ì¶”ì¶œ ë° density estimator ì—…ë°ì´íŠ¸
                                        if hasattr(model.model, 'density_estimator'):
                                            # AI-VADì˜ ë‚´ë¶€ íŠ¹ì„±ë“¤ì„ density estimatorì— ì¶”ê°€
                                            model.model.density_estimator.update(output)
                                            total_detections += 1
                                            
                                        clip_count += 1
                                        total_clips_processed += 1
                                        
                                        if clip_count == 1:  # ì²« ë²ˆì§¸ ì„±ê³µí•œ í´ë¦½
                                            print(f"   ğŸ‰ ì²« ë²ˆì§¸ ê°ì²´ ê°ì§€ ì„±ê³µ!")
                                        
                                    else:
                                        # ì¶œë ¥ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¦„
                                        if clip_count == 0:  # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ì¶œë ¥
                                            print(f"   âš ï¸ AI-VAD ì¶œë ¥ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¦„: {type(output)}")
                                        
                                except Exception as e:
                                    if "index 0 is out of bounds" in str(e):
                                        # Region Extractorì—ì„œ ê°ì²´ ê°ì§€ ì‹¤íŒ¨
                                        if clip_count == 0:  # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ì¶œë ¥
                                            print(f"   âš ï¸ ê°ì²´ ê°ì§€ ì‹¤íŒ¨: Region Extractorê°€ ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•¨")
                                    elif "amax(): Expected reduction dim 0" in str(e):
                                        # ë¹ˆ í…ì„œ ë¬¸ì œ
                                        if clip_count == 0:
                                            print(f"   âš ï¸ ë¹ˆ í…ì„œ ë¬¸ì œ: amax() ì—ëŸ¬")
                                    else:
                                        if clip_count == 0:
                                            print(f"   âš ï¸ AI-VAD ì¶”ë¡  ì‹¤íŒ¨: {e}")
                                    continue
                            
                        except Exception as e:
                            # ì²« ë²ˆì§¸ í´ë¦½ì—ì„œë§Œ ìƒì„¸ ì—ëŸ¬ ì¶œë ¥
                            if clip_count == 0:
                                print(f"   âš ï¸ í´ë¦½ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        
                        # ë²„í¼ì—ì„œ ì²« ë²ˆì§¸ í”„ë ˆì„ ì œê±°
                        frame_buffer.pop(0)
                
                cap.release()
                print(f"   âœ… ì™„ë£Œ: {clip_count}ê°œ í´ë¦½ ì²˜ë¦¬")
                
            except Exception as e:
                print(f"   âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nğŸ“Š ì „ì²´ ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   - ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {min(5, len(video_files_available))}ê°œ")
        print(f"   - ì²˜ë¦¬ëœ í´ë¦½: {total_clips_processed}ê°œ")
        print(f"   - ì´ ê°ì§€ ìˆ˜: {total_detections}ê°œ")
        
        # Density estimator ìµœì¢… í•™ìŠµ
        if hasattr(model.model, 'density_estimator') and total_detections > 0:
            print(f"\nğŸ”§ Density Estimator ìµœì¢… í•™ìŠµ...")
            model.model.density_estimator.fit()
            print("âœ… Density Estimator í•™ìŠµ ì™„ë£Œ")
        else:
            print("âš ï¸ ê°ì§€ëœ íŠ¹ì„±ì´ ì—†ì–´ density estimator í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            print("ğŸ’¡ ê·¹ë‹¨ì ì¸ í•´ê²° ë°©ë²•:")
            print("   1. ë¹„ë””ì˜¤ ë‚´ìš©ì„ ë‹¤ì‹œ í™•ì¸ (ì •ë§ ì›€ì§ì„ì´ ìˆëŠ”ì§€)")
            print("   2. ì¡°ëª… ê°œì„  (ë” ë°ê²Œ)")
            print("   3. ê°ì²´ í¬ê¸° í™•ì¸ (ìµœì†Œ 25x25 í”½ì…€)")
            print("   4. AI-VAD íŒŒë¼ë¯¸í„°ë¥¼ ë” ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì¶¤")
            print("   5. ë‹¤ë¥¸ ë¹„ë””ì˜¤ íŒŒì¼ ì‹œë„")
        
        print("âœ… AI-VAD ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ AI-VAD ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 6. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = "aivad_ultra_sensitive_learned.ckpt"
    try:
        # AI-VAD ëª¨ë¸ ìƒíƒœ ì €ì¥
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'ultra_sensitive_density_estimation',
            'total_clips_processed': total_clips_processed,
            'total_detections': total_detections,
        }, checkpoint_path)
        
        print(f"ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ AI-VAD ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ í•™ìŠµ ì™„ë£Œ!")
    print("ğŸ’¡ í•™ìŠµëœ ë‚´ìš©:")
    print("1. ê·¹ë‹¨ì ìœ¼ë¡œ ë¯¼ê°í•œ ê°ì²´ ê°ì§€")
    print("2. ëª¨ë“  ì›€ì§ì„ê³¼ ë³€í™” ê°ì§€")
    print("3. ë†’ì€ Domain ì ìš©ì„±")
    print("4. ì •ìƒ ë°ì´í„°ì˜ Feature ë¶„í¬ í•™ìŠµ")
    print("5. Density Estimatorë¡œ ì´ìƒ íƒì§€ ì¤€ë¹„")
    print("6. UIì—ì„œ 'aivad_ultra_sensitive_learned.ckpt' ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()
