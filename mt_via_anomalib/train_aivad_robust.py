"""
AI-VAD ê²¬ê³ í•œ í•™ìŠµ ë°©ë²•
ê°ì²´ ê°ì§€ ì‹¤íŒ¨ ë¬¸ì œ í•´ê²°
"""

import os
import json
import torch
import cv2
import numpy as np
from pathlib import Path
from anomalib.models.video import AiVad
import shutil
from torch.utils.data import Dataset, DataLoader

class RobustVideoDataset(Dataset):
    """ê²¬ê³ í•œ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹"""
    
    def __init__(self, image_folder, sequence_length=2, transform=None):
        self.image_folder = Path(image_folder)
        self.sequence_length = sequence_length
        self.transform = transform
        
        # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
        self.image_files = sorted([f for f in self.image_folder.glob("*.jpg")])
        print(f"ğŸ“¸ ì´ {len(self.image_files)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        
        # ì‹œí€€ìŠ¤ ìƒì„± (ì—°ì†ëœ í”„ë ˆì„ë“¤)
        self.sequences = []
        for i in range(len(self.image_files) - sequence_length + 1):
            sequence = self.image_files[i:i + sequence_length]
            self.sequences.append(sequence)
        
        print(f"ğŸ¬ {len(self.sequences)}ê°œ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ìƒì„±")
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence_paths = self.sequences[idx]
        frames = []
        
        # ê° ì‹œí€€ìŠ¤ì—ì„œ í”„ë ˆì„ ë¡œë“œ
        for path in sequence_paths:
            frame = cv2.imread(str(path))
            if frame is None:
                # ë¹ˆ í”„ë ˆì„ ìƒì„±
                frame = np.zeros((224, 224, 3), dtype=np.uint8)
            else:
                # 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                frame = cv2.resize(frame, (224, 224))
            
            # BGR to RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ì •ê·œí™” [0, 1]
            frame = frame.astype(np.float32) / 255.0
            
            # CHW í˜•íƒœë¡œ ë³€í™˜
            frame = np.transpose(frame, (2, 0, 1))
            frames.append(frame)
        
        # í…ì„œë¡œ ë³€í™˜ [sequence_length, 3, 224, 224]
        video_tensor = torch.from_numpy(np.array(frames))
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ [1, sequence_length, 3, 224, 224]
        video_tensor = video_tensor.unsqueeze(0)
        
        return {
            'image': video_tensor,
            'label': torch.tensor(0),  # ì •ìƒ ë°ì´í„°
            'image_path': str(sequence_paths[0])  # ì²« ë²ˆì§¸ í”„ë ˆì„ ê²½ë¡œ
        }

def create_robust_dataset_from_json(json_path="image_segments.json", target_dir="robust_dataset"):
    """JSONì—ì„œ ì •ìƒ í”„ë ˆì„ë“¤ì„ ê²¬ê³ í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜"""
    print(f"ğŸ“ AI-VADìš© ê²¬ê³ í•œ ë°ì´í„°ì…‹ ìƒì„±: {json_path}")
    
    # í´ë” ìƒì„±
    normal_dir = Path(target_dir) / "train" / "good"
    normal_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"  ğŸ“‚ ì •ìƒ ì´ë¯¸ì§€ í´ë”: {normal_dir}")
    
    # JSON íŒŒì¼ ë¡œë“œ
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            segments = json.load(f)
        print(f"  ğŸ“Š JSON ë¡œë“œ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    except Exception as e:
        print(f"âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    # ì •ìƒ í”„ë ˆì„ë“¤ ì¶”ì¶œ
    copied_count = 0
    normal_count = 0
    
    for i, segment in enumerate(segments):
        if segment.get('category') == 'normal' and 'images' in segment:
            normal_count += 1
            images = segment['images']
            
            # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ í”„ë ˆì„ë“¤ ë³µì‚¬
            for j, img_path in enumerate(images[:5]):  # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìµœëŒ€ 5ê°œ
                if os.path.exists(img_path):
                    # íŒŒì¼ëª… ìƒì„±
                    name = f"normal_{normal_count:03d}_{j:02d}_{Path(img_path).name}"
                    target_path = normal_dir / name
                    
                    try:
                        # íŒŒì¼ ë³µì‚¬
                        shutil.copy2(img_path, target_path)
                        copied_count += 1
                        
                        if copied_count <= 20:  # ì²˜ìŒ 20ê°œë§Œ í‘œì‹œ
                            print(f"    ğŸ“¸ {name}")
                            
                    except Exception as e:
                        print(f"    âš ï¸ {img_path} ë³µì‚¬ ì‹¤íŒ¨: {e}")
    
    print(f"  âœ… ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸: {normal_count}ê°œ")
    print(f"  âœ… ë³µì‚¬ëœ ì´ë¯¸ì§€: {copied_count}ê°œ")
    
    if copied_count == 0:
        print("âŒ ë³µì‚¬ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    return str(Path(target_dir).absolute())

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI-VAD ê²¬ê³ í•œ í•™ìŠµ ë°©ë²•")
    print("=" * 60)
    print("ğŸ’¡ ë¬¸ì œ í•´ê²°:")
    print("   1. ê°ì²´ ê°ì§€ ì‹¤íŒ¨ ë¬¸ì œ í•´ê²°")
    print("   2. Region Extractor ì„¤ì • ì¡°ì •")
    print("   3. ê²¬ê³ í•œ Feature ì¶”ì¶œ")
    print("   4. ìš°ë¦¬ ë°ì´í„°ë¡œ Domain ì ìš©ì„± í–¥ìƒ")
    print("=" * 60)
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # 1. ê²¬ê³ í•œ ë°ì´í„°ì…‹ ìƒì„±
    dataset_root = create_robust_dataset_from_json()
    if dataset_root is None:
        print("âŒ ê²¬ê³ í•œ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
        return
    
    # 2. AI-VAD ëª¨ë¸ ìƒì„± (ê²¬ê³ í•œ ì„¤ì •)
    print(f"\nğŸ¤– AI-VAD ëª¨ë¸ ìƒì„± (ê²¬ê³ í•œ ì„¤ì •)...")
    try:
        model = AiVad(
            # ê°ì²´ ê°ì§€ ì„¤ì • ì¡°ì • (ë” ê´€ëŒ€í•˜ê²Œ)
            box_score_thresh=0.3,  # 0.7 â†’ 0.3 (ë” ë‚®ì€ ì„ê³„ì¹˜)
            min_bbox_area=50,      # 100 â†’ 50 (ë” ì‘ì€ ì˜ì—­ë„ í—ˆìš©)
            max_bbox_overlap=0.8,  # 0.65 â†’ 0.8 (ë” ë§ì€ ê²¹ì¹¨ í—ˆìš©)
            
            # ì „ê²½ ê°ì§€ ì„¤ì • ì¡°ì •
            enable_foreground_detections=True,
            foreground_binary_threshold=10,  # 18 â†’ 10 (ë” ë¯¼ê°í•˜ê²Œ)
            
            # Feature ì¶”ì¶œ ì„¤ì • (ê²¬ê³ í•˜ê²Œ)
            use_velocity_features=True,
            use_pose_features=True,
            use_deep_features=True,
            
            # Density estimation ì„¤ì •
            n_components_velocity=2,
            n_neighbors_pose=1,
            n_neighbors_deep=1,
        )
        print("âœ… AI-VAD ëª¨ë¸ ìƒì„± ì™„ë£Œ (ê²¬ê³ í•œ ì„¤ì •)")
        
    except Exception as e:
        print(f"âŒ AI-VAD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 3. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    checkpoint_path = "aivad_proper_checkpoint.ckpt"
    if os.path.exists(checkpoint_path):
        print(f"\nğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, weights_only=False)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
                print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 4. ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ“Š ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±...")
    try:
        normal_dir = Path(dataset_root) / "train" / "good"
        video_dataset = RobustVideoDataset(normal_dir, sequence_length=2)
        
        # DataLoader ìƒì„±
        dataloader = DataLoader(
            video_dataset,
            batch_size=1,  # ì‘ì€ ë°°ì¹˜ í¬ê¸°
            shuffle=True,
            num_workers=0,
            drop_last=True
        )
        
        print(f"âœ… ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
        print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {dataloader.batch_size}")
        print(f"   ğŸ“Š ì´ ë°°ì¹˜ ìˆ˜: {len(dataloader)}")
        
    except Exception as e:
        print(f"âŒ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 5. AI-VAD í•™ìŠµ (ê²¬ê³ í•œ ë°©ë²•)
    print(f"\nğŸ¯ AI-VAD í•™ìŠµ ì‹œì‘ (ê²¬ê³ í•œ ë°©ë²•)...")
    print("ğŸ’¡ í•™ìŠµ ê³¼ì •:")
    print("   1. ê²¬ê³ í•œ ê°ì²´ ê°ì§€ ì„¤ì •")
    print("   2. Feature Extraction: Flow, Region, Pose, Deep features")
    print("   3. Density Update: ì •ìƒ íŠ¹ì„±ë“¤ì„ density estimatorì— ëˆ„ì ")
    print("   4. Density Fit: ëª¨ë“  íŠ¹ì„±ìœ¼ë¡œ ë¶„í¬ ëª¨ë¸ í•™ìŠµ")
    print("   5. No Backpropagation: ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì—†ìŒ!")
    
    try:
        # AI-VADì˜ ê²¬ê³ í•œ í•™ìŠµ ë°©ë²•
        model.eval().to(device)
        
        total_detections = 0
        successful_batches = 0
        
        for batch_idx, batch in enumerate(dataloader):
            try:
                # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                video_tensor = batch['image'].to(device)  # [batch_size, 1, 2, 3, 224, 224]
                
                # ë°°ì¹˜ ì°¨ì› ì¡°ì • [batch_size, 2, 3, 224, 224]
                video_tensor = video_tensor.squeeze(1)
                
                # AI-VAD ëª¨ë¸ í˜¸ì¶œ (ê²¬ê³ í•œ ë°©ë²•)
                with torch.no_grad():
                    # AI-VADì˜ ê²¬ê³ í•œ í•™ìŠµ ë°©ì‹
                    features_per_batch = model.model(video_tensor)
                
                # Density estimator ì—…ë°ì´íŠ¸ (ê²¬ê³ í•œ ë°©ë²•)
                if hasattr(model.model, 'density_estimator'):
                    for features in features_per_batch:
                        if features:  # ë¹ˆ features ì²´í¬
                            model.model.density_estimator.update(features, f"video_{batch_idx}")
                            total_detections += len(next(iter(features.values())))
                
                successful_batches += 1
                
                if batch_idx % 10 == 0:
                    print(f"  ì²˜ë¦¬ëœ ë°°ì¹˜: {batch_idx + 1}/{len(dataloader)}")
                
            except Exception as e:
                print(f"  âš ï¸ ë°°ì¹˜ {batch_idx + 1} ì‹¤íŒ¨: {e}")
                # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                continue
        
        print("âœ… AI-VAD í•™ìŠµ ì™„ë£Œ!")
        
        # Density estimator ìƒíƒœ í™•ì¸
        print(f"ğŸ“Š Density Estimator ìƒíƒœ:")
        print(f"   - ì„±ê³µí•œ ë°°ì¹˜: {successful_batches}/{len(dataloader)}")
        print(f"   - ì´ ê°ì§€ ìˆ˜: {total_detections}")
        
        # Density estimator fit í˜¸ì¶œ
        if total_detections > 0:
            model.fit()  # density estimator í•™ìŠµ
            print("âœ… Density Estimator í•™ìŠµ ì™„ë£Œ")
        else:
            print("âš ï¸ ê°ì§€ëœ ì˜ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ê²¬ê³ í•œ ì„¤ì •ìœ¼ë¡œë„ ê°ì²´ ê°ì§€ê°€ ì–´ë µìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì‹¤ì œ ë¹„ë””ì˜¤ ë°ì´í„°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ AI-VAD í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 6. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = "aivad_robust_learned.ckpt"
    try:
        # AI-VAD ëª¨ë¸ ìƒíƒœ ì €ì¥
        torch.save({
            'state_dict': model.state_dict(),
            'pytorch-lightning_version': '2.0.0',
            'model_class': 'AiVad',
            'training_type': 'robust_density_estimation',
            'total_detections': total_detections,
        }, checkpoint_path)
        
        print(f"ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ AI-VAD ê²¬ê³ í•œ í•™ìŠµ ì™„ë£Œ!")
    print("ğŸ’¡ í•™ìŠµëœ ë‚´ìš©:")
    print("1. ê²¬ê³ í•œ ê°ì²´ ê°ì§€ ì„¤ì • ì ìš©")
    print("2. ìš°ë¦¬ ë°ì´í„°ë¡œ Domain ì ìš©ì„± í–¥ìƒ")
    print("3. ì •ìƒ ë°ì´í„°ì˜ Feature ë¶„í¬ í•™ìŠµ")
    print("4. Density Estimatorë¡œ ì´ìƒ íƒì§€ ì¤€ë¹„")
    print("5. UIì—ì„œ 'aivad_robust_learned.ckpt' ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸")
    print("6. ë¹„ì •ìƒ ë°ì´í„°ëŠ” ë¶„í¬ì—ì„œ ë²—ì–´ë‚˜ ë†’ì€ ì ìˆ˜")

if __name__ == "__main__":
    main()
